{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d522811-c8b4-4fca-a6b6-81a339d75192",
   "metadata": {},
   "source": [
    "    # **TinyBERT实现案例**\n",
    "\n",
    "    BERT模型是NLP领域最著名的模型之一，它的出现带动了NLP领域预训练+微调方法的快速发展。BERT模型拥有优秀的自然语言理解能力，但模型参数庞大，训练时间耗费较长。TinyBERT是缩小版的BERT，对BERT的架构进行了简化。从推理角度看，TinyBERT比BERT-base（BERT模型基础版本）体积小了7.5倍、速度快了9.4倍，自然语言理解的性能表现更突出。\n",
    "\n",
    "    **论文** :\n",
    "\n",
    "    Transformer：https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html\n",
    "\n",
    "    bert：https://arxiv.org/abs/1810.04805 \n",
    "\n",
    "    tinyBERT:https://arxiv.org/abs/1909.10351"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2925c10-4c0b-4221-a459-2d314458278c",
   "metadata": {},
   "source": [
    "# BERT介绍\n",
    "TinyBERT的模型架构依然是BERT的架构，因此如果想掌握tinyBERT的结构，那么就必须知\n",
    "道BERT的模型结构。反过来说，如果了解BERT的模型结构，那么自然会对tinyBERT有了清晰的认知。因此这里先对BERT的内部结构进行介绍。\n",
    "\n",
    "  了解BERT模型之前，希望你对Transformer架构已经有了基本的了解。Transformer来源于自注意力机制。简单的介绍一下，Transformer的结构分为编码器和解码器，编码器可以把文字编码成高纬度的特征，而解码器可以用这些特征去完成相应任务，比如生成翻译。Transformer非常强大，NLP届的两大巨头模型都是由他的一部分改变而来，如下图。GPT取用的是Transformer的解码器部分，而BERT取的是Transformer的编码器，因此Bert的作用，大家可以理解为一个编码器,将输入的文字抽取为高维的特征。\n",
    "   ![Transformer](ipyphoto/BERTandGPT.jpg)\n",
    "                             \n",
    "                                 \n",
    "                                \n",
    "  <div align=\"center\"><i>图1：Transformer, BERT 和 GPT </i></div>\n",
    "\n",
    "   在CV领域，在imagenet上训练一个编码器，迁移到其他任务，已经是惯用手段了。 而在BERT以前，NLP领域使用预训练技术的并不多。BERT是在大规模的无标注文本上进行预训练的，它为无标注的文本设计了两个自监督预训练任务，第一个是MLM（Masked Language Model）任务： 遮盖掉句子中一定比例的单词，使用剩余单词来预测被遮盖的词。 第二个是NSP（Next Sentence Prediction）任务：预测输入的两个句子是否是上下文关系。这样，无需标注即可在海量文本数据上进行预训练，得到一个效果十分良好的编码器，然后使用此编码器在其他文本任务上进行微调。BERT所建立的预训练加微调的模式在后续的NLP网络中得到了广泛的应用。\n",
    "   \n",
    "## BERT结构  \n",
    "   BERT模型主要由三部分构成：嵌入层（embedding），BERT layer堆叠层， 和输出层。 一句文本，首先会被分词，然后进入BERT，经过嵌入层，得到嵌入向量。之后嵌入向量会进入BERT layer的自注意力模块中进行提取，最后得到深度特征，最后经过输出层输出。下面详细介绍这个步骤。\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "   <img src=\"ipyphoto/BERT.png\" alt=\"Drawing\" style=\"width: 400px;\" align=\"mid\"/>\n",
    "                      \n",
    "\n",
    "                               \n",
    "   <div align=\"center\"><i>图2： BERT结构 </i></div>\n",
    "   \n",
    "   \n",
    "   \n",
    "   **一，嵌入层（embedding layer）**：\n",
    "   \n",
    "   嵌入，有时候又称为向量化。就是把输入映射为高维的向量。比如输入一个词，一般只有一个编号，是一维的。在嵌入层会被转换成高维的向量。\n",
    "   \n",
    "   <img src=\"ipyphoto/trans_emb.jpg\" alt=\"Drawing\" style=\"width: 500px;\" align=\"mid\"/>\n",
    "   \n",
    "                                               \n",
    "   <div align=\"center\"><i>图3： Transformer嵌入层 </i></div>\n",
    "   BERT源自Transformer，他们的嵌入层也是很相似的。上图是Transformer的嵌入层，我们可以看到，当一个分词变为输入时，首先要经过词嵌入（Embedding），变为长度为hidden_size的向量（图中为6，实际一般很长），字所在的位置也要进行嵌入。两个嵌入向量直接数值相加，我们就得到了嵌入层的输出。\n",
    "   \n",
    "   下图则是BERT的嵌入层，它的输出来源于三个嵌入向量相加。我们可以看到有两个区别。1：增加了句子嵌入（segment Embeddings），因为BERT的输入是两个句子，因此要在这里用句子嵌入值标识出是哪一句。2：出现了字符token，如E[cls],E[sep]。CLStoken一般用来统计全局的信息，最后可以用此token的特征进行下游分类任务。SEPtoken则表示句子的分割和中止。  其实还存在着第三个区别：Transformer的位置嵌入是固定的，也就是公式算出的值，而BERT的位置嵌入则是模型训练得到的。（其他嵌入也都是训练得到）。\n",
    "   \n",
    "   <img src=\"ipyphoto/BERT_emb.jpg\" alt=\"Drawing\" style=\"width: 700px;\" align=\"mid\"/>\n",
    "   \n",
    "                                             \n",
    "           \n",
    "  <div align=\"center\"><i>图4： BERT嵌入层 </i></div>\n",
    "   \n",
    "   \n",
    "   \n",
    "   **二，多层BERT layer（BERT layers）**：\n",
    "   \n",
    "   这部分是BERT的核心部分，一般BERT系列的模型都会堆叠多个BERT层。首先需要知道的是，每个BERT layer ，都是不改变特征的维度的（如下图）。由于输入和输出维度一直相同，因此可以堆叠无数层。根据模型的大小，层数会有变化，BERT的base模型堆叠了12层，large模型堆叠了24层。而基础tinyBERT模型，只有4层。\n",
    "   \n",
    "   <img src=\"ipyphoto/BERT_layer.png\" alt=\"Drawing\" style=\"width: 700px;\" align=\"mid\"/>\n",
    "   \n",
    "<div align=\"center\"><i>图5： 多层BERT layer </i></div>\n",
    "\n",
    "\n",
    "   单个BERT layer是由多头自注意力层和MLP组成的。首先介绍多头自注意力层。自注意力层，有时候写作Transformer blocks，是进行特征交互，提取的关键部分。如下图所示。 图中的a1，a2与上图是对应的，指的就是一个token的特征。每一个token，都会分别经过三个不同的线性映射（也就是三个全连接），得到query,key和value(q,k,v)。然后对于每一个token，它的q会与其他token的k相乘得到一个权重，对应的 v 按照这些权重加起来，就得到了这个token的输出。\n",
    "   \n",
    "   <img src=\"ipyphoto/self_att.png\" alt=\"Drawing\" style=\"width: 700px;\" align=\"mid\"/>\n",
    "   <div align=\"center\"><i>图6： 自注意力机制 </i></div>\n",
    "   \n",
    "                                          \n",
    "   一般在模型中使用的是多头注意力机制。通过上图，我们了解到了注意力机制是如何工作的，那么多头注意力机制其实非常简单，就是将一个长维度的向量，分发到多个head中，多个token的向量在对应的head内计算输出，最后合起来。 举例说明：如下图。 一个长为6的特征 被分到3个注意力头中。每个头中，仅需长度为6/3=2的特征计算自注意力，最后得到3个长度为2的特征，再拼在一起就得到了输出。维度和输入相同，每一个token都这样操作，因此能保证输入输出的维度不变。\n",
    "      <img src=\"ipyphoto/multihead.png\" alt=\"Drawing\" style=\"width: 700px;\" align=\"mid\"/>\n",
    "      <div align=\"center\"><i> 图7： 多头自注意力 </i></div>\n",
    "   \n",
    "                                         \n",
    "   向量经过多头注意力层编码后，会经过MLP层。mlp层是两次线性映射，首先通过一个全连接从长度$L$的向量变为$L*ratio$，然后再通过一个全连接从$L*ratio$ 到$L$。当然在BERT层中也加入了残差连接的结构。\n",
    "   \n",
    "   用一个实例的参数来回顾多头注意力层：对于一个BERT-base模型，他的特征长度L为768，注意力头的个数为12.这样每个头计算的特征长度就为64. 而ratio值为4.也就是先从768映射到到3072，再从3072到768.而他的输入token数量，最大为512。\n",
    "   \n",
    "   \n",
    "   **三，输出层（pooler out）**：\n",
    "   输出前面已经提到了，与输入是一样的。 如输入是$L_{token} *L_{emb}$ 那么输出依然是$L_{token} *L_{emb}$（$L_{token}$:token数量，$L_{emb}$：特征维度）。你可以认为输入和输出是一一对应的，也可以认为他们并不对应。BERT的工作其实到这里就结束了，我们要得到的就是一个编码器而已。 \n",
    "   \n",
    "   输出层又作 池化输出。也就是将$L_{token} *L_{emb}$ 的特征池化为$1*L_{emb}$。一般常用的是将第一个token的特征作为池化输出，当然也可以采取平均池化等方式得到。\n",
    "   \n",
    "   BERT的两个预训练任务，其实也表示了bert完成生成和分类两个下游任务的一般方式。 MLM任务： 可以取遮盖token对应的输出token的特征，通过一个分类，得到输出的词，这样可以做生成任务。SEP任务： 取第一个token的特征，也就是CLStoken 进行二分类。\n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "这就是BERT的整体结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e709016a-70d8-431a-a205-32ec01a18b01",
   "metadata": {},
   "source": [
    "# tinyBERT\n",
    "tinyBERT是缩小版的BERT，与BERT的基础模型BERT-base存在着结构上的差异。此外，与BERT系列其他模型，如BERT-small，BERT-large等，不同的是，tinyBERT并没有采取直接在大规模预训练数据上无监督训练，在下游数据集微调的方法。而是对BERT-base模型进行蒸馏学习，来获取优秀的性能。\n",
    "\n",
    "\n",
    "## tinyBERT结构：\n",
    "为了介绍tinyBERT的结构，我们先来读一个BERT的设置文档BERT config，一个config便可以决定一个BERT的结构。\n",
    "     \n",
    "     tinyBERT:\n",
    "    {\n",
    "      \"hidden_size\": 384,                      #决定token被编码的长度，即特征长度，$L_{emb}$ \n",
    "\n",
    "      \"intermediate_size\": 1536,                # MLP层第一次映射的长度，这里特征长度乘以4\n",
    "\n",
    "      \"max_position_embeddings\": 512,                # 最大的输入长度。\n",
    "\n",
    "      \"model_type\": \"tiny_bert\",                \n",
    "\n",
    "      \"num_attention_heads\": 12,               # 注意力头个数\n",
    "\n",
    "      \"num_hidden_layers\": 4,                 # 堆叠多少层\n",
    "\n",
    "      \"vocab_size\": 30522                          # 训练词典个数，与训练语料有关\n",
    "\n",
    "    }\n",
    "\n",
    "    BERT-base:\n",
    "\n",
    "    {\n",
    "      \"hidden_size\": 768,           \n",
    "\n",
    "      \"intermediate_size\": 3072,\n",
    "\n",
    "      \"max_position_embeddings\": 512,\n",
    "\n",
    "      \"model_type\": \"bert\",\n",
    "\n",
    "      \"num_attention_heads\": 12,\n",
    "\n",
    "      \"num_hidden_layers\": 12,\n",
    "\n",
    "      \"vocab_size\": 30522\n",
    "\n",
    "    }\n",
    "BERT模型的结构主要由上面这些参数决定。其中上方是tinyBERT,下方是BERT-base。我们可以看到他们结构上的不同之处。参数的具体意思，可以参考上面的BERT结构。 \n",
    "    在tinyBERT中，首先是词被编码的特征维度减少一半，变为384。对应的mlp层的映射层维度也减少一半（保持四倍）。 BERTlayer变为4层。注意力头个数不变，这样每个注意力头中的特征自然也会减少至一半。\n",
    "    这样tinyBERT自然会参数减少很多。\n",
    "\n",
    "## 蒸馏学习：\n",
    "\n",
    "  参数量的降低，一般就会带来性能的降低。为了拥有良好的性能，tinyBERT并不像BERT家族那样在无标注上预训练，而是采取了蒸馏学习的方式进行训练。通过对BERT-base的蒸馏，得到了很好的性能。简单介绍一下蒸馏学习。 在一般的蒸馏学习中，有一个teacher模型和一个student模型。通过让student模型的输出去模仿teacher模型的输出，即让他们的输出靠的更近，来对student模型进行训练。\n",
    "  \n",
    "  与一般的蒸馏不同的是，tinyBERT是在模型的前向过程中进行多次蒸馏。而且在整个训练过程也进行多次蒸馏。\n",
    "  \n",
    "### 前向过程中的多次蒸馏：\n",
    "    \n",
    "   tinyBERT，对于n层的teacher bert，设计了一个mapping function ：n = g ( m )， 将student bert的第m层映射为原来的teacher的第n层，即让tinyBERT的第m层的输出去靠近teacher bert第n层的输出。 其实这个映射函数非常简单，就是$n = k*m$。k就是多少层当作tinyBERT的一层。当m=0时，对应的就是embedding layer。我们可以通过下图理解。图中仅为示例，tinyBERT每层的输出都去蒸馏学习Teacher net三层的输出，就是“一层顶三层”。\n",
    "   \n",
    "   <img src=\"ipyphoto/zhengliu.png\" alt=\"Drawing\" style=\"width: 700px;\" align=\"mid\"/>\n",
    "     <div align=\"center\"><i> 图8： 蒸馏对应层 </i></div>\n",
    "   \n",
    "   实际上的BERT-base有12层， 对于4层的tinyBERT，正好是三层对一层。 对于蒸馏学习，我们需要根据两个模型对应层的输出来计算loss，更新模型。从上图中，我们可以看到一共有四种loss，下面分别介绍。\n",
    "   \n",
    "   \n",
    "-   **Embedding-layer distillation**\n",
    "\n",
    "   <img src=\"ipyphoto/l_emb.png\" alt=\"Drawing\" style=\"width: 700px;\" align=\"mid\"/>\n",
    "   \n",
    "  这个是对embedding 矩阵的蒸馏loss，说是矩阵，其实是计算两个模型embedding输出的MSEloss。 而因为student的embedding层的特征维度和Teacher是不一样的，因此要乘上一个转换的映射矩阵$W_e$，此矩阵在模型中是一层全连接，在训练时学习。\n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "-   **Attention based distillation and Hidden states based distillation**\n",
    "\n",
    "\n",
    "\n",
    "  前文我们提到，BERT layer每一层包含两部分，一个是自注意力层，一个是MLP（也就是两层全连接）。attention指的是注意力层注意力分数矩阵，也就是对q和k乘算出来的那个值的蒸馏学习。 下面的hidden states层的蒸馏就是指的对MLP层的输出进行蒸馏学习。\n",
    "\n",
    " <img src=\"ipyphoto/l_hid.png\" alt=\"Drawing\" style=\"width: 700px;\" align=\"mid\"/>\n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "-   **Prediction-layer distillation**\n",
    "\n",
    "\n",
    "<img src=\"ipyphoto/l_pre.png\" alt=\"Drawing\" style=\"width: 700px;\" align=\"mid\"/>\n",
    "\n",
    "  pred蒸馏，是最初的蒸馏方法。是对最终输出层输出结果的softmax进行蒸馏学习。T是蒸馏学习中的温度系数。对输出蒸馏时，采用的是带温度系数的交叉熵loss。\n",
    "   \n",
    " \n",
    " \n",
    " ### 训练过程中的多次蒸馏：\n",
    " \n",
    " <img src=\"ipyphoto/train.png\" alt=\"Drawing\" style=\"width: 700px;\" align=\"mid\"/>\n",
    "    <div align=\"center\"><i> tinyBERT训练过程 </i></div>\n",
    "     \n",
    " tinyBERT 并不是像其他蒸馏那样，直接根据成品的Teacher model在分类时蒸馏，而是去模型BERT的训练过程，在预训练和微调阶段都进行蒸馏。我们知道BERT模型使用时要经过预训练和下游任务微调。所以tinyBERT的蒸馏同样分为两步：General Distillation 与 Task-specific Distillation。前者是对BERT在大规模语料库进行预训练蒸馏学习，后者则是在特定的任务上进行蒸馏学习。值得注意的是，在预训练蒸馏阶段，使用的Teacher模型是仅仅经过预训练未微调的BERT，而在特定任务分类蒸馏训练阶段，使用的Teacher 模型是在特定任务上经过微调的BERT。  \n",
    "\n",
    "\n",
    "\n",
    "## 总结：\n",
    "\n",
    "这一部分介绍了tinyBERT结构和训练的过程。可以从参数设置部分看出tinyBERT是如何缩减参数量的，又可以从蒸馏学习部分看出tinyBERT是如何在参数量大大减少的同时保持优秀的性能。下面我们将通过具体的代码对tinyBERT的结构和训练过程进行更加详细的介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3046ca04-2388-4f7b-8922-0b374ad22c4d",
   "metadata": {},
   "source": [
    "# **训练数据准备**\n",
    "\n",
    "BERT是在大规模无标注数据上进行预训练的，这里采用的是在wiki上下载文章进行预训练。 ms提供了处理这些数据文件的接口，但我们要把数据转为tfrecord格式或者是ms格式。 \n",
    "\n",
    "https://mp.csdn.net/mp_blog/creation/editor/127061849\n",
    "\n",
    "\n",
    "## 生成通用蒸馏阶段数据集，即wiki数据。\n",
    "\n",
    "下载[wiki](http://t.zoukankan.com/dhName-p-11859318.html)数据集进行预训练，\n",
    "\n",
    "使用[WikiExtractor](https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fattardi%2Fwikiextractor)提取和整理数据集中的文本，使用步骤如下：\n",
    "\n",
    "pip install wikiextractor\n",
    "python -m wikiextractor.WikiExtractor [Wikipedia dump file] -o [output file path] -b 2G\n",
    "下载[BERT](https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fgoogle-research%2Fbert)代码仓，并下载模型文件[BERT-Base, Uncased](https://gitee.com/link?target=https%3A%2F%2Fstorage.googleapis.com%2Fbert_models%2F2018_10_18%2Funcased_L-12_H-768_A-12.zip)，其中包含了转化需要使用的vocab.txt, bert_config.json和预训练模型\n",
    "\n",
    "使用create_pretraining_data.py文件，将下载得到的文件转化成tfrecord数据集，详细用法请参考bert的readme文件，其中input_file第2步会生成多个文本文件，请转化为bert0.tfrecord-bertx.tfrecord，如果出现AttributeError: module 'tokenization' has no attribute 'FullTokenizer’，请安装bert-tensorflow\n",
    "\n",
    "将下载得到的tensorflow模型转化为mindspore模型，注意这个需要环境中同时存在tensorflow 和 mindspore。PATH为bert模型存放位置。\n",
    "\n",
    "```python3\n",
    "cd bert/ms2tf\n",
    "python ms_and_tf_checkpoint_transfer_tools.py --tf_ckpt_path=PATH/model.ckpt　\\\n",
    "    --new_ckpt_path=PATH/ms_model_ckpt.ckpt　\\\n",
    "    --tarnsfer_option=tf2ms\n",
    "```\n",
    "\n",
    "## 生成下游任务蒸馏阶段数据集\n",
    "\n",
    "下载数据集进行微调和评估，如GLUE，使用download_glue_data.py脚本下载SST2, MNLI, QNLI数据集等。后面的训练是以QNLI为例。\n",
    "\n",
    "将数据集文件从JSON格式转换为TFRecord格式。使用通用蒸馏阶段的第三步BERT代码，在处理QNLI数据时，需要加入QNLI数据集的处理代码。参考readme使用代码仓中的run_classifier.py文件。  run_classifier.py代码中包含了训练，推理和预测的代码，对于转化tfrecord数据集来说，这部分代码是多余的，可以将这部分代码注释掉，只保留转化数据集的代码．其中task_name指定为QNLI，bert_config_file指定为通用蒸馏阶段下载得到的bert_config.json文件，max_seq_length为64. 更详细的下载转换方式见 [数据下载转换指南](https://mp.csdn.net/mp_blog/creation/editor/127061849)\n",
    "\n",
    "~~~python3\n",
    "...\n",
    "class QnliProcessor(DataProcessor):\n",
    "\"\"\"Processor for the QNLI data set (GLUE version).\"\"\"\n",
    "\n",
    "def get_train_examples(self, data_dir):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return self._create_examples(\n",
    "        self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "def get_dev_examples(self, data_dir):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return self._create_examples(\n",
    "        self._read_tsv(os.path.join(data_dir, \"dev.tsv\")),\n",
    "                       \"dev_matched\")\n",
    "\n",
    "def get_labels(self):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return [\"entailment\", \"not_entailment\"]\n",
    "\n",
    "def _create_examples(self, lines, set_type):\n",
    "    \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "    examples = []\n",
    "    for (i, line) in enumerate(lines):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        guid = \"%s-%s\" % (set_type, line[0])\n",
    "        text_a = line[1]\n",
    "        text_b = line[2]\n",
    "        label = line[-1]\n",
    "        examples.append(\n",
    "            InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "    return examples\n",
    "...\n",
    "\"qnli\": QnliProcessor,\n",
    "...\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdd2cc8-34a7-4b4a-b6a8-b13f6218bab2",
   "metadata": {},
   "source": [
    "# 代码部分\n",
    "    \n",
    "## 导入环境\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd779868-d3d2-4502-9614-e277da9b8487",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import mindspore.nn as nn\n",
    "from mindspore import context\n",
    "from mindspore.ops import functional as F\n",
    "from mindspore.ops import composite as C\n",
    "from mindspore.common.parameter import Parameter\n",
    "from mindspore.communication.management import get_group_size\n",
    "from mindspore.nn.wrap.grad_reducer import DistributedGradReducer\n",
    "from mindspore.context import ParallelMode\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "from mindspore import Tensor\n",
    "from mindspore.train.callback import Callback\n",
    "from mindspore.train.serialization import save_checkpoint\n",
    "from mindspore.ops import operations as P\n",
    "from mindspore.nn.learning_rate_schedule import LearningRateSchedule, PolynomialDecayLR, WarmUpLR\n",
    "import mindspore.common.dtype as mstype\n",
    "from mindspore.common.initializer import TruncatedNormal, initializer\n",
    "import mindspore.dataset as ds\n",
    "from mindspore.dataset import transforms\n",
    "from mindspore.common import set_seed\n",
    "from mindspore.nn.optim import AdamWeightDecay\n",
    "from mindspore.train.model import Model\n",
    "from mindspore.train.callback import TimeMonitor\n",
    "from mindspore.nn.wrap.loss_scale import DynamicLossScaleUpdateCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40fe0762-b753-4c2e-b9fc-37f3919f5a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import math\n",
    "import copy\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import ast\n",
    "from pprint import pformat\n",
    "import yaml\n",
    "import argparse\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a491b2c-9669-4d84-b032-4bf021620340",
   "metadata": {},
   "source": [
    "# 模型代码部分\n",
    "\n",
    "这一部分，会定义所有使用到的模型代码。看代码时，可以对照上面对bert模型的结构介绍来看。\n",
    "\n",
    "\n",
    "## BERT config\n",
    "BERT的config是模型构建的依据所在。规定了模型的整体结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "277c304f-6dbf-47f5-bec0-6a001fce3c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertConfig:\n",
    "    \"\"\"\n",
    "    Configuration for `BertModel`.\n",
    "\n",
    "    Args:\n",
    "        seq_length (int): Length of input sequence. Default: 128.\n",
    "        vocab_size (int): The shape of each embedding vector. Default: 32000.\n",
    "        hidden_size (int): Size of the bert encoder layers. Default: 768.\n",
    "        num_hidden_layers (int): Number of hidden layers in the BertTransformer encoder\n",
    "                           cell. Default: 12.\n",
    "        num_attention_heads (int): Number of attention heads in the BertTransformer\n",
    "                             encoder cell. Default: 12.\n",
    "        intermediate_size (int): Size of intermediate layer in the BertTransformer\n",
    "                           encoder cell. Default: 3072.\n",
    "        hidden_act (str): Activation function used in the BertTransformer encoder\n",
    "                    cell. Default: \"gelu\".\n",
    "        hidden_dropout_prob (float): The dropout probability for BertOutput. Default: 0.1.\n",
    "        attention_probs_dropout_prob (float): The dropout probability for\n",
    "                                      BertAttention. Default: 0.1.\n",
    "        max_position_embeddings (int): Maximum length of sequences used in this\n",
    "                                 model. Default: 512.\n",
    "        type_vocab_size (int): Size of token type vocab. Default: 16.\n",
    "        initializer_range (float): Initialization value of TruncatedNormal. Default: 0.02.\n",
    "        use_relative_positions (bool): Specifies whether to use relative positions. Default: False.\n",
    "        dtype (:class:`mindspore.dtype`): Data type of the input. Default: mstype.float32.\n",
    "        compute_type (:class:`mindspore.dtype`): Compute type in BertTransformer. Default: mstype.float32.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 seq_length=128,\n",
    "                 vocab_size=32000,\n",
    "                 hidden_size=768,\n",
    "                 num_hidden_layers=12,\n",
    "                 num_attention_heads=12,\n",
    "                 intermediate_size=3072,\n",
    "                 hidden_act=\"gelu\",\n",
    "                 hidden_dropout_prob=0.1,\n",
    "                 attention_probs_dropout_prob=0.1,\n",
    "                 max_position_embeddings=512,\n",
    "                 type_vocab_size=16,\n",
    "                 initializer_range=0.02,\n",
    "                 use_relative_positions=False,\n",
    "                 dtype=mstype.float32,\n",
    "                 compute_type=mstype.float32):\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.hidden_act = hidden_act\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.type_vocab_size = type_vocab_size\n",
    "        self.initializer_range = initializer_range\n",
    "        self.use_relative_positions = use_relative_positions\n",
    "        self.dtype = dtype\n",
    "        self.compute_type = compute_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d5c48c-0ca3-4181-aecd-51144dd47c0c",
   "metadata": {},
   "source": [
    "## BERT模型。\n",
    "bert的整体模型可以对照图片和代码。\n",
    "\n",
    "<img src=\"ipyphoto/BERT.png\" alt=\"Drawing\" style=\"width: 400px;\" align=\"mid\"/>\n",
    "\n",
    "对应代码中是，\n",
    "- EmbeddingPostprocessor \n",
    "- BertTransformer\n",
    "- pooler out\n",
    "\n",
    "但是ms官方的代码中，并没有严格按照这个来行。 输入的embedding和 pooler 都放在主体部分进行，而 位置和句子的embedding放在EmbeddingPostprocessor中进行。bert layers 在BertTransformer中。\n",
    "\n",
    "## 位置和句子embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "839d40d7-866e-4c81-a765-042e8e5c256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingPostprocessor(nn.Cell):\n",
    "    \"\"\"\n",
    "    Postprocessors apply positional and token type embeddings to word embeddings.\n",
    "\n",
    "    Args:\n",
    "        embedding_size (int): The size of each embedding vector.\n",
    "        embedding_shape (list): [batch_size, seq_length, embedding_size], the shape of\n",
    "                         each embedding vector.\n",
    "        use_token_type (bool): Specifies whether to use token type embeddings. Default: False.\n",
    "        token_type_vocab_size (int): Size of token type vocab. Default: 16.\n",
    "       use_one_hot_embeddings (bool): Specifies whether to use one hot encoding form. Default: False.\n",
    "        initializer_range (float): Initialization value of TruncatedNormal. Default: 0.02.\n",
    "        max_position_embeddings (int): Maximum length of sequences used in this\n",
    "                                 model. Default: 512.\n",
    "        dropout_prob (float): The dropout probability. Default: 0.1.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 use_relative_positions,\n",
    "                 embedding_size,\n",
    "                 embedding_shape,\n",
    "                 use_token_type=False,\n",
    "                 token_type_vocab_size=16,\n",
    "                 use_one_hot_embeddings=False,\n",
    "                 initializer_range=0.02,\n",
    "                 max_position_embeddings=512,\n",
    "                 dropout_prob=0.1):\n",
    "        super(EmbeddingPostprocessor, self).__init__()\n",
    "        self.use_token_type = use_token_type\n",
    "        self.token_type_vocab_size = token_type_vocab_size\n",
    "        self.use_one_hot_embeddings = use_one_hot_embeddings\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.token_type_embedding = nn.Embedding(\n",
    "            vocab_size=token_type_vocab_size,\n",
    "            embedding_size=embedding_size,\n",
    "            use_one_hot=use_one_hot_embeddings)\n",
    "        self.shape_flat = (-1,)\n",
    "        self.one_hot = P.OneHot()\n",
    "        self.on_value = Tensor(1.0, mstype.float32)\n",
    "        self.off_value = Tensor(0.1, mstype.float32)\n",
    "        self.array_mul = P.MatMul()\n",
    "        self.reshape = P.Reshape()\n",
    "        self.shape = tuple(embedding_shape)\n",
    "        self.dropout = nn.Dropout(1 - dropout_prob)\n",
    "        self.gather = P.Gather()\n",
    "        self.use_relative_positions = use_relative_positions\n",
    "        self.slice = P.StridedSlice()\n",
    "        _, seq, _ = self.shape\n",
    "        self.full_position_embedding = nn.Embedding(\n",
    "            vocab_size=max_position_embeddings,\n",
    "            embedding_size=embedding_size,\n",
    "            use_one_hot=False)\n",
    "        self.layernorm = nn.LayerNorm((embedding_size,))\n",
    "        self.position_ids = Tensor(np.arange(seq).reshape(-1, seq).astype(np.int32))\n",
    "        self.add = P.Add()\n",
    "\n",
    "    def construct(self, token_type_ids, word_embeddings):\n",
    "        \"\"\"Postprocessors apply positional and token type embeddings to word embeddings.\"\"\"\n",
    "        output = word_embeddings\n",
    "        if self.use_token_type:\n",
    "            token_type_embeddings = self.token_type_embedding(token_type_ids)\n",
    "            output = self.add(output, token_type_embeddings)\n",
    "        if not self.use_relative_positions:\n",
    "            position_embeddings = self.full_position_embedding(self.position_ids)\n",
    "            output = self.add(output, position_embeddings)\n",
    "        output = self.layernorm(output)\n",
    "        output = self.dropout(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1925014-1c3b-4bdf-b911-afa4506d02dc",
   "metadata": {},
   "source": [
    "下面是一些关于相对位置和格式转换的代码。 我们在这里不用相对位置编码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31da4299-2775-4dde-9a52-b3f6b1aac379",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RelaPosMatrixGenerator(nn.Cell):\n",
    "    \"\"\"\n",
    "    Generates matrix of relative positions between inputs.\n",
    "\n",
    "    Args:\n",
    "        length (int): Length of one dim for the matrix to be generated.\n",
    "        max_relative_position (int): Max value of relative position.\n",
    "    \"\"\"\n",
    "    def __init__(self, length, max_relative_position):\n",
    "        super(RelaPosMatrixGenerator, self).__init__()\n",
    "        self._length = length\n",
    "        self._max_relative_position = Tensor(max_relative_position, dtype=mstype.int32)\n",
    "        self._min_relative_position = Tensor(-max_relative_position, dtype=mstype.int32)\n",
    "        self.range_length = -length + 1\n",
    "        self.tile = P.Tile()\n",
    "        self.range_mat = P.Reshape()\n",
    "        self.sub = P.Sub()\n",
    "        self.expanddims = P.ExpandDims()\n",
    "        self.cast = P.Cast()\n",
    "\n",
    "    def construct(self):\n",
    "        \"\"\"position matrix generator\"\"\"\n",
    "        range_vec_row_out = self.cast(F.tuple_to_array(F.make_range(self._length)), mstype.int32)\n",
    "        range_vec_col_out = self.range_mat(range_vec_row_out, (self._length, -1))\n",
    "        tile_row_out = self.tile(range_vec_row_out, (self._length,))\n",
    "        tile_col_out = self.tile(range_vec_col_out, (1, self._length))\n",
    "        range_mat_out = self.range_mat(tile_row_out, (self._length, self._length))\n",
    "        transpose_out = self.range_mat(tile_col_out, (self._length, self._length))\n",
    "        distance_mat = self.sub(range_mat_out, transpose_out)\n",
    "        distance_mat_clipped = C.clip_by_value(distance_mat,\n",
    "                                               self._min_relative_position,\n",
    "                                               self._max_relative_position)\n",
    "        # Shift values to be >=0. Each integer still uniquely identifies a\n",
    "        # relative position difference.\n",
    "        final_mat = distance_mat_clipped + self._max_relative_position\n",
    "        return final_mat\n",
    "\n",
    "\n",
    "class RelaPosEmbeddingsGenerator(nn.Cell):\n",
    "    \"\"\"\n",
    "    Generates tensor of size [length, length, depth].\n",
    "\n",
    "    Args:\n",
    "        length (int): Length of one dim for the matrix to be generated.\n",
    "        depth (int): Size of each attention head.\n",
    "        max_relative_position (int): Maxmum value of relative position.\n",
    "        initializer_range (float): Initialization value of TruncatedNormal.\n",
    "        use_one_hot_embeddings (bool): Specifies whether to use one hot encoding form. Default: False.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 length,\n",
    "                 depth,\n",
    "                 max_relative_position,\n",
    "                 initializer_range,\n",
    "                 use_one_hot_embeddings=False):\n",
    "        super(RelaPosEmbeddingsGenerator, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.vocab_size = max_relative_position * 2 + 1\n",
    "        self.use_one_hot_embeddings = use_one_hot_embeddings\n",
    "        self.embeddings_table = Parameter(\n",
    "            initializer(TruncatedNormal(initializer_range),\n",
    "                        [self.vocab_size, self.depth]))\n",
    "        self.relative_positions_matrix = RelaPosMatrixGenerator(length=length,\n",
    "                                                                max_relative_position=max_relative_position)\n",
    "        self.reshape = P.Reshape()\n",
    "        self.one_hot = P.OneHot()\n",
    "        self.on_value = Tensor(1.0, mstype.float32)\n",
    "        self.off_value = Tensor(0.0, mstype.float32)\n",
    "        self.shape = P.Shape()\n",
    "        self.gather = P.Gather()  # index_select\n",
    "        self.matmul = P.BatchMatMul()\n",
    "\n",
    "    def construct(self):\n",
    "        \"\"\"position embedding generation\"\"\"\n",
    "        relative_positions_matrix_out = self.relative_positions_matrix()\n",
    "        # Generate embedding for each relative position of dimension depth.\n",
    "        if self.use_one_hot_embeddings:\n",
    "            flat_relative_positions_matrix = self.reshape(relative_positions_matrix_out, (-1,))\n",
    "            one_hot_relative_positions_matrix = self.one_hot(\n",
    "                flat_relative_positions_matrix, self.vocab_size, self.on_value, self.off_value)\n",
    "            embeddings = self.matmul(one_hot_relative_positions_matrix, self.embeddings_table)\n",
    "            my_shape = self.shape(relative_positions_matrix_out) + (self.depth,)\n",
    "            embeddings = self.reshape(embeddings, my_shape)\n",
    "        else:\n",
    "            embeddings = self.gather(self.embeddings_table,\n",
    "                                     relative_positions_matrix_out, 0)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class SaturateCast(nn.Cell):\n",
    "    \"\"\"\n",
    "    Performs a safe saturating cast. This operation applies proper clamping before casting to prevent\n",
    "    the danger that the value will overflow or underflow.\n",
    "\n",
    "    Args:\n",
    "        src_type (:class:`mindspore.dtype`): The type of the elements of the input tensor. Default: mstype.float32.\n",
    "        dst_type (:class:`mindspore.dtype`): The type of the elements of the output tensor. Default: mstype.float32.\n",
    "    \"\"\"\n",
    "    def __init__(self, src_type=mstype.float32, dst_type=mstype.float32):\n",
    "        super(SaturateCast, self).__init__()\n",
    "        np_type = mstype.dtype_to_nptype(dst_type)\n",
    "        min_type = np.finfo(np_type).min\n",
    "        max_type = np.finfo(np_type).max\n",
    "        self.tensor_min_type = Tensor([min_type], dtype=src_type)\n",
    "        self.tensor_max_type = Tensor([max_type], dtype=src_type)\n",
    "        self.min_op = P.Minimum()\n",
    "        self.max_op = P.Maximum()\n",
    "        self.cast = P.Cast()\n",
    "        self.dst_type = dst_type\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"saturate cast\"\"\"\n",
    "        out = self.max_op(x, self.tensor_min_type)\n",
    "        out = self.min_op(out, self.tensor_max_type)\n",
    "        return self.cast(out, self.dst_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba23030c-6262-44a3-b42c-cd5f65c8abfb",
   "metadata": {},
   "source": [
    "## BERT layer\n",
    "bert layer（BertEncoderCell） 主要由  BertSelfAttention 与 BertOutput 即自注意力层和线性映射层组成。\n",
    "\n",
    "而BertSelfAttention 主要包含 BertAttention 与 BertOutput 即自注意力计算和线性映射。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b08ca2ca-a846-41c3-ad48-718e51b312ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertOutput(nn.Cell):\n",
    "    \"\"\"\n",
    "    Apply a linear computation to hidden status and a residual computation to input.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Input channels.\n",
    "        out_channels (int): Output channels.\n",
    "        initializer_range (float): Initialization value of TruncatedNormal. Default: 0.02.\n",
    "        dropout_prob (float): The dropout probability. Default: 0.1.\n",
    "        compute_type (:class:`mindspore.dtype`): Compute type in BertTransformer. Default: mstype.float32.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 initializer_range=0.02,\n",
    "                 dropout_prob=0.1,\n",
    "                 compute_type=mstype.float32):\n",
    "        super(BertOutput, self).__init__()\n",
    "        self.dense = nn.Dense(in_channels, out_channels,\n",
    "                              weight_init=TruncatedNormal(initializer_range)).to_float(compute_type)\n",
    "        self.dropout = nn.Dropout(1 - dropout_prob)\n",
    "        self.add = P.Add()\n",
    "        self.is_gpu = context.get_context('device_target') == \"GPU\"\n",
    "        if self.is_gpu:\n",
    "            self.layernorm = nn.LayerNorm((out_channels,)).to_float(mstype.float32)\n",
    "            self.compute_type = compute_type\n",
    "        else:\n",
    "            self.layernorm = nn.LayerNorm((out_channels,)).to_float(compute_type)\n",
    "\n",
    "        self.cast = P.Cast()\n",
    "\n",
    "    def construct(self, hidden_status, input_tensor):\n",
    "        \"\"\"bert output\"\"\"\n",
    "        output = self.dense(hidden_status)\n",
    "        output = self.dropout(output)\n",
    "        output = self.add(input_tensor, output)\n",
    "        output = self.layernorm(output)\n",
    "        if self.is_gpu:\n",
    "            output = self.cast(output, self.compute_type)\n",
    "        return output\n",
    "\n",
    "    \n",
    "class BertAttention(nn.Cell):\n",
    "    \"\"\"\n",
    "    Apply multi-headed attention from \"from_tensor\" to \"to_tensor\".\n",
    "\n",
    "    Args:\n",
    "        from_tensor_width (int): Size of last dim of from_tensor.\n",
    "        to_tensor_width (int): Size of last dim of to_tensor.\n",
    "        from_seq_length (int): Length of from_tensor sequence.\n",
    "        to_seq_length (int): Length of to_tensor sequence.\n",
    "        num_attention_heads (int): Number of attention heads. Default: 1.\n",
    "        size_per_head (int): Size of each attention head. Default: 512.\n",
    "        query_act (str): Activation function for the query transform. Default: None.\n",
    "        key_act (str): Activation function for the key transform. Default: None.\n",
    "        value_act (str): Activation function for the value transform. Default: None.\n",
    "        has_attention_mask (bool): Specifies whether to use attention mask. Default: False.\n",
    "        attention_probs_dropout_prob (float): The dropout probability for\n",
    "                                      BertAttention. Default: 0.0.\n",
    "        use_one_hot_embeddings (bool): Specifies whether to use one hot encoding form. Default: False.\n",
    "        initializer_range (float): Initialization value of TruncatedNormal. Default: 0.02.\n",
    "        do_return_2d_tensor (bool): True for return 2d tensor. False for return 3d\n",
    "                             tensor. Default: False.\n",
    "        use_relative_positions (bool): Specifies whether to use relative positions. Default: False.\n",
    "        compute_type (:class:`mindspore.dtype`): Compute type in BertAttention. Default: mstype.float32.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 from_tensor_width,\n",
    "                 to_tensor_width,\n",
    "                 from_seq_length,\n",
    "                 to_seq_length,\n",
    "                 num_attention_heads=1,\n",
    "                 size_per_head=512,\n",
    "                 query_act=None,\n",
    "                 key_act=None,\n",
    "                 value_act=None,\n",
    "                 has_attention_mask=False,\n",
    "                 attention_probs_dropout_prob=0.0,\n",
    "                 use_one_hot_embeddings=False,\n",
    "                 initializer_range=0.02,\n",
    "                 do_return_2d_tensor=False,\n",
    "                 use_relative_positions=False,\n",
    "                 compute_type=mstype.float32):\n",
    "        super(BertAttention, self).__init__()\n",
    "        self.from_seq_length = from_seq_length\n",
    "        self.to_seq_length = to_seq_length\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.size_per_head = size_per_head\n",
    "        self.has_attention_mask = has_attention_mask\n",
    "        self.use_relative_positions = use_relative_positions\n",
    "        self.scores_mul = Tensor([1.0 / math.sqrt(float(self.size_per_head))], dtype=compute_type)\n",
    "        self.reshape = P.Reshape()\n",
    "        self.shape_from_2d = (-1, from_tensor_width)\n",
    "        self.shape_to_2d = (-1, to_tensor_width)\n",
    "        weight = TruncatedNormal(initializer_range)\n",
    "        units = num_attention_heads * size_per_head\n",
    "        self.query_layer = nn.Dense(from_tensor_width,\n",
    "                                    units,\n",
    "                                    activation=query_act,\n",
    "                                    weight_init=weight).to_float(compute_type)\n",
    "        self.key_layer = nn.Dense(to_tensor_width,\n",
    "                                  units,\n",
    "                                  activation=key_act,\n",
    "                                  weight_init=weight).to_float(compute_type)\n",
    "        self.value_layer = nn.Dense(to_tensor_width,\n",
    "                                    units,\n",
    "                                    activation=value_act,\n",
    "                                    weight_init=weight).to_float(compute_type)\n",
    "        self.shape_from = (-1, from_seq_length, num_attention_heads, size_per_head)\n",
    "        self.shape_to = (-1, to_seq_length, num_attention_heads, size_per_head)\n",
    "        self.matmul_trans_b = P.BatchMatMul(transpose_b=True)\n",
    "        self.multiply = P.Mul()\n",
    "        self.transpose = P.Transpose()\n",
    "        self.trans_shape = (0, 2, 1, 3)\n",
    "        self.trans_shape_relative = (2, 0, 1, 3)\n",
    "        self.trans_shape_position = (1, 2, 0, 3)\n",
    "        self.multiply_data = Tensor([-10000.0,], dtype=compute_type)\n",
    "        self.matmul = P.BatchMatMul()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.dropout = nn.Dropout(1 - attention_probs_dropout_prob)\n",
    "        if self.has_attention_mask:\n",
    "            self.expand_dims = P.ExpandDims()\n",
    "            self.sub = P.Sub()\n",
    "            self.add = P.Add()\n",
    "            self.cast = P.Cast()\n",
    "            self.get_dtype = P.DType()\n",
    "        if do_return_2d_tensor:\n",
    "            self.shape_return = (-1, num_attention_heads * size_per_head)\n",
    "        else:\n",
    "            self.shape_return = (-1, from_seq_length, num_attention_heads * size_per_head)\n",
    "        self.cast_compute_type = SaturateCast(dst_type=compute_type)\n",
    "        if self.use_relative_positions:\n",
    "            self._generate_relative_positions_embeddings = \\\n",
    "                RelaPosEmbeddingsGenerator(length=to_seq_length,\n",
    "                                           depth=size_per_head,\n",
    "                                           max_relative_position=16,\n",
    "                                           initializer_range=initializer_range,\n",
    "                                           use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "\n",
    "    def construct(self, from_tensor, to_tensor, attention_mask):\n",
    "        \"\"\"bert attention\"\"\"\n",
    "        # reshape 2d/3d input tensors to 2d\n",
    "        from_tensor_2d = self.reshape(from_tensor, self.shape_from_2d)\n",
    "        to_tensor_2d = self.reshape(to_tensor, self.shape_to_2d)\n",
    "        query_out = self.query_layer(from_tensor_2d)\n",
    "        key_out = self.key_layer(to_tensor_2d)\n",
    "        value_out = self.value_layer(to_tensor_2d)\n",
    "        query_layer = self.reshape(query_out, self.shape_from)\n",
    "        query_layer = self.transpose(query_layer, self.trans_shape)\n",
    "        key_layer = self.reshape(key_out, self.shape_to)\n",
    "        key_layer = self.transpose(key_layer, self.trans_shape)\n",
    "        attention_scores = self.matmul_trans_b(query_layer, key_layer)\n",
    "        # use_relative_position, supplementary logic\n",
    "        if self.use_relative_positions:\n",
    "            # relations_keys is [F|T, F|T, H]\n",
    "            relations_keys = self._generate_relative_positions_embeddings()\n",
    "            relations_keys = self.cast_compute_type(relations_keys)\n",
    "            # query_layer_t is [F, B, N, H]\n",
    "            query_layer_t = self.transpose(query_layer, self.trans_shape_relative)\n",
    "            # query_layer_r is [F, B * N, H]\n",
    "            query_layer_r = self.reshape(query_layer_t,\n",
    "                                         (self.from_seq_length,\n",
    "                                          -1,\n",
    "                                          self.size_per_head))\n",
    "            # key_position_scores is [F, B * N, F|T]\n",
    "            key_position_scores = self.matmul_trans_b(query_layer_r,\n",
    "                                                      relations_keys)\n",
    "            # key_position_scores_r is [F, B, N, F|T]\n",
    "            key_position_scores_r = self.reshape(key_position_scores,\n",
    "                                                 (self.from_seq_length,\n",
    "                                                  -1,\n",
    "                                                  self.num_attention_heads,\n",
    "                                                  self.from_seq_length))\n",
    "            # key_position_scores_r_t is [B, N, F, F|T]\n",
    "            key_position_scores_r_t = self.transpose(key_position_scores_r,\n",
    "                                                     self.trans_shape_position)\n",
    "            attention_scores = attention_scores + key_position_scores_r_t\n",
    "        attention_scores = self.multiply(self.scores_mul, attention_scores)\n",
    "        if self.has_attention_mask:\n",
    "            attention_mask = self.expand_dims(attention_mask, 1)\n",
    "            multiply_out = self.sub(self.cast(F.tuple_to_array((1.0,)), self.get_dtype(attention_scores)),\n",
    "                                    self.cast(attention_mask, self.get_dtype(attention_scores)))\n",
    "            adder = self.multiply(multiply_out, self.multiply_data)\n",
    "            attention_scores = self.add(adder, attention_scores)\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "        value_layer = self.reshape(value_out, self.shape_to)\n",
    "        value_layer = self.transpose(value_layer, self.trans_shape)\n",
    "        context_layer = self.matmul(attention_probs, value_layer)\n",
    "        # use_relative_position, supplementary logic\n",
    "        if self.use_relative_positions:\n",
    "            # relations_values is [F|T, F|T, H]\n",
    "            relations_values = self._generate_relative_positions_embeddings()\n",
    "            relations_values = self.cast_compute_type(relations_values)\n",
    "            # attention_probs_t is [F, B, N, T]\n",
    "            attention_probs_t = self.transpose(attention_probs, self.trans_shape_relative)\n",
    "            # attention_probs_r is [F, B * N, T]\n",
    "            attention_probs_r = self.reshape(\n",
    "                attention_probs_t,\n",
    "                (self.from_seq_length,\n",
    "                 -1,\n",
    "                 self.to_seq_length))\n",
    "            # value_position_scores is [F, B * N, H]\n",
    "            value_position_scores = self.matmul(attention_probs_r,\n",
    "                                                relations_values)\n",
    "            # value_position_scores_r is [F, B, N, H]\n",
    "            value_position_scores_r = self.reshape(value_position_scores,\n",
    "                                                   (self.from_seq_length,\n",
    "                                                    -1,\n",
    "                                                    self.num_attention_heads,\n",
    "                                                    self.size_per_head))\n",
    "            # value_position_scores_r_t is [B, N, F, H]\n",
    "            value_position_scores_r_t = self.transpose(value_position_scores_r,\n",
    "                                                       self.trans_shape_position)\n",
    "            context_layer = context_layer + value_position_scores_r_t\n",
    "        context_layer = self.transpose(context_layer, self.trans_shape)\n",
    "        context_layer = self.reshape(context_layer, self.shape_return)\n",
    "        return context_layer, attention_scores\n",
    "\n",
    "class BertSelfAttention(nn.Cell):\n",
    "    \"\"\"\n",
    "    Apply self-attention.\n",
    "\n",
    "    Args:\n",
    "        seq_length (int): Length of input sequence.\n",
    "        hidden_size (int): Size of the bert encoder layers.\n",
    "        num_attention_heads (int): Number of attention heads. Default: 12.\n",
    "        attention_probs_dropout_prob (float): The dropout probability for\n",
    "                                      BertAttention. Default: 0.1.\n",
    "        use_one_hot_embeddings (bool): Specifies whether to use one_hot encoding form. Default: False.\n",
    "        initializer_range (float): Initialization value of TruncatedNormal. Default: 0.02.\n",
    "        hidden_dropout_prob (float): The dropout probability for BertOutput. Default: 0.1.\n",
    "        use_relative_positions (bool): Specifies whether to use relative positions. Default: False.\n",
    "        compute_type (:class:`mindspore.dtype`): Compute type in BertSelfAttention. Default: mstype.float32.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 seq_length,\n",
    "                 hidden_size,\n",
    "                 num_attention_heads=12,\n",
    "                 attention_probs_dropout_prob=0.1,\n",
    "                 use_one_hot_embeddings=False,\n",
    "                 initializer_range=0.02,\n",
    "                 hidden_dropout_prob=0.1,\n",
    "                 use_relative_positions=False,\n",
    "                 compute_type=mstype.float32):\n",
    "        super(BertSelfAttention, self).__init__()\n",
    "        if hidden_size % num_attention_heads != 0:\n",
    "            raise ValueError(\"The hidden size (%d) is not a multiple of the number \"\n",
    "                             \"of attention heads (%d)\" % (hidden_size, num_attention_heads))\n",
    "        self.size_per_head = int(hidden_size / num_attention_heads)\n",
    "        self.attention = BertAttention(\n",
    "            from_tensor_width=hidden_size,\n",
    "            to_tensor_width=hidden_size,\n",
    "            from_seq_length=seq_length,\n",
    "            to_seq_length=seq_length,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "            size_per_head=self.size_per_head,\n",
    "            attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "            use_one_hot_embeddings=use_one_hot_embeddings,\n",
    "            initializer_range=initializer_range,\n",
    "            use_relative_positions=use_relative_positions,\n",
    "            has_attention_mask=True,\n",
    "            do_return_2d_tensor=True,\n",
    "            compute_type=compute_type)\n",
    "        self.output = BertOutput(in_channels=hidden_size,\n",
    "                                 out_channels=hidden_size,\n",
    "                                 initializer_range=initializer_range,\n",
    "                                 dropout_prob=hidden_dropout_prob,\n",
    "                                 compute_type=compute_type)\n",
    "        self.reshape = P.Reshape()\n",
    "        self.shape = (-1, hidden_size)\n",
    "\n",
    "    def construct(self, input_tensor, attention_mask):\n",
    "        \"\"\"bert self attention\"\"\"\n",
    "        input_tensor = self.reshape(input_tensor, self.shape)\n",
    "        attention_output, attention_scores = self.attention(input_tensor, input_tensor, attention_mask)\n",
    "        output = self.output(attention_output, input_tensor)\n",
    "        return output, attention_scores\n",
    "\n",
    "\n",
    "class BertEncoderCell(nn.Cell):\n",
    "    \"\"\"\n",
    "    Encoder cells used in BertTransformer.\n",
    "\n",
    "    Args:\n",
    "        hidden_size (int): Size of the bert encoder layers. Default: 768.\n",
    "        seq_length (int): Length of input sequence. Default: 512.\n",
    "        num_attention_heads (int): Number of attention heads. Default: 12.\n",
    "        intermediate_size (int): Size of intermediate layer. Default: 3072.\n",
    "        attention_probs_dropout_prob (float): The dropout probability for\n",
    "                                      BertAttention. Default: 0.02.\n",
    "        use_one_hot_embeddings (bool): Specifies whether to use one hot encoding form. Default: False.\n",
    "        initializer_range (float): Initialization value of TruncatedNormal. Default: 0.02.\n",
    "        hidden_dropout_prob (float): The dropout probability for BertOutput. Default: 0.1.\n",
    "        use_relative_positions (bool): Specifies whether to use relative positions. Default: False.\n",
    "        hidden_act (str): Activation function. Default: \"gelu\".\n",
    "        compute_type (:class:`mindspore.dtype`): Compute type in attention. Default: mstype.float32.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 hidden_size=768,\n",
    "                 seq_length=512,\n",
    "                 num_attention_heads=12,\n",
    "                 intermediate_size=3072,\n",
    "                 attention_probs_dropout_prob=0.02,\n",
    "                 use_one_hot_embeddings=False,\n",
    "                 initializer_range=0.02,\n",
    "                 hidden_dropout_prob=0.1,\n",
    "                 use_relative_positions=False,\n",
    "                 hidden_act=\"gelu\",\n",
    "                 compute_type=mstype.float32):\n",
    "        super(BertEncoderCell, self).__init__()\n",
    "        self.attention = BertSelfAttention(\n",
    "            hidden_size=hidden_size,\n",
    "            seq_length=seq_length,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "            attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "            use_one_hot_embeddings=use_one_hot_embeddings,\n",
    "            initializer_range=initializer_range,\n",
    "            hidden_dropout_prob=hidden_dropout_prob,\n",
    "            use_relative_positions=use_relative_positions,\n",
    "            compute_type=compute_type)\n",
    "        self.intermediate = nn.Dense(in_channels=hidden_size,\n",
    "                                     out_channels=intermediate_size,\n",
    "                                     activation=hidden_act,\n",
    "                                     weight_init=TruncatedNormal(initializer_range)).to_float(compute_type)\n",
    "        self.output = BertOutput(in_channels=intermediate_size,\n",
    "                                 out_channels=hidden_size,\n",
    "                                 initializer_range=initializer_range,\n",
    "                                 dropout_prob=hidden_dropout_prob,\n",
    "                                 compute_type=compute_type)\n",
    "    def construct(self, hidden_states, attention_mask):\n",
    "        \"\"\"bert encoder cell\"\"\"\n",
    "        # self-attention\n",
    "        attention_output, attention_scores = self.attention(hidden_states, attention_mask)\n",
    "        # feed construct\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        # add and normalize\n",
    "        output = self.output(intermediate_output, attention_output)\n",
    "        return output, attention_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaa7059-7a61-42fc-91e3-449e02f09878",
   "metadata": {},
   "source": [
    "## BERT layers \n",
    "\n",
    "BERT layer的堆叠。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad7f131d-690c-4386-bb3c-6866d0e10698",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTransformer(nn.Cell):\n",
    "    \"\"\"\n",
    "    Multi-layer bert transformer.\n",
    "\n",
    "    Args:\n",
    "        hidden_size (int): Size of the encoder layers.\n",
    "        seq_length (int): Length of input sequence.\n",
    "        num_hidden_layers (int): Number of hidden layers in encoder cells.\n",
    "        num_attention_heads (int): Number of attention heads in encoder cells. Default: 12.\n",
    "        intermediate_size (int): Size of intermediate layer in encoder cells. Default: 3072.\n",
    "        attention_probs_dropout_prob (float): The dropout probability for\n",
    "                                      BertAttention. Default: 0.1.\n",
    "        use_one_hot_embeddings (bool): Specifies whether to use one hot encoding form. Default: False.\n",
    "        initializer_range (float): Initialization value of TruncatedNormal. Default: 0.02.\n",
    "        hidden_dropout_prob (float): The dropout probability for BertOutput. Default: 0.1.\n",
    "        use_relative_positions (bool): Specifies whether to use relative positions. Default: False.\n",
    "        hidden_act (str): Activation function used in the encoder cells. Default: \"gelu\".\n",
    "        compute_type (:class:`mindspore.dtype`): Compute type in BertTransformer. Default: mstype.float32.\n",
    "        return_all_encoders (bool): Specifies whether to return all encoders. Default: False.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 hidden_size,\n",
    "                 seq_length,\n",
    "                 num_hidden_layers,\n",
    "                 num_attention_heads=12,\n",
    "                 intermediate_size=3072,\n",
    "                 attention_probs_dropout_prob=0.1,\n",
    "                 use_one_hot_embeddings=False,\n",
    "                 initializer_range=0.02,\n",
    "                 hidden_dropout_prob=0.1,\n",
    "                 use_relative_positions=False,\n",
    "                 hidden_act=\"gelu\",\n",
    "                 compute_type=mstype.float32,\n",
    "                 return_all_encoders=False):\n",
    "        super(BertTransformer, self).__init__()\n",
    "        self.return_all_encoders = return_all_encoders\n",
    "        layers = []\n",
    "        for _ in range(num_hidden_layers):\n",
    "            layer = BertEncoderCell(hidden_size=hidden_size,\n",
    "                                    seq_length=seq_length,\n",
    "                                    num_attention_heads=num_attention_heads,\n",
    "                                    intermediate_size=intermediate_size,\n",
    "                                    attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "                                    use_one_hot_embeddings=use_one_hot_embeddings,\n",
    "                                    initializer_range=initializer_range,\n",
    "                                    hidden_dropout_prob=hidden_dropout_prob,\n",
    "                                    use_relative_positions=use_relative_positions,\n",
    "                                    hidden_act=hidden_act,\n",
    "                                    compute_type=compute_type)\n",
    "            layers.append(layer)\n",
    "        self.layers = nn.CellList(layers)\n",
    "        self.reshape = P.Reshape()\n",
    "        self.shape = (-1, hidden_size)\n",
    "        self.out_shape = (-1, seq_length, hidden_size)\n",
    "    def construct(self, input_tensor, attention_mask):\n",
    "        \"\"\"bert transformer\"\"\"\n",
    "        prev_output = self.reshape(input_tensor, self.shape)\n",
    "        all_encoder_layers = ()\n",
    "        all_encoder_atts = ()\n",
    "        all_encoder_outputs = ()\n",
    "        all_encoder_outputs += (prev_output,)\n",
    "        for layer_module in self.layers:\n",
    "            layer_output, encoder_att = layer_module(prev_output, attention_mask)\n",
    "            prev_output = layer_output\n",
    "            if self.return_all_encoders:\n",
    "                all_encoder_outputs += (layer_output,)\n",
    "                layer_output = self.reshape(layer_output, self.out_shape)\n",
    "                all_encoder_layers += (layer_output,)\n",
    "                all_encoder_atts += (encoder_att,)\n",
    "        if not self.return_all_encoders:\n",
    "            prev_output = self.reshape(prev_output, self.out_shape)\n",
    "            all_encoder_layers += (prev_output,)\n",
    "        return all_encoder_layers, all_encoder_outputs, all_encoder_atts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46072dc2-9dc1-432c-93f6-8b39da0e2d42",
   "metadata": {},
   "source": [
    "可以选择在模型内或者外 创建attension mask。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00ffa3ce-ed5b-4e2e-a940-732a6dd83219",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateAttentionMaskFromInputMask(nn.Cell):\n",
    "    \"\"\"\n",
    "    Create attention mask according to input mask.\n",
    "\n",
    "    Args:\n",
    "        config (Class): Configuration for BertModel.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(CreateAttentionMaskFromInputMask, self).__init__()\n",
    "        self.input_mask = None\n",
    "        self.cast = P.Cast()\n",
    "        self.reshape = P.Reshape()\n",
    "        self.shape = (-1, 1, config.seq_length)\n",
    "\n",
    "    def construct(self, input_mask):\n",
    "        attention_mask = self.cast(self.reshape(input_mask, self.shape), mstype.float32)\n",
    "        return attention_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5887ea5-73af-4b8f-a629-024f8abcbd3a",
   "metadata": {},
   "source": [
    "## BERT主模型.\n",
    "\n",
    "上面规定了BERT的组件，这些组件将在BERT主模型中拼接起来形成BERT模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef099232-1329-4bb8-92b9-d7743cc7f608",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(nn.Cell):\n",
    "    \"\"\"\n",
    "    Bidirectional Encoder Representations from Transformers.\n",
    "\n",
    "    Args:\n",
    "        config (Class): Configuration for BertModel.\n",
    "        is_training (bool): True for training mode. False for eval mode.\n",
    "        use_one_hot_embeddings (bool): Specifies whether to use one hot encoding form. Default: False.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 is_training,\n",
    "                 use_one_hot_embeddings=False):\n",
    "        super(BertModel, self).__init__()\n",
    "        config = copy.deepcopy(config)\n",
    "        if not is_training:\n",
    "            config.hidden_dropout_prob = 0.0\n",
    "            config.attention_probs_dropout_prob = 0.0\n",
    "        self.seq_length = config.seq_length\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_hidden_layers = config.num_hidden_layers\n",
    "        self.embedding_size = config.hidden_size\n",
    "        self.token_type_ids = None\n",
    "        self.last_idx = self.num_hidden_layers - 1\n",
    "        output_embedding_shape = [-1, self.seq_length,\n",
    "                                  self.embedding_size]\n",
    "        self.bert_embedding_lookup = nn.Embedding(\n",
    "            vocab_size=config.vocab_size,\n",
    "            embedding_size=self.embedding_size,\n",
    "            use_one_hot=use_one_hot_embeddings)\n",
    "        self.bert_embedding_postprocessor = EmbeddingPostprocessor(\n",
    "            use_relative_positions=config.use_relative_positions,\n",
    "            embedding_size=self.embedding_size,\n",
    "            embedding_shape=output_embedding_shape,\n",
    "            use_token_type=True,\n",
    "            token_type_vocab_size=config.type_vocab_size,\n",
    "            use_one_hot_embeddings=use_one_hot_embeddings,\n",
    "            initializer_range=0.02,\n",
    "            max_position_embeddings=config.max_position_embeddings,\n",
    "            dropout_prob=config.hidden_dropout_prob)\n",
    "        self.bert_encoder = BertTransformer(\n",
    "            hidden_size=self.hidden_size,\n",
    "            seq_length=self.seq_length,\n",
    "            num_attention_heads=config.num_attention_heads,\n",
    "            num_hidden_layers=self.num_hidden_layers,\n",
    "            intermediate_size=config.intermediate_size,\n",
    "            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n",
    "            use_one_hot_embeddings=use_one_hot_embeddings,\n",
    "            initializer_range=config.initializer_range,\n",
    "            hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "            use_relative_positions=config.use_relative_positions,\n",
    "            hidden_act=config.hidden_act,\n",
    "            compute_type=config.compute_type,\n",
    "            return_all_encoders=True)\n",
    "        self.cast = P.Cast()\n",
    "        self.dtype = config.dtype\n",
    "        self.cast_compute_type = SaturateCast(dst_type=config.compute_type)\n",
    "        self.slice = P.StridedSlice()\n",
    "        self.squeeze_1 = P.Squeeze(axis=1)\n",
    "        self.dense = nn.Dense(self.hidden_size, self.hidden_size,\n",
    "                              activation=\"tanh\",\n",
    "                              weight_init=TruncatedNormal(config.initializer_range)).to_float(config.compute_type)\n",
    "        self._create_attention_mask_from_input_mask = CreateAttentionMaskFromInputMask(config)\n",
    "\n",
    "    def construct(self, input_ids, token_type_ids, input_mask):\n",
    "        \"\"\"bert model\"\"\"\n",
    "        # embedding\n",
    "        embedding_tables = self.bert_embedding_lookup.embedding_table\n",
    "        word_embeddings = self.bert_embedding_lookup(input_ids)\n",
    "        embedding_output = self.bert_embedding_postprocessor(token_type_ids, word_embeddings)\n",
    "        # attention mask [batch_size, seq_length, seq_length]\n",
    "        attention_mask = self._create_attention_mask_from_input_mask(input_mask)\n",
    "        # bert encoder\n",
    "        encoder_output, encoder_layers, layer_atts = self.bert_encoder(self.cast_compute_type(embedding_output),\n",
    "                                                                       attention_mask)\n",
    "        sequence_output = self.cast(encoder_output[self.last_idx], self.dtype)\n",
    "        # pooler\n",
    "        batch_size = P.Shape()(input_ids)[0]\n",
    "        sequence_slice = self.slice(sequence_output,\n",
    "                                    (0, 0, 0),\n",
    "                                    (batch_size, 1, self.hidden_size),\n",
    "                                    (1, 1, 1))\n",
    "        first_token = self.squeeze_1(sequence_slice)\n",
    "        pooled_output = self.dense(first_token)\n",
    "        pooled_output = self.cast(pooled_output, self.dtype)\n",
    "        encoder_outputs = ()\n",
    "        for output in encoder_layers:\n",
    "            encoder_outputs += (self.cast(output, self.dtype),)\n",
    "        attention_outputs = ()\n",
    "        for output in layer_atts:\n",
    "            attention_outputs += (self.cast(output, self.dtype),)\n",
    "        return sequence_output, pooled_output, embedding_tables, encoder_outputs, attention_outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e30dc9-6894-4e55-95e1-9a65f20800b6",
   "metadata": {},
   "source": [
    "## BERT应用\n",
    "\n",
    "BERT用于分类和NER两个任务的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e53e1ea-ce1e-4769-b8db-c52f1eee6162",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModelCLS(nn.Cell):\n",
    "    \"\"\"\n",
    "    This class is responsible for classification task evaluation,\n",
    "    i.e. XNLI(num_labels=3), LCQMC(num_labels=2), Chnsenti(num_labels=2).\n",
    "    The returned output represents the final logits as the results of log_softmax is proportional to that of softmax.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, is_training, num_labels=2, dropout_prob=0.0,\n",
    "                 use_one_hot_embeddings=False, phase_type=\"student\"):\n",
    "        super(BertModelCLS, self).__init__()\n",
    "        self.bert = BertModel(config, is_training, use_one_hot_embeddings)\n",
    "        self.cast = P.Cast()\n",
    "        self.weight_init = TruncatedNormal(config.initializer_range)\n",
    "        self.log_softmax = P.LogSoftmax(axis=-1)\n",
    "        self.dtype = config.dtype\n",
    "        self.num_labels = num_labels\n",
    "        self.phase_type = phase_type\n",
    "        self.dense_1 = nn.Dense(config.hidden_size, self.num_labels, weight_init=self.weight_init,\n",
    "                                has_bias=True).to_float(config.compute_type)\n",
    "        self.dropout = nn.ReLU()\n",
    "\n",
    "    def construct(self, input_ids, token_type_id, input_mask):\n",
    "        \"\"\"classification bert model\"\"\"\n",
    "        _, pooled_output, _, seq_output, att_output = self.bert(input_ids, token_type_id, input_mask)\n",
    "        cls = self.cast(pooled_output, self.dtype)\n",
    "        cls = self.dropout(cls)\n",
    "        logits = self.dense_1(cls)\n",
    "        logits = self.cast(logits, self.dtype)\n",
    "        log_probs = self.log_softmax(logits)\n",
    "        if self._phase == 'train' or self.phase_type == \"teacher\":\n",
    "            return seq_output, att_output, logits, log_probs\n",
    "        return log_probs\n",
    "\n",
    "class BertModelNER(nn.Cell):\n",
    "    \"\"\"\n",
    "    This class is responsible for sequence labeling task evaluation, i.e. NER(num_labels=11).\n",
    "    The returned output represents the final logits as the results of log_softmax is proportional to that of softmax.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, is_training, num_labels=11, dropout_prob=0.0,\n",
    "                 use_one_hot_embeddings=False, phase_type=\"student\"):\n",
    "        super(BertModelNER, self).__init__()\n",
    "        if not is_training:\n",
    "            config.hidden_dropout_prob = 0.0\n",
    "            config.hidden_probs_dropout_prob = 0.0\n",
    "        self.bert = BertModel(config, is_training, use_one_hot_embeddings)\n",
    "        self.cast = P.Cast()\n",
    "        self.weight_init = TruncatedNormal(config.initializer_range)\n",
    "        self.log_softmax = P.LogSoftmax(axis=-1)\n",
    "        self.dtype = config.dtype\n",
    "        self.num_labels = num_labels\n",
    "        self.dense_1 = nn.Dense(config.hidden_size, self.num_labels, weight_init=self.weight_init,\n",
    "                                has_bias=True).to_float(config.compute_type)\n",
    "        self.dropout = nn.ReLU()\n",
    "        self.reshape = P.Reshape()\n",
    "        self.shape = (-1, config.hidden_size)\n",
    "        self.origin_shape = (-1, config.seq_length, self.num_labels)\n",
    "        self.phase_type = phase_type\n",
    "\n",
    "    def construct(self, input_ids, input_mask, token_type_id):\n",
    "        \"\"\"Return the final logits as the results of log_softmax.\"\"\"\n",
    "        sequence_output, _, _, encoder_outputs, attention_outputs = \\\n",
    "            self.bert(input_ids, token_type_id, input_mask)\n",
    "        seq = self.dropout(sequence_output)\n",
    "        seq = self.reshape(seq, self.shape)\n",
    "        logits = self.dense_1(seq)\n",
    "        logits = self.cast(logits, self.dtype)\n",
    "        return_value = self.log_softmax(logits)\n",
    "        if self._phase == 'train' or self.phase_type == \"teacher\":\n",
    "            return encoder_outputs, attention_outputs, logits, return_value\n",
    "        return return_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c4e0d8-1569-4847-b059-4990bdfe22b6",
   "metadata": {},
   "source": [
    "## tinyBERT 预训练蒸馏模型和下游蒸馏模型。\n",
    "\n",
    "下面是tinyBERT蒸馏时用到的几个模型。 tinyBERT本身可以使用BERT的模型，导入tinyBERT的config即可。\n",
    "\n",
    "注意在ms中，可以将梯度计算和梯度传递写进模型中，下面有几个模型就是进行这一项工作的。\n",
    "\n",
    "先规定一些处理梯度的杂项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80415a4b-5bcb-49dc-92ac-dab9a698db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRADIENT_CLIP_TYPE = 1\n",
    "GRADIENT_CLIP_VALUE = 1.0\n",
    "\n",
    "clip_grad = C.MultitypeFuncGraph(\"clip_grad\")\n",
    "@clip_grad.register(\"Number\", \"Number\", \"Tensor\")\n",
    "def _clip_grad(clip_type, clip_value, grad):\n",
    "    \"\"\"\n",
    "    Clip gradients.\n",
    "\n",
    "    Inputs:\n",
    "        clip_type (int): The way to clip, 0 for 'value', 1 for 'norm'.\n",
    "        clip_value (float): Specifies how much to clip.\n",
    "        grad (tuple[Tensor]): Gradients.\n",
    "\n",
    "    Outputs:\n",
    "        tuple[Tensor], clipped gradients.\n",
    "    \"\"\"\n",
    "    if clip_type not in (0, 1):\n",
    "        return grad\n",
    "    dt = F.dtype(grad)\n",
    "    if clip_type == 0:\n",
    "        new_grad = C.clip_by_value(grad, F.cast(F.tuple_to_array((-clip_value,)), dt),\n",
    "                                   F.cast(F.tuple_to_array((clip_value,)), dt))\n",
    "    else:\n",
    "        new_grad = nn.ClipByNorm()(grad, F.cast(F.tuple_to_array((clip_value,)), dt))\n",
    "    return new_grad\n",
    "\n",
    "grad_scale = C.MultitypeFuncGraph(\"grad_scale\")\n",
    "reciprocal = P.Reciprocal()\n",
    "\n",
    "@grad_scale.register(\"Tensor\", \"Tensor\")\n",
    "def tensor_grad_scale(scale, grad):\n",
    "    return grad * reciprocal(scale)\n",
    "\n",
    "class ClipGradients(nn.Cell):\n",
    "    \"\"\"\n",
    "    Clip gradients.\n",
    "\n",
    "    Args:\n",
    "        grads (list): List of gradient tuples.\n",
    "        clip_type (Tensor): The way to clip, 'value' or 'norm'.\n",
    "        clip_value (Tensor): Specifies how much to clip.\n",
    "\n",
    "    Returns:\n",
    "        List, a list of clipped_grad tuples.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ClipGradients, self).__init__()\n",
    "        self.clip_by_norm = nn.ClipByNorm()\n",
    "        self.cast = P.Cast()\n",
    "        self.dtype = P.DType()\n",
    "\n",
    "    def construct(self,\n",
    "                  grads,\n",
    "                  clip_type,\n",
    "                  clip_value):\n",
    "        \"\"\"clip gradients\"\"\"\n",
    "        if clip_type not in (0, 1):\n",
    "            return grads\n",
    "        new_grads = ()\n",
    "        for grad in grads:\n",
    "            dt = self.dtype(grad)\n",
    "            if clip_type == 0:\n",
    "                t = C.clip_by_value(grad, self.cast(F.tuple_to_array((-clip_value,)), dt),\n",
    "                                    self.cast(F.tuple_to_array((clip_value,)), dt))\n",
    "            else:\n",
    "                t = self.clip_by_norm(grad, self.cast(F.tuple_to_array((clip_value,)), dt))\n",
    "            new_grads = new_grads + (t,)\n",
    "        return new_grads\n",
    "\n",
    "class SoftCrossEntropy(nn.Cell):\n",
    "    \"\"\"SoftCrossEntropy loss\"\"\"\n",
    "    def __init__(self):\n",
    "        super(SoftCrossEntropy, self).__init__()\n",
    "        self.log_softmax = P.LogSoftmax(axis=-1)\n",
    "        self.softmax = P.Softmax(axis=-1)\n",
    "        self.reduce_mean = P.ReduceMean()\n",
    "        self.cast = P.Cast()\n",
    "\n",
    "    def construct(self, predicts, targets):\n",
    "        likelihood = self.log_softmax(predicts)\n",
    "        target_prob = self.softmax(targets)\n",
    "        loss = self.reduce_mean(-target_prob * likelihood)\n",
    "\n",
    "        return self.cast(loss, mstype.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bc7edf-e55c-4617-acb6-e8ffe75353ad",
   "metadata": {},
   "source": [
    "##  预训练蒸馏模型\n",
    "下面是预训练蒸馏时所用到的计算loss的模型。 注意层级之间的对应关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a261b677-3d45-4958-aca4-3d4d0b10dcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertNetworkWithLoss_gd(nn.Cell):\n",
    "    \"\"\"\n",
    "    Provide bert pre-training loss through network.\n",
    "    Args:\n",
    "        config (BertConfig): The config of BertModel.\n",
    "        is_training (bool): Specifies whether to use the training mode.\n",
    "        use_one_hot_embeddings (bool): Specifies whether to use one-hot for embeddings. Default: False.\n",
    "    Returns:\n",
    "        Tensor, the loss of the network.\n",
    "    \"\"\"\n",
    "    def __init__(self, teacher_config, teacher_ckpt, student_config, is_training, use_one_hot_embeddings=False,\n",
    "                 is_att_fit=True, is_rep_fit=True):\n",
    "        super(BertNetworkWithLoss_gd, self).__init__()\n",
    "        # load teacher model\n",
    "        self.teacher = BertModel(teacher_config, False, use_one_hot_embeddings)\n",
    "        param_dict = load_checkpoint(teacher_ckpt)\n",
    "        new_param_dict = {}\n",
    "        for key, value in param_dict.items():\n",
    "            new_key = re.sub('^bert.bert.', 'teacher.', key)\n",
    "            new_param_dict[new_key] = value\n",
    "        load_param_into_net(self.teacher, new_param_dict)\n",
    "        # no_grad\n",
    "        self.teacher.set_train(False)\n",
    "        params = self.teacher.trainable_params()\n",
    "        for param in params:\n",
    "            param.requires_grad = False\n",
    "        # student model\n",
    "        self.bert = BertModel(student_config, is_training, use_one_hot_embeddings)\n",
    "        self.cast = P.Cast()\n",
    "        self.fit_dense = nn.Dense(student_config.hidden_size,\n",
    "                                  teacher_config.hidden_size).to_float(teacher_config.compute_type)\n",
    "        self.teacher_layers_num = teacher_config.num_hidden_layers\n",
    "        self.student_layers_num = student_config.num_hidden_layers\n",
    "        self.layers_per_block = int(self.teacher_layers_num / self.student_layers_num)\n",
    "        self.is_att_fit = is_att_fit\n",
    "        self.is_rep_fit = is_rep_fit\n",
    "        self.loss_mse = nn.MSELoss()\n",
    "        self.select = P.Select()\n",
    "        self.zeroslike = P.ZerosLike()\n",
    "        self.dtype = teacher_config.dtype\n",
    "\n",
    "    def construct(self,\n",
    "                  input_ids,\n",
    "                  input_mask,\n",
    "                  token_type_id):\n",
    "        \"\"\"general distill network with loss\"\"\"\n",
    "        # teacher model\n",
    "        _, _, _, teacher_seq_output, teacher_att_output = self.teacher(input_ids, token_type_id, input_mask)\n",
    "        # student model\n",
    "        _, _, _, student_seq_output, student_att_output = self.bert(input_ids, token_type_id, input_mask)\n",
    "        total_loss = 0\n",
    "        if self.is_att_fit:\n",
    "            selected_teacher_att_output = ()\n",
    "            selected_student_att_output = ()\n",
    "            for i in range(self.student_layers_num):\n",
    "                selected_teacher_att_output += (teacher_att_output[(i + 1) * self.layers_per_block - 1],)\n",
    "                selected_student_att_output += (student_att_output[i],)\n",
    "            att_loss = 0\n",
    "            for i in range(self.student_layers_num):\n",
    "                student_att = selected_student_att_output[i]\n",
    "                teacher_att = selected_teacher_att_output[i]\n",
    "                student_att = self.select(student_att <= self.cast(-100.0, mstype.float32), self.zeroslike(student_att),\n",
    "                                          student_att)\n",
    "                teacher_att = self.select(teacher_att <= self.cast(-100.0, mstype.float32), self.zeroslike(teacher_att),\n",
    "                                          teacher_att)\n",
    "                att_loss += self.loss_mse(student_att, teacher_att)\n",
    "            total_loss += att_loss\n",
    "        if self.is_rep_fit:\n",
    "            selected_teacher_seq_output = ()\n",
    "            selected_student_seq_output = ()\n",
    "            for i in range(self.student_layers_num + 1):\n",
    "                selected_teacher_seq_output += (teacher_seq_output[i * self.layers_per_block],)\n",
    "                fit_dense_out = self.fit_dense(student_seq_output[i])\n",
    "                fit_dense_out = self.cast(fit_dense_out, self.dtype)\n",
    "                selected_student_seq_output += (fit_dense_out,)\n",
    "            rep_loss = 0\n",
    "            for i in range(self.student_layers_num + 1):\n",
    "                teacher_rep = selected_teacher_seq_output[i]\n",
    "                student_rep = selected_student_seq_output[i]\n",
    "                rep_loss += self.loss_mse(student_rep, teacher_rep)\n",
    "            total_loss += rep_loss\n",
    "        return self.cast(total_loss, mstype.float32)\n",
    "\n",
    "class BertTrainWithLossScaleCell(nn.Cell):\n",
    "    \"\"\"\n",
    "    Encapsulation class of bert network training.\n",
    "\n",
    "    Append an optimizer to the training network after that the construct\n",
    "    function can be called to create the backward graph.\n",
    "\n",
    "    Args:\n",
    "        network (Cell): The training network. Note that loss function should have been added.\n",
    "        optimizer (Optimizer): Optimizer for updating the weights.\n",
    "        scale_update_cell (Cell): Cell to do the loss scale. Default: None.\n",
    "    \"\"\"\n",
    "    def __init__(self, network, optimizer, scale_update_cell=None):\n",
    "        super(BertTrainWithLossScaleCell, self).__init__(auto_prefix=False)\n",
    "        self.network = network\n",
    "        self.network.set_grad()\n",
    "        self.weights = optimizer.parameters\n",
    "        self.optimizer = optimizer\n",
    "        self.grad = C.GradOperation(get_by_list=True,\n",
    "                                    sens_param=True)\n",
    "        self.reducer_flag = False\n",
    "        self.allreduce = P.AllReduce()\n",
    "        self.parallel_mode = context.get_auto_parallel_context(\"parallel_mode\")\n",
    "        if self.parallel_mode in [ParallelMode.DATA_PARALLEL, ParallelMode.HYBRID_PARALLEL]:\n",
    "            self.reducer_flag = True\n",
    "        self.grad_reducer = F.identity\n",
    "        self.degree = 1\n",
    "        if self.reducer_flag:\n",
    "            self.degree = get_group_size()\n",
    "            self.grad_reducer = DistributedGradReducer(optimizer.parameters, False, self.degree)\n",
    "        self.is_distributed = (self.parallel_mode != ParallelMode.STAND_ALONE)\n",
    "        self.cast = P.Cast()\n",
    "        self.alloc_status = P.NPUAllocFloatStatus()\n",
    "        self.get_status = P.NPUGetFloatStatus()\n",
    "        self.clear_status = P.NPUClearFloatStatus()\n",
    "        self.reduce_sum = P.ReduceSum(keep_dims=False)\n",
    "        self.base = Tensor(1, mstype.float32)\n",
    "        self.less_equal = P.LessEqual()\n",
    "        self.hyper_map = C.HyperMap()\n",
    "        self.loss_scale = None\n",
    "        self.loss_scaling_manager = scale_update_cell\n",
    "        if scale_update_cell:\n",
    "            self.loss_scale = Parameter(Tensor(scale_update_cell.get_loss_scale(), dtype=mstype.float32))\n",
    "\n",
    "    def construct(self,\n",
    "                  input_ids,\n",
    "                  input_mask,\n",
    "                  token_type_id,\n",
    "                  sens=None):\n",
    "        \"\"\"Defines the computation performed.\"\"\"\n",
    "        weights = self.weights\n",
    "        loss = self.network(input_ids,\n",
    "                            input_mask,\n",
    "                            token_type_id)\n",
    "        if sens is None:\n",
    "            scaling_sens = self.loss_scale\n",
    "        else:\n",
    "            scaling_sens = sens\n",
    "        # alloc status and clear should be right before gradoperation\n",
    "        init = self.alloc_status()\n",
    "        init = F.depend(init, loss)\n",
    "        clear_status = self.clear_status(init)\n",
    "        scaling_sens = F.depend(scaling_sens, clear_status)\n",
    "        grads = self.grad(self.network, weights)(input_ids,\n",
    "                                                 input_mask,\n",
    "                                                 token_type_id,\n",
    "                                                 self.cast(scaling_sens,\n",
    "                                                           mstype.float32))\n",
    "        # apply grad reducer on grads\n",
    "        grads = self.grad_reducer(grads)\n",
    "        grads = self.hyper_map(F.partial(grad_scale, scaling_sens * self.degree), grads)\n",
    "        grads = self.hyper_map(F.partial(clip_grad, GRADIENT_CLIP_TYPE, GRADIENT_CLIP_VALUE), grads)\n",
    "        init = F.depend(init, grads)\n",
    "        get_status = self.get_status(init)\n",
    "        init = F.depend(init, get_status)\n",
    "        flag_sum = self.reduce_sum(init, (0,))\n",
    "        if self.is_distributed:\n",
    "            # sum overflow flag over devices\n",
    "            flag_reduce = self.allreduce(flag_sum)\n",
    "            cond = self.less_equal(self.base, flag_reduce)\n",
    "        else:\n",
    "            cond = self.less_equal(self.base, flag_sum)\n",
    "        overflow = cond\n",
    "        if sens is None:\n",
    "            overflow = self.loss_scaling_manager(self.loss_scale, cond)\n",
    "        if not overflow:\n",
    "            self.optimizer(grads)\n",
    "        return (loss, cond, scaling_sens)\n",
    "\n",
    "class BertTrainCell(nn.Cell):\n",
    "    \"\"\"\n",
    "    Encapsulation class of bert network training.\n",
    "\n",
    "    Append an optimizer to the training network after that the construct\n",
    "    function can be called to create the backward graph.\n",
    "\n",
    "    Args:\n",
    "        network (Cell): The training network. Note that loss function should have been added.\n",
    "        optimizer (Optimizer): Optimizer for updating the weights.\n",
    "        sens (Number): The adjust parameter. Default: 1.0.\n",
    "    \"\"\"\n",
    "    def __init__(self, network, optimizer, sens=1.0):\n",
    "        super(BertTrainCell, self).__init__(auto_prefix=False)\n",
    "        self.network = network\n",
    "        self.network.set_grad()\n",
    "        self.weights = optimizer.parameters\n",
    "        self.optimizer = optimizer\n",
    "        self.sens = sens\n",
    "        self.grad = C.GradOperation(get_by_list=True,\n",
    "                                    sens_param=True)\n",
    "        self.reducer_flag = False\n",
    "        self.parallel_mode = context.get_auto_parallel_context(\"parallel_mode\")\n",
    "        if self.parallel_mode in [ParallelMode.DATA_PARALLEL, ParallelMode.HYBRID_PARALLEL]:\n",
    "            self.reducer_flag = True\n",
    "        self.grad_reducer = F.identity\n",
    "        self.degree = 1\n",
    "        if self.reducer_flag:\n",
    "            mean = context.get_auto_parallel_context(\"gradients_mean\")\n",
    "            self.degree = get_group_size()\n",
    "            self.grad_reducer = DistributedGradReducer(optimizer.parameters, mean, self.degree)\n",
    "        self.cast = P.Cast()\n",
    "        self.hyper_map = C.HyperMap()\n",
    "\n",
    "    def construct(self,\n",
    "                  input_ids,\n",
    "                  input_mask,\n",
    "                  token_type_id):\n",
    "        \"\"\"Defines the computation performed.\"\"\"\n",
    "        weights = self.weights\n",
    "        loss = self.network(input_ids,\n",
    "                            input_mask,\n",
    "                            token_type_id)\n",
    "        grads = self.grad(self.network, weights)(input_ids,\n",
    "                                                 input_mask,\n",
    "                                                 token_type_id,\n",
    "                                                 self.cast(F.tuple_to_array((self.sens,)),\n",
    "                                                           mstype.float32))\n",
    "        # apply grad reducer on grads\n",
    "        grads = self.grad_reducer(grads)\n",
    "        grads = self.hyper_map(F.partial(clip_grad, GRADIENT_CLIP_TYPE, GRADIENT_CLIP_VALUE), grads)\n",
    "        self.optimizer(grads)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d5b50e-58c1-4653-bcd4-1db6a9bffe15",
   "metadata": {},
   "source": [
    "## 下游蒸馏模型\n",
    "\n",
    "下面是下游蒸馏时所用到的计算loss的模型。\n",
    "\n",
    "BertEvaluationCell中规定了梯度的回传。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99f042a5-f4aa-45cd-a19b-d36e51ed7152",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertNetworkWithLoss_td(nn.Cell):\n",
    "    \"\"\"\n",
    "    Provide bert pre-training loss through network.\n",
    "    Args:\n",
    "        config (BertConfig): The config of BertModel.\n",
    "        is_training (bool): Specifies whether to use the training mode.\n",
    "        use_one_hot_embeddings (bool): Specifies whether to use one-hot for embeddings. Default: False.\n",
    "    Returns:\n",
    "        Tensor, the loss of the network.\n",
    "    \"\"\"\n",
    "    def __init__(self, teacher_config, teacher_ckpt, student_config, student_ckpt,\n",
    "                 is_training, task_type, num_labels, use_one_hot_embeddings=False,\n",
    "                 is_predistill=True, is_att_fit=True, is_rep_fit=True,\n",
    "                 temperature=1.0, dropout_prob=0.1):\n",
    "        super(BertNetworkWithLoss_td, self).__init__()\n",
    "        # load teacher model\n",
    "        if task_type == \"classification\":\n",
    "            self.teacher = BertModelCLS(teacher_config, False, num_labels, dropout_prob,\n",
    "                                        use_one_hot_embeddings, \"teacher\")\n",
    "            self.bert = BertModelCLS(student_config, is_training, num_labels, dropout_prob,\n",
    "                                     use_one_hot_embeddings, \"student\")\n",
    "        elif task_type == \"ner\":\n",
    "            self.teacher = BertModelNER(teacher_config, False, num_labels, dropout_prob,\n",
    "                                        use_one_hot_embeddings, \"teacher\")\n",
    "            self.bert = BertModelNER(student_config, is_training, num_labels, dropout_prob,\n",
    "                                     use_one_hot_embeddings, \"student\")\n",
    "        else:\n",
    "            raise ValueError(f\"Not support task type: {task_type}\")\n",
    "        # param_dict = load_checkpoint(teacher_ckpt)\n",
    "        # new_param_dict = {}\n",
    "        # for key, value in param_dict.items():\n",
    "        #     new_key = re.sub('^bert.', 'teacher.', key)\n",
    "        #     new_param_dict[new_key] = value\n",
    "        # load_param_into_net(self.teacher, new_param_dict)\n",
    "\n",
    "        # no_grad\n",
    "        self.teacher.set_train(False)\n",
    "        # params = self.teacher.trainable_params()\n",
    "        # for param in params:\n",
    "        #     param.requires_grad = False\n",
    "        # # load student model\n",
    "        # param_dict = load_checkpoint(student_ckpt)\n",
    "        # if is_predistill:\n",
    "        #     new_param_dict = {}\n",
    "        #     for key, value in param_dict.items():\n",
    "        #         new_key = re.sub('tinybert_', 'bert_', 'bert.' + key)\n",
    "        #         new_param_dict[new_key] = value\n",
    "        #     load_param_into_net(self.bert, new_param_dict)\n",
    "        # else:\n",
    "        #     new_param_dict = {}\n",
    "        #     for key, value in param_dict.items():\n",
    "        #         new_key = re.sub('tinybert_', 'bert_', key)\n",
    "        #         new_param_dict[new_key] = value\n",
    "        #     load_param_into_net(self.bert, new_param_dict)\n",
    "        self.cast = P.Cast()\n",
    "        self.fit_dense = nn.Dense(student_config.hidden_size,\n",
    "                                  teacher_config.hidden_size).to_float(teacher_config.compute_type)\n",
    "        self.teacher_layers_num = teacher_config.num_hidden_layers\n",
    "        self.student_layers_num = student_config.num_hidden_layers\n",
    "        self.layers_per_block = int(self.teacher_layers_num / self.student_layers_num)\n",
    "        self.is_predistill = is_predistill\n",
    "        self.is_att_fit = is_att_fit\n",
    "        self.is_rep_fit = is_rep_fit\n",
    "        self.use_soft_cross_entropy = task_type in [\"classification\", \"ner\"]\n",
    "        self.temperature = temperature\n",
    "        self.loss_mse = nn.MSELoss()\n",
    "        self.select = P.Select()\n",
    "        self.zeroslike = P.ZerosLike()\n",
    "        self.dtype = student_config.dtype\n",
    "        self.num_labels = num_labels\n",
    "        self.dtype = teacher_config.dtype\n",
    "        self.soft_cross_entropy = SoftCrossEntropy()\n",
    "\n",
    "    def construct(self,\n",
    "                  input_ids,\n",
    "                  input_mask,\n",
    "                  token_type_id,\n",
    "                  label_ids):\n",
    "        \"\"\"task distill network with loss\"\"\"\n",
    "        # teacher model\n",
    "        teacher_seq_output, teacher_att_output, teacher_logits, _ = self.teacher(input_ids, token_type_id, input_mask)\n",
    "        # student model\n",
    "        student_seq_output, student_att_output, student_logits, _ = self.bert(input_ids, token_type_id, input_mask)\n",
    "        total_loss = 0\n",
    "        if self.is_predistill:\n",
    "            if self.is_att_fit:\n",
    "                selected_teacher_att_output = ()\n",
    "                selected_student_att_output = ()\n",
    "                for i in range(self.student_layers_num):\n",
    "                    selected_teacher_att_output += (teacher_att_output[(i + 1) * self.layers_per_block - 1],)\n",
    "                    selected_student_att_output += (student_att_output[i],)\n",
    "                att_loss = 0\n",
    "                for i in range(self.student_layers_num):\n",
    "                    student_att = selected_student_att_output[i]\n",
    "                    teacher_att = selected_teacher_att_output[i]\n",
    "                    student_att = self.select(student_att <= self.cast(-100.0, mstype.float32),\n",
    "                                              self.zeroslike(student_att),\n",
    "                                              student_att)\n",
    "                    teacher_att = self.select(teacher_att <= self.cast(-100.0, mstype.float32),\n",
    "                                              self.zeroslike(teacher_att),\n",
    "                                              teacher_att)\n",
    "                    att_loss += self.loss_mse(student_att, teacher_att)\n",
    "                total_loss += att_loss\n",
    "            if self.is_rep_fit:\n",
    "                selected_teacher_seq_output = ()\n",
    "                selected_student_seq_output = ()\n",
    "                for i in range(self.student_layers_num + 1):\n",
    "                    selected_teacher_seq_output += (teacher_seq_output[i * self.layers_per_block],)\n",
    "                    fit_dense_out = self.fit_dense(student_seq_output[i])\n",
    "                    fit_dense_out = self.cast(fit_dense_out, self.dtype)\n",
    "                    selected_student_seq_output += (fit_dense_out,)\n",
    "                rep_loss = 0\n",
    "                for i in range(self.student_layers_num + 1):\n",
    "                    teacher_rep = selected_teacher_seq_output[i]\n",
    "                    student_rep = selected_student_seq_output[i]\n",
    "                    rep_loss += self.loss_mse(student_rep, teacher_rep)\n",
    "                total_loss += rep_loss\n",
    "        else:\n",
    "            if self.use_soft_cross_entropy:\n",
    "                cls_loss = self.soft_cross_entropy(student_logits / self.temperature, teacher_logits / self.temperature)\n",
    "            else:\n",
    "                cls_loss = self.loss_mse(student_logits[len(student_logits) - 1], label_ids[len(label_ids) - 1])\n",
    "            total_loss += cls_loss\n",
    "        return self.cast(total_loss, mstype.float32)\n",
    "\n",
    "class BertEvaluationCell(nn.Cell):\n",
    "    \"\"\"\n",
    "    Especially defined for finetuning where only four inputs tensor are needed.\n",
    "    \"\"\"\n",
    "    def __init__(self, network, optimizer, sens=1.0):\n",
    "        super(BertEvaluationCell, self).__init__(auto_prefix=False)\n",
    "        self.network = network\n",
    "        self.network.set_grad()\n",
    "        self.weights = optimizer.parameters\n",
    "        self.optimizer = optimizer\n",
    "        self.sens = sens\n",
    "        self.grad = C.GradOperation(get_by_list=True,\n",
    "                                    sens_param=True)\n",
    "        self.reducer_flag = False\n",
    "        self.parallel_mode = context.get_auto_parallel_context(\"parallel_mode\")\n",
    "        if self.parallel_mode in [ParallelMode.DATA_PARALLEL, ParallelMode.HYBRID_PARALLEL]:\n",
    "            self.reducer_flag = True\n",
    "        self.grad_reducer = F.identity\n",
    "        self.degree = 1\n",
    "        if self.reducer_flag:\n",
    "            mean = context.get_auto_parallel_context(\"gradients_mean\")\n",
    "            self.degree = get_group_size()\n",
    "            self.grad_reducer = DistributedGradReducer(optimizer.parameters, mean, self.degree)\n",
    "        self.is_distributed = (self.parallel_mode != ParallelMode.STAND_ALONE)\n",
    "        self.cast = P.Cast()\n",
    "        self.hyper_map = C.HyperMap()\n",
    "\n",
    "    def construct(self,\n",
    "                  input_ids,\n",
    "                  input_mask,\n",
    "                  token_type_id,\n",
    "                  label_ids):\n",
    "        \"\"\"Defines the computation performed.\"\"\"\n",
    "        weights = self.weights\n",
    "        loss = self.network(input_ids,\n",
    "                            input_mask,\n",
    "                            token_type_id,\n",
    "                            label_ids)\n",
    "        grads = self.grad(self.network, weights)(input_ids,\n",
    "                                                 input_mask,\n",
    "                                                 token_type_id,\n",
    "                                                 label_ids,\n",
    "                                                 self.cast(F.tuple_to_array((self.sens,)),\n",
    "                                                           mstype.float32))\n",
    "        # apply grad reducer on grads\n",
    "        grads = self.grad_reducer(grads)\n",
    "        grads = self.hyper_map(F.partial(clip_grad, GRADIENT_CLIP_TYPE, GRADIENT_CLIP_VALUE), grads)\n",
    "        self.optimizer(grads)\n",
    "        return loss\n",
    "\n",
    "class BertEvaluationWithLossScaleCell(nn.Cell):\n",
    "    \"\"\"\n",
    "    Especially defined for finetuning where only four inputs tensor are needed.\n",
    "    \"\"\"\n",
    "    def __init__(self, network, optimizer, scale_update_cell=None):\n",
    "        super(BertEvaluationWithLossScaleCell, self).__init__(auto_prefix=False)\n",
    "        self.network = network\n",
    "        self.network.set_grad()\n",
    "        self.weights = optimizer.parameters\n",
    "        self.optimizer = optimizer\n",
    "        self.grad = C.GradOperation(get_by_list=True,\n",
    "                                    sens_param=True)\n",
    "        self.reducer_flag = False\n",
    "        self.allreduce = P.AllReduce()\n",
    "        self.parallel_mode = context.get_auto_parallel_context(\"parallel_mode\")\n",
    "        if self.parallel_mode in [ParallelMode.DATA_PARALLEL, ParallelMode.HYBRID_PARALLEL]:\n",
    "            self.reducer_flag = True\n",
    "        self.grad_reducer = F.identity\n",
    "        self.degree = 1\n",
    "        if self.reducer_flag:\n",
    "            self.degree = get_group_size()\n",
    "            self.grad_reducer = DistributedGradReducer(optimizer.parameters, False, self.degree)\n",
    "        self.is_distributed = (self.parallel_mode != ParallelMode.STAND_ALONE)\n",
    "        self.cast = P.Cast()\n",
    "        self.alloc_status = P.NPUAllocFloatStatus()\n",
    "        self.get_status = P.NPUGetFloatStatus()\n",
    "        self.clear_status = P.NPUClearFloatStatus()\n",
    "        self.reduce_sum = P.ReduceSum(keep_dims=False)\n",
    "        self.base = Tensor(1, mstype.float32)\n",
    "        self.less_equal = P.LessEqual()\n",
    "        self.hyper_map = C.HyperMap()\n",
    "        self.loss_scale = None\n",
    "        self.loss_scaling_manager = scale_update_cell\n",
    "        if scale_update_cell:\n",
    "            self.loss_scale = Parameter(Tensor(scale_update_cell.get_loss_scale(), dtype=mstype.float32))\n",
    "\n",
    "    def construct(self,\n",
    "                  input_ids,\n",
    "                  input_mask,\n",
    "                  token_type_id,\n",
    "                  label_ids,\n",
    "                  sens=None):\n",
    "        \"\"\"Defines the computation performed.\"\"\"\n",
    "        weights = self.weights\n",
    "        loss = self.network(input_ids,\n",
    "                            input_mask,\n",
    "                            token_type_id,\n",
    "                            label_ids)\n",
    "        if sens is None:\n",
    "            scaling_sens = self.loss_scale\n",
    "        else:\n",
    "            scaling_sens = sens\n",
    "        # alloc status and clear should be right before gradoperation\n",
    "        init = self.alloc_status()\n",
    "        init = F.depend(init, loss)\n",
    "        clear_status = self.clear_status(init)\n",
    "        scaling_sens = F.depend(scaling_sens, clear_status)\n",
    "        grads = self.grad(self.network, weights)(input_ids,\n",
    "                                                 input_mask,\n",
    "                                                 token_type_id,\n",
    "                                                 label_ids,\n",
    "                                                 self.cast(scaling_sens,\n",
    "                                                           mstype.float32))\n",
    "        # apply grad reducer on grads\n",
    "        grads = self.grad_reducer(grads)\n",
    "        grads = self.hyper_map(F.partial(grad_scale, scaling_sens * self.degree), grads)\n",
    "        grads = self.hyper_map(F.partial(clip_grad, GRADIENT_CLIP_TYPE, GRADIENT_CLIP_VALUE), grads)\n",
    "        init = F.depend(init, grads)\n",
    "        get_status = self.get_status(init)\n",
    "        init = F.depend(init, get_status)\n",
    "        flag_sum = self.reduce_sum(init, (0,))\n",
    "        if self.is_distributed:\n",
    "            # sum overflow flag over devices\n",
    "            flag_reduce = self.allreduce(flag_sum)\n",
    "            cond = self.less_equal(self.base, flag_reduce)\n",
    "        else:\n",
    "            cond = self.less_equal(self.base, flag_sum)\n",
    "        overflow = cond\n",
    "        if sens is None:\n",
    "            overflow = self.loss_scaling_manager(self.loss_scale, cond)\n",
    "        if not overflow:\n",
    "            self.optimizer(grads)\n",
    "        return (loss, cond, scaling_sens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213178a6-e3ad-442f-b821-8775ec132c8c",
   "metadata": {},
   "source": [
    "# 数据集\n",
    "\n",
    "  创建tinybert训练所需的dataset。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32c46a24-d1d7-46cd-9a77-d5766e3805bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################data_############################\n",
    "class DataType(Enum):\n",
    "    \"\"\"Enumerate supported dataset format\"\"\"\n",
    "    TFRECORD = 1\n",
    "    MINDRECORD = 2\n",
    "\n",
    "def create_tinybert_dataset(task='td', batch_size=32, device_num=1, rank=0,\n",
    "                            do_shuffle=\"true\", data_dir=None, schema_dir=None,\n",
    "                            data_type=DataType.TFRECORD):\n",
    "    \"\"\"create tinybert dataset\"\"\"\n",
    "    files = os.listdir(data_dir)\n",
    "    data_files = []\n",
    "    for file_name in files:\n",
    "        if \"record\" in file_name and \"db\" not in file_name:\n",
    "            data_files.append(os.path.join(data_dir, file_name))\n",
    "    if task == \"td\":\n",
    "        columns_list = [\"input_ids\", \"input_mask\", \"segment_ids\", \"label_ids\"]\n",
    "    else:\n",
    "        columns_list = [\"input_ids\", \"input_mask\", \"segment_ids\"]\n",
    "\n",
    "    shard_equal_rows = True\n",
    "    shuffle = (do_shuffle == \"true\")\n",
    "    if device_num == 1:\n",
    "        shard_equal_rows = False\n",
    "        shuffle = False\n",
    "\n",
    "    if data_type == DataType.MINDRECORD:\n",
    "        data_set = ds.MindDataset(data_files, columns_list=columns_list,\n",
    "                                  shuffle=(do_shuffle == \"true\"), num_shards=device_num, shard_id=rank)\n",
    "    else:\n",
    "        data_set = ds.TFRecordDataset(data_files, schema_dir if schema_dir != \"\" else None, columns_list=columns_list,\n",
    "                                      shuffle=shuffle, num_shards=device_num, shard_id=rank,\n",
    "                                      shard_equal_rows=shard_equal_rows)\n",
    "    if device_num == 1 and shuffle is True:\n",
    "        data_set = data_set.shuffle(10000)\n",
    "\n",
    "    type_cast_op = transforms.TypeCast(mstype.int32)\n",
    "    data_set = data_set.map(operations=type_cast_op, input_columns=\"segment_ids\")\n",
    "    data_set = data_set.map(operations=type_cast_op, input_columns=\"input_mask\")\n",
    "    data_set = data_set.map(operations=type_cast_op, input_columns=\"input_ids\")\n",
    "    if task == \"td\":\n",
    "        data_set = data_set.map(operations=type_cast_op, input_columns=\"label_ids\")\n",
    "    # apply batch operations\n",
    "    data_set = data_set.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174d7b0f-8f6f-48e7-9845-6aae3b3010da",
   "metadata": {},
   "source": [
    "# 训练部分。\n",
    "\n",
    "由于大部分关于训练的部分都已经写在模型之中，所以这一部分只简单的规定了学习率优化器，评价指标， 模型保存等内容。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2621b7d-499c-4839-91ea-18266ea8d36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy():\n",
    "    \"\"\"Accuracy\"\"\"\n",
    "    def __init__(self):\n",
    "        self.acc_num = 0\n",
    "        self.total_num = 0\n",
    "\n",
    "    def update(self, logits, labels):\n",
    "        labels = labels.asnumpy()\n",
    "        labels = np.reshape(labels, -1)\n",
    "        logits = logits.asnumpy()\n",
    "        logit_id = np.argmax(logits, axis=-1)\n",
    "        self.acc_num += np.sum(labels == logit_id)\n",
    "        self.total_num += len(labels)\n",
    "\n",
    "class BertLearningRate(LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    Warmup-decay learning rate for Bert network.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate, end_learning_rate, warmup_steps, decay_steps, power):\n",
    "        super(BertLearningRate, self).__init__()\n",
    "        self.warmup_flag = False\n",
    "        if warmup_steps > 0:\n",
    "            self.warmup_flag = True\n",
    "            self.warmup_lr = WarmUpLR(learning_rate, warmup_steps)\n",
    "        self.decay_lr = PolynomialDecayLR(learning_rate, end_learning_rate, decay_steps, power)\n",
    "        self.warmup_steps = Tensor(np.array([warmup_steps]).astype(np.float32))\n",
    "\n",
    "        self.greater = P.Greater()\n",
    "        self.one = Tensor(np.array([1.0]).astype(np.float32))\n",
    "        self.cast = P.Cast()\n",
    "\n",
    "    def construct(self, global_step):\n",
    "        decay_lr = self.decay_lr(global_step)\n",
    "        if self.warmup_flag:\n",
    "            is_warmup = self.cast(self.greater(self.warmup_steps, global_step), mstype.float32)\n",
    "            warmup_lr = self.warmup_lr(global_step)\n",
    "            lr = (self.one - is_warmup) * decay_lr + is_warmup * warmup_lr\n",
    "        else:\n",
    "            lr = decay_lr\n",
    "        return lr\n",
    "\n",
    "\n",
    "class LossCallBack(Callback):\n",
    "    \"\"\"\n",
    "    Monitor the loss in training.\n",
    "    If the loss in NAN or INF terminating training.\n",
    "    Note:\n",
    "        if per_print_times is 0 do not print loss.\n",
    "    Args:\n",
    "        per_print_times (int): Print loss every times. Default: 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, per_print_times=1):\n",
    "        super(LossCallBack, self).__init__()\n",
    "        if not isinstance(per_print_times, int) or per_print_times < 0:\n",
    "            raise ValueError(\"print_step must be int and >= 0\")\n",
    "        self._per_print_times = per_print_times\n",
    "\n",
    "    def step_end(self, run_context):\n",
    "        \"\"\"step end and print loss\"\"\"\n",
    "        cb_params = run_context.original_args()\n",
    "        print(\"epoch: {}, step: {}, outputs are {}\".format(cb_params.cur_epoch_num,\n",
    "                                                           cb_params.cur_step_num,\n",
    "                                                           str(cb_params.net_outputs)))\n",
    "\n",
    "class ModelSaveCkpt(Callback):\n",
    "    \"\"\"\n",
    "    Saves checkpoint.\n",
    "    If the loss in NAN or INF terminating training.\n",
    "    Args:\n",
    "        network (Network): The train network for training.\n",
    "        save_ckpt_num (int): The number to save checkpoint, default is 1000.\n",
    "        max_ckpt_num (int): The max checkpoint number, default is 3.\n",
    "    \"\"\"\n",
    "    def __init__(self, network, save_ckpt_step, max_ckpt_num, output_dir):\n",
    "        super(ModelSaveCkpt, self).__init__()\n",
    "        self.count = 0\n",
    "        self.network = network\n",
    "        self.save_ckpt_step = save_ckpt_step\n",
    "        self.max_ckpt_num = max_ckpt_num\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def step_end(self, run_context):\n",
    "        \"\"\"step end and save ckpt\"\"\"\n",
    "        cb_params = run_context.original_args()\n",
    "        if cb_params.cur_step_num % self.save_ckpt_step == 0:\n",
    "            saved_ckpt_num = cb_params.cur_step_num / self.save_ckpt_step\n",
    "            if saved_ckpt_num > self.max_ckpt_num:\n",
    "                oldest_ckpt_index = saved_ckpt_num - self.max_ckpt_num\n",
    "                # path = os.path.join(self.output_dir, \"tiny_bert_{}_{}.ckpt\".format(int(oldest_ckpt_index),\n",
    "                #                                                                    self.save_ckpt_step))\n",
    "                path = os.path.join(self.output_dir, \"tiny_bert_wiki.ckpt\".format(int(oldest_ckpt_index),\n",
    "                                                                   self.save_ckpt_step))\n",
    "                if os.path.exists(path):\n",
    "                    os.remove(path)\n",
    "            # save_checkpoint(self.network, os.path.join(self.output_dir,\n",
    "            #                                            \"tiny_bert_{}_{}.ckpt\".format(int(saved_ckpt_num),\n",
    "            #                                                                          self.save_ckpt_step)))\n",
    "            save_checkpoint(self.network, os.path.join(self.output_dir,\n",
    "                                                       \"tiny_bert_wiki.ckpt\".format(int(saved_ckpt_num),\n",
    "                                                                                     self.save_ckpt_step)))\n",
    "\n",
    "class EvalCallBack(Callback):\n",
    "    \"\"\"Evaluation callback\"\"\"\n",
    "    def __init__(self, network, dataset):\n",
    "        super(EvalCallBack, self).__init__()\n",
    "        self.network = network\n",
    "        self.global_acc = 0.0\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def epoch_end(self, run_context):\n",
    "        \"\"\"step end and do evaluation\"\"\"\n",
    "        callback = Accuracy()\n",
    "        columns_list = [\"input_ids\", \"input_mask\", \"segment_ids\", \"label_ids\"]\n",
    "        for data in self.dataset.create_dict_iterator(num_epochs=1):\n",
    "            input_data = []\n",
    "            for i in columns_list:\n",
    "                input_data.append(data[i])\n",
    "            input_ids, input_mask, token_type_id, label_ids = input_data\n",
    "            self.network.set_train(False)\n",
    "            logits = self.network(input_ids, token_type_id, input_mask)\n",
    "            self.network.set_train(True)\n",
    "            callback.update(logits, label_ids)\n",
    "        acc = callback.acc_num / callback.total_num\n",
    "        with open(\"./eval.log\", \"a+\") as f:\n",
    "            f.write(\"acc_num {}, total_num{}, accuracy{:.6f}\".format(callback.acc_num, callback.total_num,\n",
    "                                                                     callback.acc_num / callback.total_num))\n",
    "            f.write('\\n')\n",
    "\n",
    "        if acc > self.global_acc:\n",
    "            self.global_acc = acc\n",
    "            print(\"The best acc is {}\".format(acc))\n",
    "            eval_model_ckpt_file = \"eval_model.ckpt\"\n",
    "            if os.path.exists(eval_model_ckpt_file):\n",
    "                os.remove(eval_model_ckpt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6543878c-ccf9-4fbf-92b2-d15db63bf6dd",
   "metadata": {},
   "source": [
    "# Config 设置\n",
    "\n",
    "config 是代码运行的基础。这里通过手动设置和yaml文件读取两个方式共同创建config。\n",
    "\n",
    "首先定义从文件读取config的函数。\n",
    "- Config ： 定义config\n",
    "- parse_yaml ： 读取yaml文件\n",
    "- parse_cli_to_yaml： 把yaml文件加入argparse中\n",
    "- extra_operations : 将总config分离成几个负责单独部分的config。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9243c11b-7315-4ad8-a525-bfc21c175a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"\n",
    "    Configuration namespace. Convert dictionary to members.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg_dict):\n",
    "        for k, v in cfg_dict.items():\n",
    "            if isinstance(v, (list, tuple)):\n",
    "                setattr(self, k, [Config(x) if isinstance(x, dict) else x for x in v])\n",
    "            else:\n",
    "                setattr(self, k, Config(v) if isinstance(v, dict) else v)\n",
    "\n",
    "    def __str__(self):\n",
    "        return pformat(self.__dict__)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "def parse_cli_to_yaml(parser, cfg, helper=None, choices=None, cfg_path=\"pretrain_base_config.yaml\"):\n",
    "    \"\"\"\n",
    "    Parse command line arguments to the configuration according to the default yaml.\n",
    "\n",
    "    Args:\n",
    "        parser: Parent parser.\n",
    "        cfg: Base configuration.\n",
    "        helper: Helper description.\n",
    "        cfg_path: Path to the default yaml config.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"[REPLACE THIS at config.py]\",\n",
    "                                     parents=[parser])\n",
    "    helper = {} if helper is None else helper\n",
    "    choices = {} if choices is None else choices\n",
    "    for item in cfg:\n",
    "        try:\n",
    "            if not isinstance(cfg[item], list) and not isinstance(cfg[item], dict):\n",
    "                help_description = helper[item] if item in helper else \"Please reference to {}\".format(cfg_path)\n",
    "                choice = choices[item] if item in choices else None\n",
    "                if isinstance(cfg[item], bool):\n",
    "                    parser.add_argument(\"--\" + item, type=ast.literal_eval, default=cfg[item], choices=choice,\n",
    "                                        help=help_description)\n",
    "                else:\n",
    "                    parser.add_argument(\"--\" + item, type=type(cfg[item]), default=cfg[item], choices=choice,\n",
    "                                        help=help_description)\n",
    "        except:\n",
    "            pass\n",
    "    args = parser.parse_args(args=[])\n",
    "    return args\n",
    "\n",
    "\n",
    "def parse_yaml(yaml_path):\n",
    "    \"\"\"\n",
    "    Parse the yaml config file.\n",
    "\n",
    "    Args:\n",
    "        yaml_path: Path to the yaml config.\n",
    "    \"\"\"\n",
    "    with open(yaml_path, 'r') as fin:\n",
    "        try:\n",
    "            cfgs = yaml.load_all(fin.read(), Loader=yaml.FullLoader)\n",
    "            cfgs = [x for x in cfgs]\n",
    "            if len(cfgs) == 1:\n",
    "                cfg_helper = {}\n",
    "                cfg = cfgs[0]\n",
    "                cfg_choices = {}\n",
    "            elif len(cfgs) == 2:\n",
    "                cfg, cfg_helper = cfgs\n",
    "                cfg_choices = {}\n",
    "            elif len(cfgs) == 3:\n",
    "                cfg, cfg_helper, cfg_choices = cfgs\n",
    "            else:\n",
    "                raise ValueError(\"At most 3 docs (config, description for help, choices) are supported in config yaml\")\n",
    "            # print(cfg_helper)\n",
    "        except:\n",
    "            raise ValueError(\"Failed to parse yaml\")\n",
    "    return cfg, cfg_helper, cfg_choices\n",
    "\n",
    "\n",
    "def merge(args, cfg):\n",
    "    \"\"\"\n",
    "    Merge the base config from yaml file and command line arguments.\n",
    "\n",
    "    Args:\n",
    "        args: Command line arguments.\n",
    "        cfg: Base configuration.\n",
    "    \"\"\"\n",
    "    args_var = vars(args)\n",
    "    for item in args_var:\n",
    "        cfg[item] = args_var[item]\n",
    "    return cfg\n",
    "\n",
    "def extra_operations(cfg):\n",
    "    \"\"\"\n",
    "    Do extra work on config\n",
    "\n",
    "    Args:\n",
    "        config: Object after instantiation of class 'Config'.\n",
    "    \"\"\"\n",
    "    def create_filter_fun(keywords):\n",
    "        return lambda x: not (True in [key in x.name.lower() for key in keywords])\n",
    "\n",
    "    if cfg.description == 'general_distill':\n",
    "        cfg.common_cfg.loss_scale_value = 2 ** 16\n",
    "        cfg.common_cfg.AdamWeightDecay.decay_filter = create_filter_fun(cfg.common_cfg.AdamWeightDecay.decay_filter)\n",
    "        cfg.bert_teacher_net_cfg.dtype = mstype.float32\n",
    "        cfg.bert_teacher_net_cfg.compute_type = mstype.float16\n",
    "        cfg.bert_student_net_cfg.dtype = mstype.float32\n",
    "        cfg.bert_student_net_cfg.compute_type = mstype.float16\n",
    "        cfg.bert_teacher_net_cfg = BertConfig(**cfg.bert_teacher_net_cfg.__dict__)\n",
    "        cfg.bert_student_net_cfg = BertConfig(**cfg.bert_student_net_cfg.__dict__)\n",
    "    elif cfg.description == 'task_distill':\n",
    "        cfg.phase1_cfg.loss_scale_value = 2 ** 8\n",
    "        cfg.phase1_cfg.optimizer_cfg.AdamWeightDecay.decay_filter = create_filter_fun(\n",
    "            cfg.phase1_cfg.optimizer_cfg.AdamWeightDecay.decay_filter)\n",
    "        cfg.phase2_cfg.loss_scale_value = 2 ** 16\n",
    "        cfg.phase2_cfg.optimizer_cfg.AdamWeightDecay.decay_filter = create_filter_fun(\n",
    "            cfg.phase2_cfg.optimizer_cfg.AdamWeightDecay.decay_filter)\n",
    "        cfg.td_teacher_net_cfg.dtype = mstype.float32\n",
    "        cfg.td_teacher_net_cfg.compute_type = mstype.float16\n",
    "        cfg.td_student_net_cfg.dtype = mstype.float32\n",
    "        cfg.td_student_net_cfg.compute_type = mstype.float16\n",
    "        cfg.td_teacher_net_cfg = BertConfig(**cfg.td_teacher_net_cfg.__dict__)\n",
    "        cfg.td_student_net_cfg = BertConfig(**cfg.td_student_net_cfg.__dict__)\n",
    "    else:\n",
    "        pass\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21df2693-243c-4ed4-ad35-9f51f8bb7a1e",
   "metadata": {},
   "source": [
    "\n",
    "# 预训练蒸馏。\n",
    "定义好了模型，数据集，config和一些杂项，我们便可以开始在wiki数据集上的预训练蒸馏任务。\n",
    "\n",
    "\n",
    "## 预训练蒸馏config\n",
    "config 是代码运行的基础。这里通过手动设置和yaml文件读取两个方式共同创建config。 \n",
    "其中 与模型和训练相关的大部分config都在 yaml文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f81d090f-c752-4730-818d-8c59364ad16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bert_student_net_cfg': <__main__.BertConfig object at 0xffff26024d90>,\n",
      " 'bert_teacher_net_cfg': <__main__.BertConfig object at 0xffff24e84490>,\n",
      " 'checkpoint_url': '',\n",
      " 'common_cfg': {'AdamWeightDecay': {'decay_filter': <function extra_operations.<locals>.create_filter_fun.<locals>.<lambda> at 0xffff24e16170>,\n",
      " 'end_learning_rate': 1e-14,\n",
      " 'eps': 1e-06,\n",
      " 'learning_rate': 5e-05,\n",
      " 'power': 1.0,\n",
      " 'weight_decay': 0.0001},\n",
      " 'batch_size': 32,\n",
      " 'loss_scale_value': 65536,\n",
      " 'scale_factor': 2,\n",
      " 'scale_window': 1000},\n",
      " 'data_dir': 'data/wiki',\n",
      " 'data_path': '/cache/data',\n",
      " 'data_sink_steps': 1,\n",
      " 'data_url': '',\n",
      " 'dataset_type': 'tfrecord',\n",
      " 'description': 'general_distill',\n",
      " 'device_id': 0,\n",
      " 'device_num': 1,\n",
      " 'device_target': 'Ascend',\n",
      " 'distribute': 'False',\n",
      " 'do_shuffle': 'true',\n",
      " 'enable_data_sink': 'true',\n",
      " 'enable_modelarts': False,\n",
      " 'enable_profiling': False,\n",
      " 'epoch_size': 3,\n",
      " 'folder_name_under_zip_file': './',\n",
      " 'load_path': '/cache/checkpoint_path',\n",
      " 'load_teacher_ckpt_path': 'bert/ms_model_ckpt.ckpt',\n",
      " 'max_ckpt_num': 1,\n",
      " 'modelarts_dataset_unzip_name': '',\n",
      " 'output_path': '/cache/train',\n",
      " 'save_ckpt_path': 'save/tinybert_wiki',\n",
      " 'save_ckpt_step': 1,\n",
      " 'schema_dir': '',\n",
      " 'train_url': ''}\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "parser_gen = argparse.ArgumentParser(description=\"default name\", add_help=False)\n",
    "\n",
    "parser_gen.add_argument(\"--distribute\", default=\"False\",\n",
    "                    help=\"if distribute\")\n",
    "\n",
    "parser_gen.add_argument(\"--device_target\", default=\"Ascend\",\n",
    "                    help=\"device_target\")\n",
    "\n",
    "parser_gen.add_argument(\"--epoch_size\", default=3,\n",
    "                    help=\"epoch_size\")\n",
    "\n",
    "\n",
    "parser_gen.add_argument(\"--save_ckpt_step\", default=1,\n",
    "                    help=\"save_ckpt_step\")\n",
    "\n",
    "parser_gen.add_argument(\"--max_ckpt_num\", default=1,\n",
    "                    help=\"max_ckpt_num\")\n",
    "\n",
    "parser_gen.add_argument(\"--save_ckpt_path\", default=\"save/tinybert_wiki\",\n",
    "                    help=\"save_ckpt_path\")\n",
    "\n",
    "parser_gen.add_argument(\"--data_dir\", default=\"data/wiki\",\n",
    "                    help=\"data_dir\")\n",
    "\n",
    "parser_gen.add_argument(\"--load_teacher_ckpt_path\", default=\"bert/ms_model_ckpt.ckpt\",\n",
    "                    help=\"load_teacher_ckpt_path\")\n",
    "\n",
    "parser_gen.add_argument(\"--dataset_type\", default=\"tfrecord\",\n",
    "                    help=\"dataset_type\")\n",
    "\n",
    "config_gen_path = 'config/gd_config.yaml'\n",
    "default_gen, helper_gen, choices_gen = parse_yaml(config_gen_path)\n",
    "args_gen = parse_cli_to_yaml(parser=parser_gen, cfg=default_gen, helper=helper_gen, choices=choices_gen, cfg_path=config_gen_path)\n",
    "\n",
    "final_config_gen = merge(args_gen, default_gen)\n",
    "config_obj_gen = Config(final_config_gen)\n",
    "\n",
    "config = extra_operations(config_obj_gen)\n",
    "\n",
    "\n",
    "common_cfg = config.common_cfg\n",
    "bert_teacher_net_cfg = config.bert_teacher_net_cfg\n",
    "bert_student_net_cfg = config.bert_student_net_cfg\n",
    "\n",
    "args_opt = config\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92522250-0e8a-40ec-a7bc-363b2cf51cd4",
   "metadata": {},
   "source": [
    "## 环境设置。\n",
    "\n",
    "context 是用来存储训练时的环境变量的。 这里定义了存储文件夹和训练时的一些基础环境。具体含义见[ms文档](https://www.mindspore.cn/docs/zh-CN/r1.8/index.html)， \n",
    "\n",
    "在这里不启用分布式训练。在Ascend 环境中，也不使用混合精度训练。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dae6d093-48bb-4e7d-990d-9f60a2c7b493",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "set_seed(0)\n",
    "\n",
    "context.set_context(mode=context.PYNATIVE_MODE, device_target=args_opt.device_target,\n",
    "                    reserve_class_name_in_scope=False)\n",
    "\n",
    "\n",
    "# save_ckpt_dir = os.path.join(args_opt.save_ckpt_path,\n",
    "#                              datetime.datetime.now().strftime('%Y-%m-%d_time_%H_%M_%S'))\n",
    "save_ckpt_dir = args_opt.save_ckpt_path\n",
    "\n",
    "if not os.path.exists(save_ckpt_dir):\n",
    "    os.makedirs(save_ckpt_dir)\n",
    "    \n",
    "if args_opt.distribute == \"true\":\n",
    "    if args_opt.device_target == 'Ascend':\n",
    "        D.init()\n",
    "        device_num = args_opt.device_num\n",
    "        rank = args_opt.device_id % device_num\n",
    "    else:\n",
    "        D.init()\n",
    "        device_num = D.get_group_size()\n",
    "        rank = D.get_rank()\n",
    "    save_ckpt_dir = save_ckpt_dir + '_ckpt_' + str(rank)\n",
    "    context.reset_auto_parallel_context()\n",
    "    context.set_auto_parallel_context(parallel_mode=ParallelMode.DATA_PARALLEL, gradients_mean=True,\n",
    "                                      device_num=device_num)\n",
    "else:\n",
    "    rank = 0\n",
    "    device_num = 1\n",
    "    \n",
    "enable_loss_scale = True \n",
    "\n",
    "if args_opt.device_target == \"Ascend\":\n",
    "    context.set_context(device_id=args_opt.device_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1166bf95-81f9-4000-b71b-4214c5b604d6",
   "metadata": {},
   "source": [
    "## 读取数据\n",
    "\n",
    "这里数据格式为TFRECORD， 利用上面定义的函数创建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30f1925e-2546-45ec-bf61-1d2160fe4d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size:  251\n",
      "dataset repeatcount:  1\n"
     ]
    }
   ],
   "source": [
    " \n",
    "if args_opt.dataset_type == \"tfrecord\":\n",
    "    dataset_type = DataType.TFRECORD\n",
    "elif args_opt.dataset_type == \"mindrecord\":\n",
    "    dataset_type = DataType.MINDRECORD\n",
    "else:\n",
    "    raise Exception(\"dataset format is not supported yet\")\n",
    "    \n",
    "dataset = create_tinybert_dataset('gd', common_cfg.batch_size, device_num, rank,\n",
    "                                  args_opt.do_shuffle, args_opt.data_dir, args_opt.schema_dir,\n",
    "                                  data_type=dataset_type)\n",
    "dataset_size = dataset.get_dataset_size()\n",
    "print('dataset size: ', dataset_size)\n",
    "print(\"dataset repeatcount: \", dataset.get_repeat_count())\n",
    "\n",
    "\n",
    "repeat_count = args_opt.epoch_size\n",
    "time_monitor_steps = dataset_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561c8f7a-a220-4d16-ab79-142495c714e1",
   "metadata": {},
   "source": [
    "## 定义模型\n",
    "  \n",
    "我们使用刚才定义的用于general distill的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ace3842-f79a-4db7-85d8-50724797ad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "netwithloss = BertNetworkWithLoss_gd(teacher_config=bert_teacher_net_cfg,\n",
    "                                     teacher_ckpt=args_opt.load_teacher_ckpt_path,\n",
    "                                     student_config=bert_student_net_cfg,\n",
    "                                     is_training=True, use_one_hot_embeddings=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ab640d-53ac-4f9b-8e83-c7b17f54dc4f",
   "metadata": {},
   "source": [
    "## 定义学习率和优化器\n",
    "\n",
    "学习率使用上面定义的BERT的学习率函数，优化器用adam。\n",
    "\n",
    "netwithgrads 模型中已经定义好了梯度计算与回传，将学习率与优化器传入即可。\n",
    "\n",
    "\n",
    "ModelSaveCkpt 定义了模型的保存方式。 注意因为这里只是预训练示例，因此训练数据非常小。 示例中只能把保存的step：<font color=black size=2 face=雅黑>**save_ckpt_step**</font> 调整的非常小。这里定义为1. 实际模型中可以考虑设置为100 或者 200等。这里为了简化， 将保存的模型统一命名为 <font color=black size=2 face=雅黑>**tiny_bert_wiki.ckpt**</font>. 方便后续使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4ffb446-4c85-46c1-9027-df8fb8e10db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = BertLearningRate(learning_rate=common_cfg.AdamWeightDecay.learning_rate,\n",
    "                               end_learning_rate=common_cfg.AdamWeightDecay.end_learning_rate,\n",
    "                               warmup_steps=int(dataset_size * args_opt.epoch_size / 10),\n",
    "                               decay_steps=int(dataset_size * args_opt.epoch_size),\n",
    "                               power=common_cfg.AdamWeightDecay.power)\n",
    "params = netwithloss.trainable_params()\n",
    "decay_params = list(filter(common_cfg.AdamWeightDecay.decay_filter, params))\n",
    "other_params = list(filter(lambda x: not common_cfg.AdamWeightDecay.decay_filter(x), params))\n",
    "group_params = [{'params': decay_params, 'weight_decay': common_cfg.AdamWeightDecay.weight_decay},\n",
    "                {'params': other_params, 'weight_decay': 0.0},\n",
    "                {'order_params': params}]\n",
    "\n",
    "optimizer = AdamWeightDecay(group_params, learning_rate=lr_schedule, eps=common_cfg.AdamWeightDecay.eps)\n",
    "\n",
    "callback = [TimeMonitor(time_monitor_steps), LossCallBack(), ModelSaveCkpt(netwithloss.bert,\n",
    "                                                                           args_opt.save_ckpt_step,\n",
    "                                                                           args_opt.max_ckpt_num,\n",
    "                                                                           save_ckpt_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9713a14a-1763-4b83-963a-8a770b5bc380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enable_loss_scale=True\n"
     ]
    }
   ],
   "source": [
    "if enable_loss_scale:\n",
    "    update_cell = DynamicLossScaleUpdateCell(loss_scale_value=common_cfg.loss_scale_value,\n",
    "                                             scale_factor=common_cfg.scale_factor,\n",
    "                                             scale_window=common_cfg.scale_window)\n",
    "    netwithgrads = BertTrainWithLossScaleCell(netwithloss, optimizer=optimizer, scale_update_cell=update_cell)\n",
    "    print('enable_loss_scale=True')\n",
    "else:\n",
    "    netwithgrads = BertTrainCell(netwithloss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e993927f-0190-4dea-80a7-4f3ed608e6d2",
   "metadata": {},
   "source": [
    "## 模型训练\n",
    "\n",
    "mindspore中，[Model](https://mindspore.cn/docs/zh-CN/r1.7/api_python/mindspore/mindspore.Model.html#mindspore.Model)是MindSpore提供的高阶API，可以进行模型训练、评估和推理。 \n",
    "\n",
    "具体见[Model详情页](https://www.mindspore.cn/tutorials/zh-CN/r1.7/advanced/train/model.html)\n",
    "\n",
    "我们可以把上面定义的模型，训练设置等传入Model，Model即可自动训练。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e82ff4e7-853c-4297-854c-b532f94aea02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(95050:281472998983552,MainProcess):2022-10-17-20:02:34.655.348 [mindspore/train/model.py:1077] For LossCallBack callback, {'step_end'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.\n",
      "[WARNING] ME(95050:281472998983552,MainProcess):2022-10-17-20:02:34.656.587 [mindspore/train/model.py:1077] For ModelSaveCkpt callback, {'step_end'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:02:51.944.824 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op191] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:02:52.443.805 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op237] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:02:52.629.875 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op256] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:02:52.888.801 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op277] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:02:53.018.364 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op287] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:02:53.131.147 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op305] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:02:53.312.097 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op323] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:02:53.605.010 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op355] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:02:53.717.397 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op373] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:02:53.897.754 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op391] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:02:54.207.967 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op423] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:02:54.324.348 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op441] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:02:54.516.432 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op459] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:02:58.503.249 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/gradStridedSlice/StridedSliceGrad-op916] don't support int64, reduce precision from int64 to int32.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, step: 1, outputs are (Tensor(shape=[], dtype=Float32, value= 52.5542), Tensor(shape=[], dtype=Bool, value= False), Parameter (name=loss_scale, shape=(), dtype=Float32, requires_grad=True))\n",
      "Train epoch time: 48191.528 ms, per step time: 48191.528 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:03:24.250.950 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op4729] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:03:24.778.992 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op4795] don't support int64, reduce precision from int64 to int32.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, step: 2, outputs are (Tensor(shape=[], dtype=Float32, value= 48.2302), Tensor(shape=[], dtype=Bool, value= False), Parameter (name=loss_scale, shape=(), dtype=Float32, requires_grad=True))\n",
      "Train epoch time: 3890.811 ms, per step time: 3890.811 ms\n",
      "epoch: 3, step: 3, outputs are (Tensor(shape=[], dtype=Float32, value= 49.1845), Tensor(shape=[], dtype=Bool, value= False), Parameter (name=loss_scale, shape=(), dtype=Float32, requires_grad=True))\n",
      "Train epoch time: 858.037 ms, per step time: 858.037 ms\n"
     ]
    }
   ],
   "source": [
    "model = Model(netwithgrads)\n",
    "model.train(repeat_count, dataset, callbacks=callback,\n",
    "            dataset_sink_mode=(args_opt.enable_data_sink == \"true\"),\n",
    "            sink_size=args_opt.data_sink_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a22e05a-eef3-4191-9856-4cbfd9ccb353",
   "metadata": {},
   "source": [
    "# 下游任务蒸馏。\n",
    "\n",
    "下面我们开始以QNLI数据集为例，进行任务蒸馏。\n",
    "\n",
    "QNLI是从另一个数据集The Stanford Question Answering Dataset(斯坦福问答数据集, SQuAD 1.0）转换而来的。SQuAD 1.0是有一个问题-段落对组成的问答数据集，其中段落来自维基百科，段落中的一个句子包含问题的答案。\n",
    "\n",
    "QNLI目标是判断问题（question）和句子（sentence，维基百科段落中的一句）是否蕴含，蕴含和不蕴含，二分类。我们可以简单的当作分类任务即可。\n",
    "\n",
    "## 任务蒸馏config\n",
    "config 是代码运行的基础。这里通过手动设置和yaml文件读取两个方式共同创建config。 \n",
    "其中 与模型和训练相关的大部分config都在 yaml文件中。\n",
    "\n",
    "注意在代码中，任务蒸馏分成了两个阶段。 而两个阶段仅仅是训练超参不同，然后仅在第二阶段进行对验证集的预测。 所以config也要分开为两个。 这一点， 可以在自己训练时根据自己喜好，只进行一个阶段也是可以的。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73352f1d-872c-4619-bff9-d233ccf2175e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'assessment_method': 'accuracy',\n",
      " 'checkpoint_url': '',\n",
      " 'ckpt_file': '',\n",
      " 'data_path': '/cache/data',\n",
      " 'data_sink_steps': 1,\n",
      " 'data_url': '',\n",
      " 'dataset_type': 'tfrecord',\n",
      " 'description': 'task_distill',\n",
      " 'device_id': 0,\n",
      " 'device_target': 'Ascend',\n",
      " 'do_eval': 'True',\n",
      " 'do_shuffle': 'true',\n",
      " 'do_train': 'True',\n",
      " 'enable_data_sink': 'true',\n",
      " 'enable_modelarts': False,\n",
      " 'enable_profiling': False,\n",
      " 'eval_cfg': {'batch_size': 32},\n",
      " 'eval_data_dir': 'data/glue/qnli',\n",
      " 'file_format': 'MINDIR',\n",
      " 'file_name': 'tinybert',\n",
      " 'folder_name_under_zip_file': '',\n",
      " 'load_gd_ckpt_path': 'save/tinybert_wiki/tiny_bert_wiki.ckpt',\n",
      " 'load_path': '/cache/checkpoint_path',\n",
      " 'load_td1_ckpt_path': '',\n",
      " 'load_teacher_ckpt_path': 'bert/ms_model_ckpt.ckpt',\n",
      " 'max_ckpt_num': 1,\n",
      " 'modelarts_dataset_unzip_name': '',\n",
      " 'num_labels': 2,\n",
      " 'onnx_path': '',\n",
      " 'output_path': '/cache/train',\n",
      " 'phase1_cfg': {'batch_size': 32,\n",
      " 'loss_scale_value': 256,\n",
      " 'optimizer_cfg': {'AdamWeightDecay': {'decay_filter': <function extra_operations.<locals>.create_filter_fun.<locals>.<lambda> at 0xffff08737710>,\n",
      " 'end_learning_rate': 0.0,\n",
      " 'eps': 1e-06,\n",
      " 'learning_rate': 5e-05,\n",
      " 'power': 1.0,\n",
      " 'weight_decay': 0.0001}},\n",
      " 'scale_factor': 2,\n",
      " 'scale_window': 50},\n",
      " 'phase2_cfg': {'batch_size': 32,\n",
      " 'loss_scale_value': 65536,\n",
      " 'optimizer_cfg': {'AdamWeightDecay': {'decay_filter': <function extra_operations.<locals>.create_filter_fun.<locals>.<lambda> at 0xffff08737830>,\n",
      " 'end_learning_rate': 0.0,\n",
      " 'eps': 1e-06,\n",
      " 'learning_rate': 2e-05,\n",
      " 'power': 1.0,\n",
      " 'weight_decay': 0.0001}},\n",
      " 'scale_factor': 2,\n",
      " 'scale_window': 50},\n",
      " 'schema_dir': '',\n",
      " 'task_name': 'QNLI',\n",
      " 'task_type': 'classification',\n",
      " 'td_phase1_epoch_size': 1,\n",
      " 'td_phase2_epoch_size': 3,\n",
      " 'td_student_net_cfg': <__main__.BertConfig object at 0xffff08750850>,\n",
      " 'td_teacher_net_cfg': <__main__.BertConfig object at 0xffff08763c10>,\n",
      " 'train_data_dir': 'data/glue/qnli',\n",
      " 'train_url': ''}\n"
     ]
    }
   ],
   "source": [
    "###################task_config####################\n",
    "\n",
    "\n",
    "parser_task = argparse.ArgumentParser(description=\"default name\", add_help=False)\n",
    "\n",
    "parser_task.add_argument(\"--do_train\", default=\"True\",\n",
    "                    help=\"do_train\")\n",
    "\n",
    "parser_task.add_argument(\"--do_eval\", default=\"True\",\n",
    "                    help=\"do_eval\")\n",
    "\n",
    "parser_task.add_argument(\"--device_target\", default=\"Ascend\",\n",
    "                    help=\"device_target\")\n",
    "\n",
    "parser_task.add_argument(\"--device_id\", default=0,\n",
    "                    help=\"device_id\")\n",
    "\n",
    "parser_task.add_argument(\"--td_phase1_epoch_size\", default=1,\n",
    "                    help=\"td_phase1_epoch_size\")\n",
    "\n",
    "parser_task.add_argument(\"--td_phase2_epoch_size\", default=3,\n",
    "                    help=\"td_phase2_epoch_size\")\n",
    "\n",
    "parser_task.add_argument(\"--do_shuffle\", default=\"true\",\n",
    "                    help=\"do_shuffle\")\n",
    "\n",
    "parser_task.add_argument(\"--max_ckpt_num\", default=1,\n",
    "                    help=\"max_ckpt_num\")\n",
    "\n",
    "parser_task.add_argument(\"--load_teacher_ckpt_path\", default=\"bert/ms_model_ckpt.ckpt\",\n",
    "                    help=\"load_teacher_ckpt_path\")\n",
    "\n",
    "parser_task.add_argument(\"--load_gd_ckpt_path\", default=\"save/tinybert_wiki/tiny_bert_wiki.ckpt\",\n",
    "                    help=\"load_gd_ckpt_path\")\n",
    "\n",
    "parser_task.add_argument(\"--load_td1_ckpt_path\", default=\"\",\n",
    "                    help=\"load_td1_ckpt_path\")\n",
    "\n",
    "parser_task.add_argument(\"--train_data_dir\", default=\"data/glue/qnli\",\n",
    "                    help=\"train_data_dir\")\n",
    "\n",
    "parser_task.add_argument(\"--eval_data_dir\", default=\"data/glue/qnli\",\n",
    "                    help=\"eval_data_dir\")\n",
    "\n",
    "parser_task.add_argument(\"--dataset_type\", default=\"tfrecord\",\n",
    "                    help=\"dataset_type\")\n",
    "\n",
    "parser_task.add_argument(\"--task_type\", default=\"classification\",\n",
    "                    help=\"task_type\")\n",
    "\n",
    "parser_task.add_argument(\"--task_name\", default=\"QNLI\",\n",
    "                    help=\"task_name\")\n",
    "\n",
    "parser_task.add_argument(\"--assessment_method\", default=\"accuracy\",\n",
    "                    help=\"assessment_method\")\n",
    "\n",
    "\n",
    "config_task_path = 'config/td_config_qnli.yaml'\n",
    "default_task, helper_task, choices_task = parse_yaml(config_task_path)\n",
    "args_task = parse_cli_to_yaml(parser=parser_task, cfg=default_task, helper=helper_task, choices=choices_task, cfg_path=config_task_path)\n",
    "final_config_task = merge(args_task, default_task)\n",
    "config_obj_task = Config(final_config_task)\n",
    "config_task = extra_operations(config_obj_task)\n",
    "config = config_task\n",
    "\n",
    "phase1_cfg = config.phase1_cfg\n",
    "phase2_cfg = config.phase2_cfg\n",
    "eval_cfg = config.eval_cfg\n",
    "td_teacher_net_cfg = config.td_teacher_net_cfg\n",
    "td_student_net_cfg = config.td_student_net_cfg\n",
    "\n",
    "\n",
    "print(config)\n",
    "args_opt = config\n",
    "\n",
    "_cur_dir = os.getcwd()\n",
    "td_phase1_save_ckpt_dir = os.path.join(_cur_dir, 'save/tinybert_td_phase1_save_ckpt')\n",
    "td_phase2_save_ckpt_dir = os.path.join(_cur_dir, 'save/tinybert_td_phase2_save_ckpt')\n",
    "if not os.path.exists(td_phase1_save_ckpt_dir):\n",
    "    os.makedirs(td_phase1_save_ckpt_dir)\n",
    "if not os.path.exists(td_phase2_save_ckpt_dir):\n",
    "    os.makedirs(td_phase2_save_ckpt_dir)\n",
    "    \n",
    "enable_loss_scale = False\n",
    "set_seed(123)\n",
    "ds.config.set_seed(12345)\n",
    "dataset_type = DataType.TFRECORD\n",
    "cfg = phase1_cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5937d26c-5e3a-4651-83a2-3e964e40fd11",
   "metadata": {},
   "source": [
    "## 第一阶段\n",
    "\n",
    "### 环境 数据集 与模型设置\n",
    "\n",
    "  与预训练蒸馏相似。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4d7d48b-7b3f-4f91-9b60-0e0831b34795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "td1 dataset size:  3443\n",
      "td1 dataset repeatcount:  1\n"
     ]
    }
   ],
   "source": [
    "rank = 0\n",
    "device_num = 1\n",
    "\n",
    "dataset = create_tinybert_dataset('td', cfg.batch_size,\n",
    "                                  device_num, rank, args_opt.do_shuffle,\n",
    "                                  args_opt.train_data_dir, args_opt.schema_dir,\n",
    "                                  data_type=dataset_type)\n",
    "\n",
    "dataset_size = dataset.get_dataset_size()\n",
    "print('td1 dataset size: ', dataset_size)\n",
    "print('td1 dataset repeatcount: ', dataset.get_repeat_count())\n",
    "args_opt.data_sink_steps = dataset_size\n",
    "repeat_count = args_opt.td_phase1_epoch_size\n",
    "time_monitor_steps = dataset_size\n",
    "\n",
    "load_teacher_checkpoint_path = args_opt.load_teacher_ckpt_path\n",
    "load_student_checkpoint_path = args_opt.load_gd_ckpt_path\n",
    "netwithloss = BertNetworkWithLoss_td(teacher_config=td_teacher_net_cfg, teacher_ckpt=load_teacher_checkpoint_path,\n",
    "                                     student_config=td_student_net_cfg, student_ckpt=load_student_checkpoint_path,\n",
    "                                     is_training=True, task_type=args_opt.task_type,\n",
    "                                     num_labels=args_opt.num_labels, is_predistill=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4755e0de-bc70-4e3c-b85d-45953a1b9c4e",
   "metadata": {},
   "source": [
    "### 设置学习率与优化器\n",
    "\n",
    " ModelSaveCkpt 规定了保存的位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b30ae11-65a3-4698-8c42-132ded1dc7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_cfg = cfg.optimizer_cfg\n",
    "\n",
    "lr_schedule = BertLearningRate(learning_rate=optimizer_cfg.AdamWeightDecay.learning_rate,\n",
    "                               end_learning_rate=optimizer_cfg.AdamWeightDecay.end_learning_rate,\n",
    "                               warmup_steps=int(dataset_size / 10),\n",
    "                               decay_steps=int(dataset_size * args_opt.td_phase1_epoch_size),\n",
    "                               power=optimizer_cfg.AdamWeightDecay.power)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "params = netwithloss.trainable_params()\n",
    "decay_params = list(filter(optimizer_cfg.AdamWeightDecay.decay_filter, params))\n",
    "other_params = list(filter(lambda x: not optimizer_cfg.AdamWeightDecay.decay_filter(x), params))\n",
    "group_params = [{'params': decay_params, 'weight_decay': optimizer_cfg.AdamWeightDecay.weight_decay},\n",
    "                {'params': other_params, 'weight_decay': 0.0},\n",
    "                {'order_params': params}]\n",
    "\n",
    "optimizer = AdamWeightDecay(group_params, learning_rate=lr_schedule, eps=optimizer_cfg.AdamWeightDecay.eps)\n",
    "\n",
    "callback = [TimeMonitor(time_monitor_steps), LossCallBack(), ModelSaveCkpt(netwithloss.bert,\n",
    "                                                                           dataset_size,\n",
    "                                                                           args_opt.max_ckpt_num,\n",
    "                                                                           td_phase1_save_ckpt_dir)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57268e6a-fb78-432b-8048-56e973005647",
   "metadata": {},
   "source": [
    "### 载入梯度模型并训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0437b1a-92cf-43bd-b767-4cefd697e628",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(95050:281472998983552,MainProcess):2022-10-17-20:03:45.637.746 [mindspore/train/model.py:1077] For LossCallBack callback, {'step_end'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.\n",
      "[WARNING] ME(95050:281472998983552,MainProcess):2022-10-17-20:03:45.639.284 [mindspore/train/model.py:1077] For ModelSaveCkpt callback, {'step_end'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:03:47.705.754 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op4879] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:03:48.033.553 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op4910] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:03:48.151.926 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op4928] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:03:48.349.308 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op4946] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:03:48.656.343 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op4978] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:03:48.781.768 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op4996] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:03:48.985.970 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op5014] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:03:49.293.646 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op5046] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:03:49.427.716 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op5064] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:03:49.631.020 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op5082] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:03:49.939.848 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op5114] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:03:50.087.437 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op5132] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:03:50.295.588 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op5150] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:03:54.335.916 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/gradStridedSlice/StridedSliceGrad-op5708] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:03:54.582.995 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/gradStridedSlice/StridedSliceGrad-op5752] don't support int64, reduce precision from int64 to int32.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, step: 1, outputs are 5.313729\n",
      "epoch: 1, step: 2, outputs are 5.312618\n",
      "epoch: 1, step: 3, outputs are 5.2948303\n",
      "epoch: 1, step: 4, outputs are 5.285376\n",
      "epoch: 1, step: 5, outputs are 5.257374\n",
      "epoch: 1, step: 6, outputs are 5.221371\n",
      "epoch: 1, step: 7, outputs are 5.2034407\n",
      "epoch: 1, step: 8, outputs are 5.147038\n",
      "epoch: 1, step: 9, outputs are 5.101118\n",
      "epoch: 1, step: 10, outputs are 5.045333\n",
      "epoch: 1, step: 11, outputs are 4.9651184\n",
      "epoch: 1, step: 12, outputs are 4.8991895\n",
      "epoch: 1, step: 13, outputs are 4.870689\n",
      "epoch: 1, step: 14, outputs are 4.816765\n",
      "epoch: 1, step: 15, outputs are 4.786441\n",
      "epoch: 1, step: 16, outputs are 4.7112\n",
      "epoch: 1, step: 17, outputs are 4.6605344\n",
      "epoch: 1, step: 18, outputs are 4.569983\n",
      "epoch: 1, step: 19, outputs are 4.5126414\n",
      "epoch: 1, step: 20, outputs are 4.4686213\n",
      "epoch: 1, step: 21, outputs are 4.394112\n",
      "epoch: 1, step: 22, outputs are 4.3465514\n",
      "epoch: 1, step: 23, outputs are 4.2930713\n",
      "epoch: 1, step: 24, outputs are 4.1946926\n",
      "epoch: 1, step: 25, outputs are 4.186329\n",
      "epoch: 1, step: 26, outputs are 4.0994344\n",
      "epoch: 1, step: 27, outputs are 4.027304\n",
      "epoch: 1, step: 28, outputs are 3.9545126\n",
      "epoch: 1, step: 29, outputs are 3.9417763\n",
      "epoch: 1, step: 30, outputs are 3.8786254\n",
      "epoch: 1, step: 31, outputs are 3.776836\n",
      "epoch: 1, step: 32, outputs are 3.6830013\n",
      "epoch: 1, step: 33, outputs are 3.6672745\n",
      "epoch: 1, step: 34, outputs are 3.5803556\n",
      "epoch: 1, step: 35, outputs are 3.4836063\n",
      "epoch: 1, step: 36, outputs are 3.4266207\n",
      "epoch: 1, step: 37, outputs are 3.3850753\n",
      "epoch: 1, step: 38, outputs are 3.2672954\n",
      "epoch: 1, step: 39, outputs are 3.2222567\n",
      "epoch: 1, step: 40, outputs are 3.1396303\n",
      "epoch: 1, step: 41, outputs are 3.064334\n",
      "epoch: 1, step: 42, outputs are 3.0013542\n",
      "epoch: 1, step: 43, outputs are 2.9071765\n",
      "epoch: 1, step: 44, outputs are 2.8422122\n",
      "epoch: 1, step: 45, outputs are 2.7618973\n",
      "epoch: 1, step: 46, outputs are 2.705669\n",
      "epoch: 1, step: 47, outputs are 2.6129014\n",
      "epoch: 1, step: 48, outputs are 2.5325186\n",
      "epoch: 1, step: 49, outputs are 2.4807842\n",
      "epoch: 1, step: 50, outputs are 2.3997226\n",
      "epoch: 1, step: 51, outputs are 2.326813\n",
      "epoch: 1, step: 52, outputs are 2.265773\n",
      "epoch: 1, step: 53, outputs are 2.1949942\n",
      "epoch: 1, step: 54, outputs are 2.1271648\n",
      "epoch: 1, step: 55, outputs are 2.066194\n",
      "epoch: 1, step: 56, outputs are 2.0088055\n",
      "epoch: 1, step: 57, outputs are 1.9663322\n",
      "epoch: 1, step: 58, outputs are 1.8877211\n",
      "epoch: 1, step: 59, outputs are 1.806083\n",
      "epoch: 1, step: 60, outputs are 1.7646257\n",
      "epoch: 1, step: 61, outputs are 1.697179\n",
      "epoch: 1, step: 62, outputs are 1.6402022\n",
      "epoch: 1, step: 63, outputs are 1.5818413\n",
      "epoch: 1, step: 64, outputs are 1.5226017\n",
      "epoch: 1, step: 65, outputs are 1.4767538\n",
      "epoch: 1, step: 66, outputs are 1.4249562\n",
      "epoch: 1, step: 67, outputs are 1.3673601\n",
      "epoch: 1, step: 68, outputs are 1.3371391\n",
      "epoch: 1, step: 69, outputs are 1.3004227\n",
      "epoch: 1, step: 70, outputs are 1.2508045\n",
      "epoch: 1, step: 71, outputs are 1.1980137\n",
      "epoch: 1, step: 72, outputs are 1.1504757\n",
      "epoch: 1, step: 73, outputs are 1.1206815\n",
      "epoch: 1, step: 74, outputs are 1.0992465\n",
      "epoch: 1, step: 75, outputs are 1.0387293\n",
      "epoch: 1, step: 76, outputs are 1.0047945\n",
      "epoch: 1, step: 77, outputs are 0.99545276\n",
      "epoch: 1, step: 78, outputs are 0.9508848\n",
      "epoch: 1, step: 79, outputs are 0.91748315\n",
      "epoch: 1, step: 80, outputs are 0.9099466\n",
      "epoch: 1, step: 81, outputs are 0.8755719\n",
      "epoch: 1, step: 82, outputs are 0.87321824\n",
      "epoch: 1, step: 83, outputs are 0.8219618\n",
      "epoch: 1, step: 84, outputs are 0.8223685\n",
      "epoch: 1, step: 85, outputs are 0.8068956\n",
      "epoch: 1, step: 86, outputs are 0.79242086\n",
      "epoch: 1, step: 87, outputs are 0.7866544\n",
      "epoch: 1, step: 88, outputs are 0.7685249\n",
      "epoch: 1, step: 89, outputs are 0.74661183\n",
      "epoch: 1, step: 90, outputs are 0.7383852\n",
      "epoch: 1, step: 91, outputs are 0.72149676\n",
      "epoch: 1, step: 92, outputs are 0.71558154\n",
      "epoch: 1, step: 93, outputs are 0.70172983\n",
      "epoch: 1, step: 94, outputs are 0.6939778\n",
      "epoch: 1, step: 95, outputs are 0.6760866\n",
      "epoch: 1, step: 96, outputs are 0.68220526\n",
      "epoch: 1, step: 97, outputs are 0.65083927\n",
      "epoch: 1, step: 98, outputs are 0.6626868\n",
      "epoch: 1, step: 99, outputs are 0.64948106\n",
      "epoch: 1, step: 100, outputs are 0.63800997\n",
      "epoch: 1, step: 101, outputs are 0.6491778\n",
      "epoch: 1, step: 102, outputs are 0.6420114\n",
      "epoch: 1, step: 103, outputs are 0.6037541\n",
      "epoch: 1, step: 104, outputs are 0.5930712\n",
      "epoch: 1, step: 105, outputs are 0.6021759\n",
      "epoch: 1, step: 106, outputs are 0.60431206\n",
      "epoch: 1, step: 107, outputs are 0.57833004\n",
      "epoch: 1, step: 108, outputs are 0.5735928\n",
      "epoch: 1, step: 109, outputs are 0.58460826\n",
      "epoch: 1, step: 110, outputs are 0.55072045\n",
      "epoch: 1, step: 111, outputs are 0.53296363\n",
      "epoch: 1, step: 112, outputs are 0.5663448\n",
      "epoch: 1, step: 113, outputs are 0.52949125\n",
      "epoch: 1, step: 114, outputs are 0.52445024\n",
      "epoch: 1, step: 115, outputs are 0.52863693\n",
      "epoch: 1, step: 116, outputs are 0.5021817\n",
      "epoch: 1, step: 117, outputs are 0.5154499\n",
      "epoch: 1, step: 118, outputs are 0.53317785\n",
      "epoch: 1, step: 119, outputs are 0.50695235\n",
      "epoch: 1, step: 120, outputs are 0.48873904\n",
      "epoch: 1, step: 121, outputs are 0.48370406\n",
      "epoch: 1, step: 122, outputs are 0.51910496\n",
      "epoch: 1, step: 123, outputs are 0.4735695\n",
      "epoch: 1, step: 124, outputs are 0.4843454\n",
      "epoch: 1, step: 125, outputs are 0.46103868\n",
      "epoch: 1, step: 126, outputs are 0.460104\n",
      "epoch: 1, step: 127, outputs are 0.46181226\n",
      "epoch: 1, step: 128, outputs are 0.45336556\n",
      "epoch: 1, step: 129, outputs are 0.4594047\n",
      "epoch: 1, step: 130, outputs are 0.43591943\n",
      "epoch: 1, step: 131, outputs are 0.4441378\n",
      "epoch: 1, step: 132, outputs are 0.42122015\n",
      "epoch: 1, step: 133, outputs are 0.41964418\n",
      "epoch: 1, step: 134, outputs are 0.41685703\n",
      "epoch: 1, step: 135, outputs are 0.455779\n",
      "epoch: 1, step: 136, outputs are 0.41002318\n",
      "epoch: 1, step: 137, outputs are 0.4231655\n",
      "epoch: 1, step: 138, outputs are 0.40175137\n",
      "epoch: 1, step: 139, outputs are 0.3984376\n",
      "epoch: 1, step: 140, outputs are 0.4009223\n",
      "epoch: 1, step: 141, outputs are 0.4261828\n",
      "epoch: 1, step: 142, outputs are 0.3945118\n",
      "epoch: 1, step: 143, outputs are 0.40203306\n",
      "epoch: 1, step: 144, outputs are 0.39004654\n",
      "epoch: 1, step: 145, outputs are 0.38650894\n",
      "epoch: 1, step: 146, outputs are 0.36628577\n",
      "epoch: 1, step: 147, outputs are 0.37671548\n",
      "epoch: 1, step: 148, outputs are 0.36758894\n",
      "epoch: 1, step: 149, outputs are 0.37492314\n",
      "epoch: 1, step: 150, outputs are 0.3507352\n",
      "epoch: 1, step: 151, outputs are 0.34572396\n",
      "epoch: 1, step: 152, outputs are 0.35851875\n",
      "epoch: 1, step: 153, outputs are 0.35721397\n",
      "epoch: 1, step: 154, outputs are 0.3513193\n",
      "epoch: 1, step: 155, outputs are 0.32428205\n",
      "epoch: 1, step: 156, outputs are 0.3550619\n",
      "epoch: 1, step: 157, outputs are 0.35040888\n",
      "epoch: 1, step: 158, outputs are 0.3532036\n",
      "epoch: 1, step: 159, outputs are 0.35160613\n",
      "epoch: 1, step: 160, outputs are 0.32710925\n",
      "epoch: 1, step: 161, outputs are 0.33897012\n",
      "epoch: 1, step: 162, outputs are 0.31147736\n",
      "epoch: 1, step: 163, outputs are 0.31715858\n",
      "epoch: 1, step: 164, outputs are 0.3121003\n",
      "epoch: 1, step: 165, outputs are 0.3049363\n",
      "epoch: 1, step: 166, outputs are 0.30790266\n",
      "epoch: 1, step: 167, outputs are 0.30086493\n",
      "epoch: 1, step: 168, outputs are 0.2941036\n",
      "epoch: 1, step: 169, outputs are 0.2860667\n",
      "epoch: 1, step: 170, outputs are 0.3144065\n",
      "epoch: 1, step: 171, outputs are 0.29800874\n",
      "epoch: 1, step: 172, outputs are 0.26144195\n",
      "epoch: 1, step: 173, outputs are 0.27622384\n",
      "epoch: 1, step: 174, outputs are 0.27341402\n",
      "epoch: 1, step: 175, outputs are 0.29004982\n",
      "epoch: 1, step: 176, outputs are 0.28033435\n",
      "epoch: 1, step: 177, outputs are 0.27978724\n",
      "epoch: 1, step: 178, outputs are 0.27334866\n",
      "epoch: 1, step: 179, outputs are 0.25348037\n",
      "epoch: 1, step: 180, outputs are 0.27546787\n",
      "epoch: 1, step: 181, outputs are 0.25859305\n",
      "epoch: 1, step: 182, outputs are 0.27676064\n",
      "epoch: 1, step: 183, outputs are 0.2543155\n",
      "epoch: 1, step: 184, outputs are 0.25115877\n",
      "epoch: 1, step: 185, outputs are 0.24948332\n",
      "epoch: 1, step: 186, outputs are 0.24190299\n",
      "epoch: 1, step: 187, outputs are 0.2487621\n",
      "epoch: 1, step: 188, outputs are 0.23785299\n",
      "epoch: 1, step: 189, outputs are 0.24276978\n",
      "epoch: 1, step: 190, outputs are 0.22423565\n",
      "epoch: 1, step: 191, outputs are 0.25504065\n",
      "epoch: 1, step: 192, outputs are 0.24503693\n",
      "epoch: 1, step: 193, outputs are 0.24110828\n",
      "epoch: 1, step: 194, outputs are 0.22566406\n",
      "epoch: 1, step: 195, outputs are 0.22049968\n",
      "epoch: 1, step: 196, outputs are 0.2181113\n",
      "epoch: 1, step: 197, outputs are 0.2254473\n",
      "epoch: 1, step: 198, outputs are 0.22974792\n",
      "epoch: 1, step: 199, outputs are 0.23816231\n",
      "epoch: 1, step: 200, outputs are 0.22108689\n",
      "epoch: 1, step: 201, outputs are 0.21531902\n",
      "epoch: 1, step: 202, outputs are 0.22277667\n",
      "epoch: 1, step: 203, outputs are 0.20888071\n",
      "epoch: 1, step: 204, outputs are 0.22767043\n",
      "epoch: 1, step: 205, outputs are 0.23017584\n",
      "epoch: 1, step: 206, outputs are 0.19555421\n",
      "epoch: 1, step: 207, outputs are 0.20665163\n",
      "epoch: 1, step: 208, outputs are 0.19882892\n",
      "epoch: 1, step: 209, outputs are 0.20881413\n",
      "epoch: 1, step: 210, outputs are 0.21539134\n",
      "epoch: 1, step: 211, outputs are 0.2041694\n",
      "epoch: 1, step: 212, outputs are 0.19898504\n",
      "epoch: 1, step: 213, outputs are 0.21027964\n",
      "epoch: 1, step: 214, outputs are 0.20617524\n",
      "epoch: 1, step: 215, outputs are 0.18664649\n",
      "epoch: 1, step: 216, outputs are 0.20019917\n",
      "epoch: 1, step: 217, outputs are 0.19784638\n",
      "epoch: 1, step: 218, outputs are 0.20180075\n",
      "epoch: 1, step: 219, outputs are 0.1995503\n",
      "epoch: 1, step: 220, outputs are 0.18592104\n",
      "epoch: 1, step: 221, outputs are 0.18495484\n",
      "epoch: 1, step: 222, outputs are 0.18624887\n",
      "epoch: 1, step: 223, outputs are 0.18096\n",
      "epoch: 1, step: 224, outputs are 0.18463627\n",
      "epoch: 1, step: 225, outputs are 0.19106849\n",
      "epoch: 1, step: 226, outputs are 0.17037287\n",
      "epoch: 1, step: 227, outputs are 0.16394967\n",
      "epoch: 1, step: 228, outputs are 0.17314278\n",
      "epoch: 1, step: 229, outputs are 0.16040123\n",
      "epoch: 1, step: 230, outputs are 0.16825593\n",
      "epoch: 1, step: 231, outputs are 0.1664826\n",
      "epoch: 1, step: 232, outputs are 0.15940341\n",
      "epoch: 1, step: 233, outputs are 0.16813995\n",
      "epoch: 1, step: 234, outputs are 0.16283858\n",
      "epoch: 1, step: 235, outputs are 0.15234168\n",
      "epoch: 1, step: 236, outputs are 0.16328008\n",
      "epoch: 1, step: 237, outputs are 0.16690321\n",
      "epoch: 1, step: 238, outputs are 0.15750489\n",
      "epoch: 1, step: 239, outputs are 0.15024441\n",
      "epoch: 1, step: 240, outputs are 0.14854309\n",
      "epoch: 1, step: 241, outputs are 0.15191714\n",
      "epoch: 1, step: 242, outputs are 0.15128185\n",
      "epoch: 1, step: 243, outputs are 0.14215736\n",
      "epoch: 1, step: 244, outputs are 0.14939693\n",
      "epoch: 1, step: 245, outputs are 0.14531928\n",
      "epoch: 1, step: 246, outputs are 0.14644843\n",
      "epoch: 1, step: 247, outputs are 0.14284314\n",
      "epoch: 1, step: 248, outputs are 0.14653343\n",
      "epoch: 1, step: 249, outputs are 0.13681841\n",
      "epoch: 1, step: 250, outputs are 0.14481811\n",
      "epoch: 1, step: 251, outputs are 0.15104535\n",
      "epoch: 1, step: 252, outputs are 0.14244485\n",
      "epoch: 1, step: 253, outputs are 0.13920987\n",
      "epoch: 1, step: 254, outputs are 0.1348808\n",
      "epoch: 1, step: 255, outputs are 0.14399236\n",
      "epoch: 1, step: 256, outputs are 0.13150021\n",
      "epoch: 1, step: 257, outputs are 0.1411058\n",
      "epoch: 1, step: 258, outputs are 0.13162702\n",
      "epoch: 1, step: 259, outputs are 0.1345258\n",
      "epoch: 1, step: 260, outputs are 0.12785561\n",
      "epoch: 1, step: 261, outputs are 0.13307576\n",
      "epoch: 1, step: 262, outputs are 0.13339624\n",
      "epoch: 1, step: 263, outputs are 0.12969996\n",
      "epoch: 1, step: 264, outputs are 0.13424465\n",
      "epoch: 1, step: 265, outputs are 0.13515662\n",
      "epoch: 1, step: 266, outputs are 0.12721619\n",
      "epoch: 1, step: 267, outputs are 0.12800682\n",
      "epoch: 1, step: 268, outputs are 0.12143689\n",
      "epoch: 1, step: 269, outputs are 0.124063395\n",
      "epoch: 1, step: 270, outputs are 0.123532206\n",
      "epoch: 1, step: 271, outputs are 0.123854205\n",
      "epoch: 1, step: 272, outputs are 0.12165792\n",
      "epoch: 1, step: 273, outputs are 0.12498237\n",
      "epoch: 1, step: 274, outputs are 0.12262802\n",
      "epoch: 1, step: 275, outputs are 0.11893911\n",
      "epoch: 1, step: 276, outputs are 0.12023888\n",
      "epoch: 1, step: 277, outputs are 0.12347298\n",
      "epoch: 1, step: 278, outputs are 0.119079754\n",
      "epoch: 1, step: 279, outputs are 0.11881849\n",
      "epoch: 1, step: 280, outputs are 0.12005118\n",
      "epoch: 1, step: 281, outputs are 0.11670654\n",
      "epoch: 1, step: 282, outputs are 0.11598817\n",
      "epoch: 1, step: 283, outputs are 0.105927125\n",
      "epoch: 1, step: 284, outputs are 0.10950805\n",
      "epoch: 1, step: 285, outputs are 0.11934237\n",
      "epoch: 1, step: 286, outputs are 0.11200929\n",
      "epoch: 1, step: 287, outputs are 0.10790621\n",
      "epoch: 1, step: 288, outputs are 0.1160526\n",
      "epoch: 1, step: 289, outputs are 0.11992682\n",
      "epoch: 1, step: 290, outputs are 0.11367343\n",
      "epoch: 1, step: 291, outputs are 0.105563596\n",
      "epoch: 1, step: 292, outputs are 0.100644365\n",
      "epoch: 1, step: 293, outputs are 0.1171626\n",
      "epoch: 1, step: 294, outputs are 0.104685485\n",
      "epoch: 1, step: 295, outputs are 0.116534024\n",
      "epoch: 1, step: 296, outputs are 0.10972097\n",
      "epoch: 1, step: 297, outputs are 0.11196207\n",
      "epoch: 1, step: 298, outputs are 0.10350926\n",
      "epoch: 1, step: 299, outputs are 0.107660085\n",
      "epoch: 1, step: 300, outputs are 0.10664171\n",
      "epoch: 1, step: 301, outputs are 0.11380267\n",
      "epoch: 1, step: 302, outputs are 0.10601108\n",
      "epoch: 1, step: 303, outputs are 0.11656968\n",
      "epoch: 1, step: 304, outputs are 0.10103935\n",
      "epoch: 1, step: 305, outputs are 0.10106177\n",
      "epoch: 1, step: 306, outputs are 0.11836366\n",
      "epoch: 1, step: 307, outputs are 0.105052456\n",
      "epoch: 1, step: 308, outputs are 0.10299586\n",
      "epoch: 1, step: 309, outputs are 0.09362331\n",
      "epoch: 1, step: 310, outputs are 0.0978166\n",
      "epoch: 1, step: 311, outputs are 0.10519446\n",
      "epoch: 1, step: 312, outputs are 0.09886797\n",
      "epoch: 1, step: 313, outputs are 0.090642706\n",
      "epoch: 1, step: 314, outputs are 0.09832965\n",
      "epoch: 1, step: 315, outputs are 0.094736\n",
      "epoch: 1, step: 316, outputs are 0.08990969\n",
      "epoch: 1, step: 317, outputs are 0.10127911\n",
      "epoch: 1, step: 318, outputs are 0.10689473\n",
      "epoch: 1, step: 319, outputs are 0.100772664\n",
      "epoch: 1, step: 320, outputs are 0.09343618\n",
      "epoch: 1, step: 321, outputs are 0.09168103\n",
      "epoch: 1, step: 322, outputs are 0.08936714\n",
      "epoch: 1, step: 323, outputs are 0.09057592\n",
      "epoch: 1, step: 324, outputs are 0.09384252\n",
      "epoch: 1, step: 325, outputs are 0.09181206\n",
      "epoch: 1, step: 326, outputs are 0.098402694\n",
      "epoch: 1, step: 327, outputs are 0.092757344\n",
      "epoch: 1, step: 328, outputs are 0.09804435\n",
      "epoch: 1, step: 329, outputs are 0.0903784\n",
      "epoch: 1, step: 330, outputs are 0.09190263\n",
      "epoch: 1, step: 331, outputs are 0.09712455\n",
      "epoch: 1, step: 332, outputs are 0.09138273\n",
      "epoch: 1, step: 333, outputs are 0.09004944\n",
      "epoch: 1, step: 334, outputs are 0.09080015\n",
      "epoch: 1, step: 335, outputs are 0.08791618\n",
      "epoch: 1, step: 336, outputs are 0.092812404\n",
      "epoch: 1, step: 337, outputs are 0.09216793\n",
      "epoch: 1, step: 338, outputs are 0.08276877\n",
      "epoch: 1, step: 339, outputs are 0.085403144\n",
      "epoch: 1, step: 340, outputs are 0.08845994\n",
      "epoch: 1, step: 341, outputs are 0.09447436\n",
      "epoch: 1, step: 342, outputs are 0.09343759\n",
      "epoch: 1, step: 343, outputs are 0.08686215\n",
      "epoch: 1, step: 344, outputs are 0.0893383\n",
      "epoch: 1, step: 345, outputs are 0.088628665\n",
      "epoch: 1, step: 346, outputs are 0.08245315\n",
      "epoch: 1, step: 347, outputs are 0.07967517\n",
      "epoch: 1, step: 348, outputs are 0.084365495\n",
      "epoch: 1, step: 349, outputs are 0.08814448\n",
      "epoch: 1, step: 350, outputs are 0.087957874\n",
      "epoch: 1, step: 351, outputs are 0.08620782\n",
      "epoch: 1, step: 352, outputs are 0.08625352\n",
      "epoch: 1, step: 353, outputs are 0.08039796\n",
      "epoch: 1, step: 354, outputs are 0.08685018\n",
      "epoch: 1, step: 355, outputs are 0.08037858\n",
      "epoch: 1, step: 356, outputs are 0.082740955\n",
      "epoch: 1, step: 357, outputs are 0.08060121\n",
      "epoch: 1, step: 358, outputs are 0.084702104\n",
      "epoch: 1, step: 359, outputs are 0.08197218\n",
      "epoch: 1, step: 360, outputs are 0.08645922\n",
      "epoch: 1, step: 361, outputs are 0.07794431\n",
      "epoch: 1, step: 362, outputs are 0.08345199\n",
      "epoch: 1, step: 363, outputs are 0.07736096\n",
      "epoch: 1, step: 364, outputs are 0.086355396\n",
      "epoch: 1, step: 365, outputs are 0.07898002\n",
      "epoch: 1, step: 366, outputs are 0.074164435\n",
      "epoch: 1, step: 367, outputs are 0.081877865\n",
      "epoch: 1, step: 368, outputs are 0.07734971\n",
      "epoch: 1, step: 369, outputs are 0.08455506\n",
      "epoch: 1, step: 370, outputs are 0.07712108\n",
      "epoch: 1, step: 371, outputs are 0.082843214\n",
      "epoch: 1, step: 372, outputs are 0.08003916\n",
      "epoch: 1, step: 373, outputs are 0.07719737\n",
      "epoch: 1, step: 374, outputs are 0.0754947\n",
      "epoch: 1, step: 375, outputs are 0.075785264\n",
      "epoch: 1, step: 376, outputs are 0.079303786\n",
      "epoch: 1, step: 377, outputs are 0.07970072\n",
      "epoch: 1, step: 378, outputs are 0.073382854\n",
      "epoch: 1, step: 379, outputs are 0.07843086\n",
      "epoch: 1, step: 380, outputs are 0.07642077\n",
      "epoch: 1, step: 381, outputs are 0.07883068\n",
      "epoch: 1, step: 382, outputs are 0.07584235\n",
      "epoch: 1, step: 383, outputs are 0.078452185\n",
      "epoch: 1, step: 384, outputs are 0.07389669\n",
      "epoch: 1, step: 385, outputs are 0.077018775\n",
      "epoch: 1, step: 386, outputs are 0.085281715\n",
      "epoch: 1, step: 387, outputs are 0.07450008\n",
      "epoch: 1, step: 388, outputs are 0.074897036\n",
      "epoch: 1, step: 389, outputs are 0.07672459\n",
      "epoch: 1, step: 390, outputs are 0.07382497\n",
      "epoch: 1, step: 391, outputs are 0.084300816\n",
      "epoch: 1, step: 392, outputs are 0.06853703\n",
      "epoch: 1, step: 393, outputs are 0.07935667\n",
      "epoch: 1, step: 394, outputs are 0.076437876\n",
      "epoch: 1, step: 395, outputs are 0.07940546\n",
      "epoch: 1, step: 396, outputs are 0.07199144\n",
      "epoch: 1, step: 397, outputs are 0.080208346\n",
      "epoch: 1, step: 398, outputs are 0.07571098\n",
      "epoch: 1, step: 399, outputs are 0.083150774\n",
      "epoch: 1, step: 400, outputs are 0.073486835\n",
      "epoch: 1, step: 401, outputs are 0.07389137\n",
      "epoch: 1, step: 402, outputs are 0.07460359\n",
      "epoch: 1, step: 403, outputs are 0.070365004\n",
      "epoch: 1, step: 404, outputs are 0.07887618\n",
      "epoch: 1, step: 405, outputs are 0.07580037\n",
      "epoch: 1, step: 406, outputs are 0.0754741\n",
      "epoch: 1, step: 407, outputs are 0.07020505\n",
      "epoch: 1, step: 408, outputs are 0.06656334\n",
      "epoch: 1, step: 409, outputs are 0.07700968\n",
      "epoch: 1, step: 410, outputs are 0.0813846\n",
      "epoch: 1, step: 411, outputs are 0.07612848\n",
      "epoch: 1, step: 412, outputs are 0.07751851\n",
      "epoch: 1, step: 413, outputs are 0.075365216\n",
      "epoch: 1, step: 414, outputs are 0.082551315\n",
      "epoch: 1, step: 415, outputs are 0.06770007\n",
      "epoch: 1, step: 416, outputs are 0.07128457\n",
      "epoch: 1, step: 417, outputs are 0.07590598\n",
      "epoch: 1, step: 418, outputs are 0.07324104\n",
      "epoch: 1, step: 419, outputs are 0.069790825\n",
      "epoch: 1, step: 420, outputs are 0.07189655\n",
      "epoch: 1, step: 421, outputs are 0.0773501\n",
      "epoch: 1, step: 422, outputs are 0.075381346\n",
      "epoch: 1, step: 423, outputs are 0.07230565\n",
      "epoch: 1, step: 424, outputs are 0.06868149\n",
      "epoch: 1, step: 425, outputs are 0.06810242\n",
      "epoch: 1, step: 426, outputs are 0.07155942\n",
      "epoch: 1, step: 427, outputs are 0.07643919\n",
      "epoch: 1, step: 428, outputs are 0.07274213\n",
      "epoch: 1, step: 429, outputs are 0.079035655\n",
      "epoch: 1, step: 430, outputs are 0.072742335\n",
      "epoch: 1, step: 431, outputs are 0.065050974\n",
      "epoch: 1, step: 432, outputs are 0.07092932\n",
      "epoch: 1, step: 433, outputs are 0.071981646\n",
      "epoch: 1, step: 434, outputs are 0.07264816\n",
      "epoch: 1, step: 435, outputs are 0.074934915\n",
      "epoch: 1, step: 436, outputs are 0.07367532\n",
      "epoch: 1, step: 437, outputs are 0.06658691\n",
      "epoch: 1, step: 438, outputs are 0.07098967\n",
      "epoch: 1, step: 439, outputs are 0.07497953\n",
      "epoch: 1, step: 440, outputs are 0.065677136\n",
      "epoch: 1, step: 441, outputs are 0.07204309\n",
      "epoch: 1, step: 442, outputs are 0.07087432\n",
      "epoch: 1, step: 443, outputs are 0.07142249\n",
      "epoch: 1, step: 444, outputs are 0.069348715\n",
      "epoch: 1, step: 445, outputs are 0.06528687\n",
      "epoch: 1, step: 446, outputs are 0.07082491\n",
      "epoch: 1, step: 447, outputs are 0.07234424\n",
      "epoch: 1, step: 448, outputs are 0.06589644\n",
      "epoch: 1, step: 449, outputs are 0.06993981\n",
      "epoch: 1, step: 450, outputs are 0.072521865\n",
      "epoch: 1, step: 451, outputs are 0.06558942\n",
      "epoch: 1, step: 452, outputs are 0.06242326\n",
      "epoch: 1, step: 453, outputs are 0.0711089\n",
      "epoch: 1, step: 454, outputs are 0.071538016\n",
      "epoch: 1, step: 455, outputs are 0.06948103\n",
      "epoch: 1, step: 456, outputs are 0.075976044\n",
      "epoch: 1, step: 457, outputs are 0.069606595\n",
      "epoch: 1, step: 458, outputs are 0.07374647\n",
      "epoch: 1, step: 459, outputs are 0.06721471\n",
      "epoch: 1, step: 460, outputs are 0.07064538\n",
      "epoch: 1, step: 461, outputs are 0.06282287\n",
      "epoch: 1, step: 462, outputs are 0.06585165\n",
      "epoch: 1, step: 463, outputs are 0.06792719\n",
      "epoch: 1, step: 464, outputs are 0.065245934\n",
      "epoch: 1, step: 465, outputs are 0.068500444\n",
      "epoch: 1, step: 466, outputs are 0.066545695\n",
      "epoch: 1, step: 467, outputs are 0.06473609\n",
      "epoch: 1, step: 468, outputs are 0.06835004\n",
      "epoch: 1, step: 469, outputs are 0.07282224\n",
      "epoch: 1, step: 470, outputs are 0.074438095\n",
      "epoch: 1, step: 471, outputs are 0.07162854\n",
      "epoch: 1, step: 472, outputs are 0.0657143\n",
      "epoch: 1, step: 473, outputs are 0.069294855\n",
      "epoch: 1, step: 474, outputs are 0.06358787\n",
      "epoch: 1, step: 475, outputs are 0.06668748\n",
      "epoch: 1, step: 476, outputs are 0.06619053\n",
      "epoch: 1, step: 477, outputs are 0.06625234\n",
      "epoch: 1, step: 478, outputs are 0.06509288\n",
      "epoch: 1, step: 479, outputs are 0.06227475\n",
      "epoch: 1, step: 480, outputs are 0.064549215\n",
      "epoch: 1, step: 481, outputs are 0.070158616\n",
      "epoch: 1, step: 482, outputs are 0.06356246\n",
      "epoch: 1, step: 483, outputs are 0.067623645\n",
      "epoch: 1, step: 484, outputs are 0.065581545\n",
      "epoch: 1, step: 485, outputs are 0.06329407\n",
      "epoch: 1, step: 486, outputs are 0.06476878\n",
      "epoch: 1, step: 487, outputs are 0.06361914\n",
      "epoch: 1, step: 488, outputs are 0.06348108\n",
      "epoch: 1, step: 489, outputs are 0.06648254\n",
      "epoch: 1, step: 490, outputs are 0.06635995\n",
      "epoch: 1, step: 491, outputs are 0.06011311\n",
      "epoch: 1, step: 492, outputs are 0.061961878\n",
      "epoch: 1, step: 493, outputs are 0.06666311\n",
      "epoch: 1, step: 494, outputs are 0.06212224\n",
      "epoch: 1, step: 495, outputs are 0.06272434\n",
      "epoch: 1, step: 496, outputs are 0.070717834\n",
      "epoch: 1, step: 497, outputs are 0.06341629\n",
      "epoch: 1, step: 498, outputs are 0.07199901\n",
      "epoch: 1, step: 499, outputs are 0.069045626\n",
      "epoch: 1, step: 500, outputs are 0.0674152\n",
      "epoch: 1, step: 501, outputs are 0.06441557\n",
      "epoch: 1, step: 502, outputs are 0.064162746\n",
      "epoch: 1, step: 503, outputs are 0.061946414\n",
      "epoch: 1, step: 504, outputs are 0.06698913\n",
      "epoch: 1, step: 505, outputs are 0.061663724\n",
      "epoch: 1, step: 506, outputs are 0.063683435\n",
      "epoch: 1, step: 507, outputs are 0.062813655\n",
      "epoch: 1, step: 508, outputs are 0.07464677\n",
      "epoch: 1, step: 509, outputs are 0.072736494\n",
      "epoch: 1, step: 510, outputs are 0.055740327\n",
      "epoch: 1, step: 511, outputs are 0.06712982\n",
      "epoch: 1, step: 512, outputs are 0.060716033\n",
      "epoch: 1, step: 513, outputs are 0.06703786\n",
      "epoch: 1, step: 514, outputs are 0.06654294\n",
      "epoch: 1, step: 515, outputs are 0.06258639\n",
      "epoch: 1, step: 516, outputs are 0.062401317\n",
      "epoch: 1, step: 517, outputs are 0.0597606\n",
      "epoch: 1, step: 518, outputs are 0.07182823\n",
      "epoch: 1, step: 519, outputs are 0.060320117\n",
      "epoch: 1, step: 520, outputs are 0.06738778\n",
      "epoch: 1, step: 521, outputs are 0.06440424\n",
      "epoch: 1, step: 522, outputs are 0.06715\n",
      "epoch: 1, step: 523, outputs are 0.061691053\n",
      "epoch: 1, step: 524, outputs are 0.063850984\n",
      "epoch: 1, step: 525, outputs are 0.062350977\n",
      "epoch: 1, step: 526, outputs are 0.06279285\n",
      "epoch: 1, step: 527, outputs are 0.06168648\n",
      "epoch: 1, step: 528, outputs are 0.069760576\n",
      "epoch: 1, step: 529, outputs are 0.061733797\n",
      "epoch: 1, step: 530, outputs are 0.060030628\n",
      "epoch: 1, step: 531, outputs are 0.06694475\n",
      "epoch: 1, step: 532, outputs are 0.060375754\n",
      "epoch: 1, step: 533, outputs are 0.062791936\n",
      "epoch: 1, step: 534, outputs are 0.06797102\n",
      "epoch: 1, step: 535, outputs are 0.062145017\n",
      "epoch: 1, step: 536, outputs are 0.06642474\n",
      "epoch: 1, step: 537, outputs are 0.0675953\n",
      "epoch: 1, step: 538, outputs are 0.054758944\n",
      "epoch: 1, step: 539, outputs are 0.061685864\n",
      "epoch: 1, step: 540, outputs are 0.061776835\n",
      "epoch: 1, step: 541, outputs are 0.059447236\n",
      "epoch: 1, step: 542, outputs are 0.059275832\n",
      "epoch: 1, step: 543, outputs are 0.06897705\n",
      "epoch: 1, step: 544, outputs are 0.0619334\n",
      "epoch: 1, step: 545, outputs are 0.06072317\n",
      "epoch: 1, step: 546, outputs are 0.06115607\n",
      "epoch: 1, step: 547, outputs are 0.059666123\n",
      "epoch: 1, step: 548, outputs are 0.06932257\n",
      "epoch: 1, step: 549, outputs are 0.05949336\n",
      "epoch: 1, step: 550, outputs are 0.05928178\n",
      "epoch: 1, step: 551, outputs are 0.05775751\n",
      "epoch: 1, step: 552, outputs are 0.06159281\n",
      "epoch: 1, step: 553, outputs are 0.06856037\n",
      "epoch: 1, step: 554, outputs are 0.06703147\n",
      "epoch: 1, step: 555, outputs are 0.061058782\n",
      "epoch: 1, step: 556, outputs are 0.05806759\n",
      "epoch: 1, step: 557, outputs are 0.062154967\n",
      "epoch: 1, step: 558, outputs are 0.06115806\n",
      "epoch: 1, step: 559, outputs are 0.06638935\n",
      "epoch: 1, step: 560, outputs are 0.06270814\n",
      "epoch: 1, step: 561, outputs are 0.059177656\n",
      "epoch: 1, step: 562, outputs are 0.06148779\n",
      "epoch: 1, step: 563, outputs are 0.06003541\n",
      "epoch: 1, step: 564, outputs are 0.059457835\n",
      "epoch: 1, step: 565, outputs are 0.056526132\n",
      "epoch: 1, step: 566, outputs are 0.06860718\n",
      "epoch: 1, step: 567, outputs are 0.060663305\n",
      "epoch: 1, step: 568, outputs are 0.06386824\n",
      "epoch: 1, step: 569, outputs are 0.059015274\n",
      "epoch: 1, step: 570, outputs are 0.060944654\n",
      "epoch: 1, step: 571, outputs are 0.057255067\n",
      "epoch: 1, step: 572, outputs are 0.057792604\n",
      "epoch: 1, step: 573, outputs are 0.0645955\n",
      "epoch: 1, step: 574, outputs are 0.061995305\n",
      "epoch: 1, step: 575, outputs are 0.06170985\n",
      "epoch: 1, step: 576, outputs are 0.058165673\n",
      "epoch: 1, step: 577, outputs are 0.060143426\n",
      "epoch: 1, step: 578, outputs are 0.059519775\n",
      "epoch: 1, step: 579, outputs are 0.061173648\n",
      "epoch: 1, step: 580, outputs are 0.06275874\n",
      "epoch: 1, step: 581, outputs are 0.065878555\n",
      "epoch: 1, step: 582, outputs are 0.05404777\n",
      "epoch: 1, step: 583, outputs are 0.065082565\n",
      "epoch: 1, step: 584, outputs are 0.06186245\n",
      "epoch: 1, step: 585, outputs are 0.06120149\n",
      "epoch: 1, step: 586, outputs are 0.060851946\n",
      "epoch: 1, step: 587, outputs are 0.060654834\n",
      "epoch: 1, step: 588, outputs are 0.06242892\n",
      "epoch: 1, step: 589, outputs are 0.06402612\n",
      "epoch: 1, step: 590, outputs are 0.058931895\n",
      "epoch: 1, step: 591, outputs are 0.06477234\n",
      "epoch: 1, step: 592, outputs are 0.058901146\n",
      "epoch: 1, step: 593, outputs are 0.05667469\n",
      "epoch: 1, step: 594, outputs are 0.058760528\n",
      "epoch: 1, step: 595, outputs are 0.056881342\n",
      "epoch: 1, step: 596, outputs are 0.06259358\n",
      "epoch: 1, step: 597, outputs are 0.07062262\n",
      "epoch: 1, step: 598, outputs are 0.06023273\n",
      "epoch: 1, step: 599, outputs are 0.05843138\n",
      "epoch: 1, step: 600, outputs are 0.05890498\n",
      "epoch: 1, step: 601, outputs are 0.060288653\n",
      "epoch: 1, step: 602, outputs are 0.060616344\n",
      "epoch: 1, step: 603, outputs are 0.057140198\n",
      "epoch: 1, step: 604, outputs are 0.060317997\n",
      "epoch: 1, step: 605, outputs are 0.060694546\n",
      "epoch: 1, step: 606, outputs are 0.06491886\n",
      "epoch: 1, step: 607, outputs are 0.057944693\n",
      "epoch: 1, step: 608, outputs are 0.06161818\n",
      "epoch: 1, step: 609, outputs are 0.06252408\n",
      "epoch: 1, step: 610, outputs are 0.06542392\n",
      "epoch: 1, step: 611, outputs are 0.05933144\n",
      "epoch: 1, step: 612, outputs are 0.055437014\n",
      "epoch: 1, step: 613, outputs are 0.05954513\n",
      "epoch: 1, step: 614, outputs are 0.06294328\n",
      "epoch: 1, step: 615, outputs are 0.05745311\n",
      "epoch: 1, step: 616, outputs are 0.059431925\n",
      "epoch: 1, step: 617, outputs are 0.054548696\n",
      "epoch: 1, step: 618, outputs are 0.054077692\n",
      "epoch: 1, step: 619, outputs are 0.05899309\n",
      "epoch: 1, step: 620, outputs are 0.0626596\n",
      "epoch: 1, step: 621, outputs are 0.06208448\n",
      "epoch: 1, step: 622, outputs are 0.054918174\n",
      "epoch: 1, step: 623, outputs are 0.059421428\n",
      "epoch: 1, step: 624, outputs are 0.058548108\n",
      "epoch: 1, step: 625, outputs are 0.054094613\n",
      "epoch: 1, step: 626, outputs are 0.0595887\n",
      "epoch: 1, step: 627, outputs are 0.05887945\n",
      "epoch: 1, step: 628, outputs are 0.056113046\n",
      "epoch: 1, step: 629, outputs are 0.05297857\n",
      "epoch: 1, step: 630, outputs are 0.060881056\n",
      "epoch: 1, step: 631, outputs are 0.057184197\n",
      "epoch: 1, step: 632, outputs are 0.0631458\n",
      "epoch: 1, step: 633, outputs are 0.056570247\n",
      "epoch: 1, step: 634, outputs are 0.0640791\n",
      "epoch: 1, step: 635, outputs are 0.055056892\n",
      "epoch: 1, step: 636, outputs are 0.057642203\n",
      "epoch: 1, step: 637, outputs are 0.064581044\n",
      "epoch: 1, step: 638, outputs are 0.059189044\n",
      "epoch: 1, step: 639, outputs are 0.057531193\n",
      "epoch: 1, step: 640, outputs are 0.05909633\n",
      "epoch: 1, step: 641, outputs are 0.0558357\n",
      "epoch: 1, step: 642, outputs are 0.060488347\n",
      "epoch: 1, step: 643, outputs are 0.057106078\n",
      "epoch: 1, step: 644, outputs are 0.059764177\n",
      "epoch: 1, step: 645, outputs are 0.057226717\n",
      "epoch: 1, step: 646, outputs are 0.058175564\n",
      "epoch: 1, step: 647, outputs are 0.05565255\n",
      "epoch: 1, step: 648, outputs are 0.055692565\n",
      "epoch: 1, step: 649, outputs are 0.057813436\n",
      "epoch: 1, step: 650, outputs are 0.062484875\n",
      "epoch: 1, step: 651, outputs are 0.06480162\n",
      "epoch: 1, step: 652, outputs are 0.05647359\n",
      "epoch: 1, step: 653, outputs are 0.060100887\n",
      "epoch: 1, step: 654, outputs are 0.06273107\n",
      "epoch: 1, step: 655, outputs are 0.05838917\n",
      "epoch: 1, step: 656, outputs are 0.057521332\n",
      "epoch: 1, step: 657, outputs are 0.059616696\n",
      "epoch: 1, step: 658, outputs are 0.05813299\n",
      "epoch: 1, step: 659, outputs are 0.057185076\n",
      "epoch: 1, step: 660, outputs are 0.053879637\n",
      "epoch: 1, step: 661, outputs are 0.064418495\n",
      "epoch: 1, step: 662, outputs are 0.06099136\n",
      "epoch: 1, step: 663, outputs are 0.059880313\n",
      "epoch: 1, step: 664, outputs are 0.06252389\n",
      "epoch: 1, step: 665, outputs are 0.058127012\n",
      "epoch: 1, step: 666, outputs are 0.05922675\n",
      "epoch: 1, step: 667, outputs are 0.060593143\n",
      "epoch: 1, step: 668, outputs are 0.058620874\n",
      "epoch: 1, step: 669, outputs are 0.05763763\n",
      "epoch: 1, step: 670, outputs are 0.058975883\n",
      "epoch: 1, step: 671, outputs are 0.06403701\n",
      "epoch: 1, step: 672, outputs are 0.060798883\n",
      "epoch: 1, step: 673, outputs are 0.059882563\n",
      "epoch: 1, step: 674, outputs are 0.056647338\n",
      "epoch: 1, step: 675, outputs are 0.06076844\n",
      "epoch: 1, step: 676, outputs are 0.06158537\n",
      "epoch: 1, step: 677, outputs are 0.060115762\n",
      "epoch: 1, step: 678, outputs are 0.058729738\n",
      "epoch: 1, step: 679, outputs are 0.061916724\n",
      "epoch: 1, step: 680, outputs are 0.057342537\n",
      "epoch: 1, step: 681, outputs are 0.06267625\n",
      "epoch: 1, step: 682, outputs are 0.057372704\n",
      "epoch: 1, step: 683, outputs are 0.06269181\n",
      "epoch: 1, step: 684, outputs are 0.06403812\n",
      "epoch: 1, step: 685, outputs are 0.06256247\n",
      "epoch: 1, step: 686, outputs are 0.055092357\n",
      "epoch: 1, step: 687, outputs are 0.056406677\n",
      "epoch: 1, step: 688, outputs are 0.062994204\n",
      "epoch: 1, step: 689, outputs are 0.061488986\n",
      "epoch: 1, step: 690, outputs are 0.05138139\n",
      "epoch: 1, step: 691, outputs are 0.060398795\n",
      "epoch: 1, step: 692, outputs are 0.058143653\n",
      "epoch: 1, step: 693, outputs are 0.059074014\n",
      "epoch: 1, step: 694, outputs are 0.062297948\n",
      "epoch: 1, step: 695, outputs are 0.060090505\n",
      "epoch: 1, step: 696, outputs are 0.056924388\n",
      "epoch: 1, step: 697, outputs are 0.057162218\n",
      "epoch: 1, step: 698, outputs are 0.05541454\n",
      "epoch: 1, step: 699, outputs are 0.058778398\n",
      "epoch: 1, step: 700, outputs are 0.05582206\n",
      "epoch: 1, step: 701, outputs are 0.05643742\n",
      "epoch: 1, step: 702, outputs are 0.056632757\n",
      "epoch: 1, step: 703, outputs are 0.05819401\n",
      "epoch: 1, step: 704, outputs are 0.05588419\n",
      "epoch: 1, step: 705, outputs are 0.0630083\n",
      "epoch: 1, step: 706, outputs are 0.05977816\n",
      "epoch: 1, step: 707, outputs are 0.058150988\n",
      "epoch: 1, step: 708, outputs are 0.05562139\n",
      "epoch: 1, step: 709, outputs are 0.06268953\n",
      "epoch: 1, step: 710, outputs are 0.05734571\n",
      "epoch: 1, step: 711, outputs are 0.056895938\n",
      "epoch: 1, step: 712, outputs are 0.05497751\n",
      "epoch: 1, step: 713, outputs are 0.054198615\n",
      "epoch: 1, step: 714, outputs are 0.05632606\n",
      "epoch: 1, step: 715, outputs are 0.059036806\n",
      "epoch: 1, step: 716, outputs are 0.061902612\n",
      "epoch: 1, step: 717, outputs are 0.057389654\n",
      "epoch: 1, step: 718, outputs are 0.059147276\n",
      "epoch: 1, step: 719, outputs are 0.06038046\n",
      "epoch: 1, step: 720, outputs are 0.05595214\n",
      "epoch: 1, step: 721, outputs are 0.06254983\n",
      "epoch: 1, step: 722, outputs are 0.058927134\n",
      "epoch: 1, step: 723, outputs are 0.059847973\n",
      "epoch: 1, step: 724, outputs are 0.058774494\n",
      "epoch: 1, step: 725, outputs are 0.055602632\n",
      "epoch: 1, step: 726, outputs are 0.056688707\n",
      "epoch: 1, step: 727, outputs are 0.055261858\n",
      "epoch: 1, step: 728, outputs are 0.058877803\n",
      "epoch: 1, step: 729, outputs are 0.053559683\n",
      "epoch: 1, step: 730, outputs are 0.06139759\n",
      "epoch: 1, step: 731, outputs are 0.05656504\n",
      "epoch: 1, step: 732, outputs are 0.053324766\n",
      "epoch: 1, step: 733, outputs are 0.061383113\n",
      "epoch: 1, step: 734, outputs are 0.057599917\n",
      "epoch: 1, step: 735, outputs are 0.057433672\n",
      "epoch: 1, step: 736, outputs are 0.057931308\n",
      "epoch: 1, step: 737, outputs are 0.062294852\n",
      "epoch: 1, step: 738, outputs are 0.05627252\n",
      "epoch: 1, step: 739, outputs are 0.054502167\n",
      "epoch: 1, step: 740, outputs are 0.061647266\n",
      "epoch: 1, step: 741, outputs are 0.05633599\n",
      "epoch: 1, step: 742, outputs are 0.059937235\n",
      "epoch: 1, step: 743, outputs are 0.054452777\n",
      "epoch: 1, step: 744, outputs are 0.05488802\n",
      "epoch: 1, step: 745, outputs are 0.06194686\n",
      "epoch: 1, step: 746, outputs are 0.05899635\n",
      "epoch: 1, step: 747, outputs are 0.057467073\n",
      "epoch: 1, step: 748, outputs are 0.05617626\n",
      "epoch: 1, step: 749, outputs are 0.055559807\n",
      "epoch: 1, step: 750, outputs are 0.056244954\n",
      "epoch: 1, step: 751, outputs are 0.060047187\n",
      "epoch: 1, step: 752, outputs are 0.056769207\n",
      "epoch: 1, step: 753, outputs are 0.057089135\n",
      "epoch: 1, step: 754, outputs are 0.060228236\n",
      "epoch: 1, step: 755, outputs are 0.059117597\n",
      "epoch: 1, step: 756, outputs are 0.057065204\n",
      "epoch: 1, step: 757, outputs are 0.057765037\n",
      "epoch: 1, step: 758, outputs are 0.054846406\n",
      "epoch: 1, step: 759, outputs are 0.060737472\n",
      "epoch: 1, step: 760, outputs are 0.054497506\n",
      "epoch: 1, step: 761, outputs are 0.058908846\n",
      "epoch: 1, step: 762, outputs are 0.06314775\n",
      "epoch: 1, step: 763, outputs are 0.056468025\n",
      "epoch: 1, step: 764, outputs are 0.05566317\n",
      "epoch: 1, step: 765, outputs are 0.056322217\n",
      "epoch: 1, step: 766, outputs are 0.05669491\n",
      "epoch: 1, step: 767, outputs are 0.055713266\n",
      "epoch: 1, step: 768, outputs are 0.059591923\n",
      "epoch: 1, step: 769, outputs are 0.06508652\n",
      "epoch: 1, step: 770, outputs are 0.06146619\n",
      "epoch: 1, step: 771, outputs are 0.05256796\n",
      "epoch: 1, step: 772, outputs are 0.053702362\n",
      "epoch: 1, step: 773, outputs are 0.057341542\n",
      "epoch: 1, step: 774, outputs are 0.054170087\n",
      "epoch: 1, step: 775, outputs are 0.05463496\n",
      "epoch: 1, step: 776, outputs are 0.05566143\n",
      "epoch: 1, step: 777, outputs are 0.055086177\n",
      "epoch: 1, step: 778, outputs are 0.05784425\n",
      "epoch: 1, step: 779, outputs are 0.055607647\n",
      "epoch: 1, step: 780, outputs are 0.05373774\n",
      "epoch: 1, step: 781, outputs are 0.06074618\n",
      "epoch: 1, step: 782, outputs are 0.059644297\n",
      "epoch: 1, step: 783, outputs are 0.05880443\n",
      "epoch: 1, step: 784, outputs are 0.061593093\n",
      "epoch: 1, step: 785, outputs are 0.059571803\n",
      "epoch: 1, step: 786, outputs are 0.05289744\n",
      "epoch: 1, step: 787, outputs are 0.054992363\n",
      "epoch: 1, step: 788, outputs are 0.05693402\n",
      "epoch: 1, step: 789, outputs are 0.06563333\n",
      "epoch: 1, step: 790, outputs are 0.058714245\n",
      "epoch: 1, step: 791, outputs are 0.051410977\n",
      "epoch: 1, step: 792, outputs are 0.053977\n",
      "epoch: 1, step: 793, outputs are 0.05452028\n",
      "epoch: 1, step: 794, outputs are 0.05967798\n",
      "epoch: 1, step: 795, outputs are 0.061232295\n",
      "epoch: 1, step: 796, outputs are 0.055804368\n",
      "epoch: 1, step: 797, outputs are 0.054373093\n",
      "epoch: 1, step: 798, outputs are 0.05818737\n",
      "epoch: 1, step: 799, outputs are 0.056390267\n",
      "epoch: 1, step: 800, outputs are 0.05910626\n",
      "epoch: 1, step: 801, outputs are 0.054084852\n",
      "epoch: 1, step: 802, outputs are 0.052919954\n",
      "epoch: 1, step: 803, outputs are 0.055829287\n",
      "epoch: 1, step: 804, outputs are 0.06391312\n",
      "epoch: 1, step: 805, outputs are 0.05590962\n",
      "epoch: 1, step: 806, outputs are 0.0583448\n",
      "epoch: 1, step: 807, outputs are 0.059938423\n",
      "epoch: 1, step: 808, outputs are 0.054421455\n",
      "epoch: 1, step: 809, outputs are 0.056335047\n",
      "epoch: 1, step: 810, outputs are 0.056316227\n",
      "epoch: 1, step: 811, outputs are 0.057370238\n",
      "epoch: 1, step: 812, outputs are 0.05360754\n",
      "epoch: 1, step: 813, outputs are 0.052745678\n",
      "epoch: 1, step: 814, outputs are 0.05665969\n",
      "epoch: 1, step: 815, outputs are 0.052675053\n",
      "epoch: 1, step: 816, outputs are 0.055779353\n",
      "epoch: 1, step: 817, outputs are 0.060174424\n",
      "epoch: 1, step: 818, outputs are 0.050760984\n",
      "epoch: 1, step: 819, outputs are 0.05526147\n",
      "epoch: 1, step: 820, outputs are 0.052918542\n",
      "epoch: 1, step: 821, outputs are 0.05866141\n",
      "epoch: 1, step: 822, outputs are 0.057973515\n",
      "epoch: 1, step: 823, outputs are 0.06189794\n",
      "epoch: 1, step: 824, outputs are 0.054517947\n",
      "epoch: 1, step: 825, outputs are 0.06059073\n",
      "epoch: 1, step: 826, outputs are 0.056616906\n",
      "epoch: 1, step: 827, outputs are 0.06010607\n",
      "epoch: 1, step: 828, outputs are 0.056461252\n",
      "epoch: 1, step: 829, outputs are 0.053072754\n",
      "epoch: 1, step: 830, outputs are 0.054358043\n",
      "epoch: 1, step: 831, outputs are 0.05563685\n",
      "epoch: 1, step: 832, outputs are 0.052792877\n",
      "epoch: 1, step: 833, outputs are 0.061727174\n",
      "epoch: 1, step: 834, outputs are 0.057358637\n",
      "epoch: 1, step: 835, outputs are 0.059783764\n",
      "epoch: 1, step: 836, outputs are 0.059634417\n",
      "epoch: 1, step: 837, outputs are 0.05401046\n",
      "epoch: 1, step: 838, outputs are 0.053905398\n",
      "epoch: 1, step: 839, outputs are 0.059976593\n",
      "epoch: 1, step: 840, outputs are 0.05372483\n",
      "epoch: 1, step: 841, outputs are 0.057260375\n",
      "epoch: 1, step: 842, outputs are 0.058690287\n",
      "epoch: 1, step: 843, outputs are 0.06057854\n",
      "epoch: 1, step: 844, outputs are 0.061601393\n",
      "epoch: 1, step: 845, outputs are 0.055402838\n",
      "epoch: 1, step: 846, outputs are 0.061166056\n",
      "epoch: 1, step: 847, outputs are 0.053729616\n",
      "epoch: 1, step: 848, outputs are 0.056139432\n",
      "epoch: 1, step: 849, outputs are 0.055195756\n",
      "epoch: 1, step: 850, outputs are 0.05285361\n",
      "epoch: 1, step: 851, outputs are 0.053193536\n",
      "epoch: 1, step: 852, outputs are 0.058626823\n",
      "epoch: 1, step: 853, outputs are 0.053762924\n",
      "epoch: 1, step: 854, outputs are 0.05990314\n",
      "epoch: 1, step: 855, outputs are 0.055947177\n",
      "epoch: 1, step: 856, outputs are 0.054120667\n",
      "epoch: 1, step: 857, outputs are 0.056286387\n",
      "epoch: 1, step: 858, outputs are 0.057153694\n",
      "epoch: 1, step: 859, outputs are 0.05759875\n",
      "epoch: 1, step: 860, outputs are 0.05340007\n",
      "epoch: 1, step: 861, outputs are 0.05677585\n",
      "epoch: 1, step: 862, outputs are 0.059710775\n",
      "epoch: 1, step: 863, outputs are 0.06201457\n",
      "epoch: 1, step: 864, outputs are 0.05043418\n",
      "epoch: 1, step: 865, outputs are 0.058744714\n",
      "epoch: 1, step: 866, outputs are 0.05351677\n",
      "epoch: 1, step: 867, outputs are 0.05724731\n",
      "epoch: 1, step: 868, outputs are 0.058699816\n",
      "epoch: 1, step: 869, outputs are 0.05365271\n",
      "epoch: 1, step: 870, outputs are 0.05327027\n",
      "epoch: 1, step: 871, outputs are 0.054747693\n",
      "epoch: 1, step: 872, outputs are 0.0570669\n",
      "epoch: 1, step: 873, outputs are 0.057492834\n",
      "epoch: 1, step: 874, outputs are 0.058400482\n",
      "epoch: 1, step: 875, outputs are 0.054092497\n",
      "epoch: 1, step: 876, outputs are 0.056211874\n",
      "epoch: 1, step: 877, outputs are 0.054603517\n",
      "epoch: 1, step: 878, outputs are 0.053649858\n",
      "epoch: 1, step: 879, outputs are 0.053776063\n",
      "epoch: 1, step: 880, outputs are 0.050808817\n",
      "epoch: 1, step: 881, outputs are 0.05435682\n",
      "epoch: 1, step: 882, outputs are 0.060221773\n",
      "epoch: 1, step: 883, outputs are 0.055729195\n",
      "epoch: 1, step: 884, outputs are 0.05533783\n",
      "epoch: 1, step: 885, outputs are 0.054595344\n",
      "epoch: 1, step: 886, outputs are 0.052322946\n",
      "epoch: 1, step: 887, outputs are 0.055922024\n",
      "epoch: 1, step: 888, outputs are 0.054637402\n",
      "epoch: 1, step: 889, outputs are 0.053964205\n",
      "epoch: 1, step: 890, outputs are 0.055200707\n",
      "epoch: 1, step: 891, outputs are 0.054778203\n",
      "epoch: 1, step: 892, outputs are 0.05394326\n",
      "epoch: 1, step: 893, outputs are 0.059586957\n",
      "epoch: 1, step: 894, outputs are 0.057857282\n",
      "epoch: 1, step: 895, outputs are 0.05294221\n",
      "epoch: 1, step: 896, outputs are 0.05299128\n",
      "epoch: 1, step: 897, outputs are 0.05453455\n",
      "epoch: 1, step: 898, outputs are 0.051266156\n",
      "epoch: 1, step: 899, outputs are 0.054841124\n",
      "epoch: 1, step: 900, outputs are 0.055055004\n",
      "epoch: 1, step: 901, outputs are 0.053011708\n",
      "epoch: 1, step: 902, outputs are 0.052190088\n",
      "epoch: 1, step: 903, outputs are 0.05522211\n",
      "epoch: 1, step: 904, outputs are 0.053725243\n",
      "epoch: 1, step: 905, outputs are 0.05626999\n",
      "epoch: 1, step: 906, outputs are 0.05928032\n",
      "epoch: 1, step: 907, outputs are 0.052787684\n",
      "epoch: 1, step: 908, outputs are 0.053710096\n",
      "epoch: 1, step: 909, outputs are 0.056609847\n",
      "epoch: 1, step: 910, outputs are 0.05435146\n",
      "epoch: 1, step: 911, outputs are 0.058455065\n",
      "epoch: 1, step: 912, outputs are 0.058724947\n",
      "epoch: 1, step: 913, outputs are 0.059804812\n",
      "epoch: 1, step: 914, outputs are 0.05736769\n",
      "epoch: 1, step: 915, outputs are 0.053630292\n",
      "epoch: 1, step: 916, outputs are 0.054766342\n",
      "epoch: 1, step: 917, outputs are 0.05457328\n",
      "epoch: 1, step: 918, outputs are 0.060064346\n",
      "epoch: 1, step: 919, outputs are 0.05918617\n",
      "epoch: 1, step: 920, outputs are 0.053063795\n",
      "epoch: 1, step: 921, outputs are 0.059589956\n",
      "epoch: 1, step: 922, outputs are 0.05376994\n",
      "epoch: 1, step: 923, outputs are 0.05597137\n",
      "epoch: 1, step: 924, outputs are 0.05717461\n",
      "epoch: 1, step: 925, outputs are 0.058713704\n",
      "epoch: 1, step: 926, outputs are 0.053246498\n",
      "epoch: 1, step: 927, outputs are 0.053393148\n",
      "epoch: 1, step: 928, outputs are 0.058501817\n",
      "epoch: 1, step: 929, outputs are 0.054169346\n",
      "epoch: 1, step: 930, outputs are 0.05654644\n",
      "epoch: 1, step: 931, outputs are 0.053871967\n",
      "epoch: 1, step: 932, outputs are 0.057095654\n",
      "epoch: 1, step: 933, outputs are 0.056232765\n",
      "epoch: 1, step: 934, outputs are 0.057860926\n",
      "epoch: 1, step: 935, outputs are 0.05414441\n",
      "epoch: 1, step: 936, outputs are 0.054685846\n",
      "epoch: 1, step: 937, outputs are 0.05933448\n",
      "epoch: 1, step: 938, outputs are 0.054468438\n",
      "epoch: 1, step: 939, outputs are 0.055484768\n",
      "epoch: 1, step: 940, outputs are 0.05559066\n",
      "epoch: 1, step: 941, outputs are 0.059268214\n",
      "epoch: 1, step: 942, outputs are 0.054260183\n",
      "epoch: 1, step: 943, outputs are 0.050386906\n",
      "epoch: 1, step: 944, outputs are 0.059693553\n",
      "epoch: 1, step: 945, outputs are 0.05643533\n",
      "epoch: 1, step: 946, outputs are 0.060179643\n",
      "epoch: 1, step: 947, outputs are 0.055444323\n",
      "epoch: 1, step: 948, outputs are 0.05184582\n",
      "epoch: 1, step: 949, outputs are 0.053610902\n",
      "epoch: 1, step: 950, outputs are 0.054794155\n",
      "epoch: 1, step: 951, outputs are 0.057234157\n",
      "epoch: 1, step: 952, outputs are 0.059373185\n",
      "epoch: 1, step: 953, outputs are 0.055522278\n",
      "epoch: 1, step: 954, outputs are 0.058540955\n",
      "epoch: 1, step: 955, outputs are 0.053781323\n",
      "epoch: 1, step: 956, outputs are 0.05971229\n",
      "epoch: 1, step: 957, outputs are 0.056093376\n",
      "epoch: 1, step: 958, outputs are 0.053084508\n",
      "epoch: 1, step: 959, outputs are 0.056909576\n",
      "epoch: 1, step: 960, outputs are 0.051856294\n",
      "epoch: 1, step: 961, outputs are 0.05428946\n",
      "epoch: 1, step: 962, outputs are 0.061650954\n",
      "epoch: 1, step: 963, outputs are 0.057678662\n",
      "epoch: 1, step: 964, outputs are 0.05794742\n",
      "epoch: 1, step: 965, outputs are 0.05519084\n",
      "epoch: 1, step: 966, outputs are 0.05389876\n",
      "epoch: 1, step: 967, outputs are 0.056202687\n",
      "epoch: 1, step: 968, outputs are 0.050480485\n",
      "epoch: 1, step: 969, outputs are 0.05426093\n",
      "epoch: 1, step: 970, outputs are 0.056259006\n",
      "epoch: 1, step: 971, outputs are 0.051567957\n",
      "epoch: 1, step: 972, outputs are 0.05811651\n",
      "epoch: 1, step: 973, outputs are 0.05470343\n",
      "epoch: 1, step: 974, outputs are 0.057364017\n",
      "epoch: 1, step: 975, outputs are 0.05748474\n",
      "epoch: 1, step: 976, outputs are 0.05794735\n",
      "epoch: 1, step: 977, outputs are 0.054179437\n",
      "epoch: 1, step: 978, outputs are 0.052064344\n",
      "epoch: 1, step: 979, outputs are 0.05122613\n",
      "epoch: 1, step: 980, outputs are 0.05386056\n",
      "epoch: 1, step: 981, outputs are 0.050850686\n",
      "epoch: 1, step: 982, outputs are 0.053030867\n",
      "epoch: 1, step: 983, outputs are 0.049807698\n",
      "epoch: 1, step: 984, outputs are 0.05412841\n",
      "epoch: 1, step: 985, outputs are 0.055017106\n",
      "epoch: 1, step: 986, outputs are 0.05617044\n",
      "epoch: 1, step: 987, outputs are 0.05312895\n",
      "epoch: 1, step: 988, outputs are 0.05849122\n",
      "epoch: 1, step: 989, outputs are 0.056862086\n",
      "epoch: 1, step: 990, outputs are 0.060196355\n",
      "epoch: 1, step: 991, outputs are 0.052023\n",
      "epoch: 1, step: 992, outputs are 0.052997105\n",
      "epoch: 1, step: 993, outputs are 0.057547078\n",
      "epoch: 1, step: 994, outputs are 0.04976289\n",
      "epoch: 1, step: 995, outputs are 0.05679389\n",
      "epoch: 1, step: 996, outputs are 0.0563142\n",
      "epoch: 1, step: 997, outputs are 0.058054157\n",
      "epoch: 1, step: 998, outputs are 0.05128845\n",
      "epoch: 1, step: 999, outputs are 0.050057843\n",
      "epoch: 1, step: 1000, outputs are 0.056413352\n",
      "epoch: 1, step: 1001, outputs are 0.05775535\n",
      "epoch: 1, step: 1002, outputs are 0.057161488\n",
      "epoch: 1, step: 1003, outputs are 0.054745782\n",
      "epoch: 1, step: 1004, outputs are 0.054680623\n",
      "epoch: 1, step: 1005, outputs are 0.055449113\n",
      "epoch: 1, step: 1006, outputs are 0.059154272\n",
      "epoch: 1, step: 1007, outputs are 0.0563381\n",
      "epoch: 1, step: 1008, outputs are 0.05990538\n",
      "epoch: 1, step: 1009, outputs are 0.055810694\n",
      "epoch: 1, step: 1010, outputs are 0.053230852\n",
      "epoch: 1, step: 1011, outputs are 0.052884802\n",
      "epoch: 1, step: 1012, outputs are 0.05632124\n",
      "epoch: 1, step: 1013, outputs are 0.05630768\n",
      "epoch: 1, step: 1014, outputs are 0.057317726\n",
      "epoch: 1, step: 1015, outputs are 0.058387145\n",
      "epoch: 1, step: 1016, outputs are 0.053648546\n",
      "epoch: 1, step: 1017, outputs are 0.05368227\n",
      "epoch: 1, step: 1018, outputs are 0.057712585\n",
      "epoch: 1, step: 1019, outputs are 0.05471886\n",
      "epoch: 1, step: 1020, outputs are 0.056861006\n",
      "epoch: 1, step: 1021, outputs are 0.055681966\n",
      "epoch: 1, step: 1022, outputs are 0.053922057\n",
      "epoch: 1, step: 1023, outputs are 0.052094698\n",
      "epoch: 1, step: 1024, outputs are 0.052699547\n",
      "epoch: 1, step: 1025, outputs are 0.054989256\n",
      "epoch: 1, step: 1026, outputs are 0.053716317\n",
      "epoch: 1, step: 1027, outputs are 0.05846057\n",
      "epoch: 1, step: 1028, outputs are 0.054495312\n",
      "epoch: 1, step: 1029, outputs are 0.054537557\n",
      "epoch: 1, step: 1030, outputs are 0.052824393\n",
      "epoch: 1, step: 1031, outputs are 0.051793143\n",
      "epoch: 1, step: 1032, outputs are 0.054370545\n",
      "epoch: 1, step: 1033, outputs are 0.053814203\n",
      "epoch: 1, step: 1034, outputs are 0.05726135\n",
      "epoch: 1, step: 1035, outputs are 0.059517153\n",
      "epoch: 1, step: 1036, outputs are 0.05566024\n",
      "epoch: 1, step: 1037, outputs are 0.054796025\n",
      "epoch: 1, step: 1038, outputs are 0.0524221\n",
      "epoch: 1, step: 1039, outputs are 0.052598648\n",
      "epoch: 1, step: 1040, outputs are 0.055433817\n",
      "epoch: 1, step: 1041, outputs are 0.0496636\n",
      "epoch: 1, step: 1042, outputs are 0.056241624\n",
      "epoch: 1, step: 1043, outputs are 0.054996606\n",
      "epoch: 1, step: 1044, outputs are 0.052108306\n",
      "epoch: 1, step: 1045, outputs are 0.055624455\n",
      "epoch: 1, step: 1046, outputs are 0.055797495\n",
      "epoch: 1, step: 1047, outputs are 0.051458042\n",
      "epoch: 1, step: 1048, outputs are 0.0562403\n",
      "epoch: 1, step: 1049, outputs are 0.059619263\n",
      "epoch: 1, step: 1050, outputs are 0.051728956\n",
      "epoch: 1, step: 1051, outputs are 0.04920946\n",
      "epoch: 1, step: 1052, outputs are 0.060567442\n",
      "epoch: 1, step: 1053, outputs are 0.05883342\n",
      "epoch: 1, step: 1054, outputs are 0.057990044\n",
      "epoch: 1, step: 1055, outputs are 0.05838444\n",
      "epoch: 1, step: 1056, outputs are 0.05068733\n",
      "epoch: 1, step: 1057, outputs are 0.05694887\n",
      "epoch: 1, step: 1058, outputs are 0.055897433\n",
      "epoch: 1, step: 1059, outputs are 0.060904503\n",
      "epoch: 1, step: 1060, outputs are 0.055764195\n",
      "epoch: 1, step: 1061, outputs are 0.049653046\n",
      "epoch: 1, step: 1062, outputs are 0.05561952\n",
      "epoch: 1, step: 1063, outputs are 0.05979316\n",
      "epoch: 1, step: 1064, outputs are 0.054325595\n",
      "epoch: 1, step: 1065, outputs are 0.051190436\n",
      "epoch: 1, step: 1066, outputs are 0.050397187\n",
      "epoch: 1, step: 1067, outputs are 0.050189942\n",
      "epoch: 1, step: 1068, outputs are 0.05205238\n",
      "epoch: 1, step: 1069, outputs are 0.06000468\n",
      "epoch: 1, step: 1070, outputs are 0.055054747\n",
      "epoch: 1, step: 1071, outputs are 0.04785718\n",
      "epoch: 1, step: 1072, outputs are 0.055940587\n",
      "epoch: 1, step: 1073, outputs are 0.04959039\n",
      "epoch: 1, step: 1074, outputs are 0.0554447\n",
      "epoch: 1, step: 1075, outputs are 0.051741205\n",
      "epoch: 1, step: 1076, outputs are 0.05313621\n",
      "epoch: 1, step: 1077, outputs are 0.054649357\n",
      "epoch: 1, step: 1078, outputs are 0.052233167\n",
      "epoch: 1, step: 1079, outputs are 0.055403113\n",
      "epoch: 1, step: 1080, outputs are 0.053956456\n",
      "epoch: 1, step: 1081, outputs are 0.052200045\n",
      "epoch: 1, step: 1082, outputs are 0.04981921\n",
      "epoch: 1, step: 1083, outputs are 0.06033074\n",
      "epoch: 1, step: 1084, outputs are 0.060325332\n",
      "epoch: 1, step: 1085, outputs are 0.05730831\n",
      "epoch: 1, step: 1086, outputs are 0.059388686\n",
      "epoch: 1, step: 1087, outputs are 0.05089835\n",
      "epoch: 1, step: 1088, outputs are 0.05309423\n",
      "epoch: 1, step: 1089, outputs are 0.051575072\n",
      "epoch: 1, step: 1090, outputs are 0.058990516\n",
      "epoch: 1, step: 1091, outputs are 0.0515172\n",
      "epoch: 1, step: 1092, outputs are 0.05531547\n",
      "epoch: 1, step: 1093, outputs are 0.049496263\n",
      "epoch: 1, step: 1094, outputs are 0.05541856\n",
      "epoch: 1, step: 1095, outputs are 0.056151688\n",
      "epoch: 1, step: 1096, outputs are 0.05587715\n",
      "epoch: 1, step: 1097, outputs are 0.05620995\n",
      "epoch: 1, step: 1098, outputs are 0.04934309\n",
      "epoch: 1, step: 1099, outputs are 0.051977485\n",
      "epoch: 1, step: 1100, outputs are 0.049613476\n",
      "epoch: 1, step: 1101, outputs are 0.052897662\n",
      "epoch: 1, step: 1102, outputs are 0.05980255\n",
      "epoch: 1, step: 1103, outputs are 0.053387657\n",
      "epoch: 1, step: 1104, outputs are 0.05413056\n",
      "epoch: 1, step: 1105, outputs are 0.05218985\n",
      "epoch: 1, step: 1106, outputs are 0.054516234\n",
      "epoch: 1, step: 1107, outputs are 0.057839993\n",
      "epoch: 1, step: 1108, outputs are 0.05173886\n",
      "epoch: 1, step: 1109, outputs are 0.051607244\n",
      "epoch: 1, step: 1110, outputs are 0.054101024\n",
      "epoch: 1, step: 1111, outputs are 0.054915234\n",
      "epoch: 1, step: 1112, outputs are 0.051870376\n",
      "epoch: 1, step: 1113, outputs are 0.055213377\n",
      "epoch: 1, step: 1114, outputs are 0.053266376\n",
      "epoch: 1, step: 1115, outputs are 0.05462478\n",
      "epoch: 1, step: 1116, outputs are 0.050657716\n",
      "epoch: 1, step: 1117, outputs are 0.053988203\n",
      "epoch: 1, step: 1118, outputs are 0.06038902\n",
      "epoch: 1, step: 1119, outputs are 0.051777214\n",
      "epoch: 1, step: 1120, outputs are 0.054708682\n",
      "epoch: 1, step: 1121, outputs are 0.05024417\n",
      "epoch: 1, step: 1122, outputs are 0.055665568\n",
      "epoch: 1, step: 1123, outputs are 0.054999128\n",
      "epoch: 1, step: 1124, outputs are 0.05286311\n",
      "epoch: 1, step: 1125, outputs are 0.05397781\n",
      "epoch: 1, step: 1126, outputs are 0.055378333\n",
      "epoch: 1, step: 1127, outputs are 0.05748789\n",
      "epoch: 1, step: 1128, outputs are 0.05562281\n",
      "epoch: 1, step: 1129, outputs are 0.05126802\n",
      "epoch: 1, step: 1130, outputs are 0.054617897\n",
      "epoch: 1, step: 1131, outputs are 0.05720949\n",
      "epoch: 1, step: 1132, outputs are 0.059863783\n",
      "epoch: 1, step: 1133, outputs are 0.049952686\n",
      "epoch: 1, step: 1134, outputs are 0.054092474\n",
      "epoch: 1, step: 1135, outputs are 0.057788033\n",
      "epoch: 1, step: 1136, outputs are 0.055271596\n",
      "epoch: 1, step: 1137, outputs are 0.049988717\n",
      "epoch: 1, step: 1138, outputs are 0.052983265\n",
      "epoch: 1, step: 1139, outputs are 0.051576152\n",
      "epoch: 1, step: 1140, outputs are 0.053826034\n",
      "epoch: 1, step: 1141, outputs are 0.055263333\n",
      "epoch: 1, step: 1142, outputs are 0.05159143\n",
      "epoch: 1, step: 1143, outputs are 0.050678097\n",
      "epoch: 1, step: 1144, outputs are 0.048446693\n",
      "epoch: 1, step: 1145, outputs are 0.05436859\n",
      "epoch: 1, step: 1146, outputs are 0.054440483\n",
      "epoch: 1, step: 1147, outputs are 0.052349173\n",
      "epoch: 1, step: 1148, outputs are 0.058731526\n",
      "epoch: 1, step: 1149, outputs are 0.05274743\n",
      "epoch: 1, step: 1150, outputs are 0.049331196\n",
      "epoch: 1, step: 1151, outputs are 0.05035725\n",
      "epoch: 1, step: 1152, outputs are 0.060938686\n",
      "epoch: 1, step: 1153, outputs are 0.052782618\n",
      "epoch: 1, step: 1154, outputs are 0.060786396\n",
      "epoch: 1, step: 1155, outputs are 0.05275335\n",
      "epoch: 1, step: 1156, outputs are 0.05842123\n",
      "epoch: 1, step: 1157, outputs are 0.051522303\n",
      "epoch: 1, step: 1158, outputs are 0.048768885\n",
      "epoch: 1, step: 1159, outputs are 0.058381878\n",
      "epoch: 1, step: 1160, outputs are 0.05572591\n",
      "epoch: 1, step: 1161, outputs are 0.05863715\n",
      "epoch: 1, step: 1162, outputs are 0.053401783\n",
      "epoch: 1, step: 1163, outputs are 0.05524969\n",
      "epoch: 1, step: 1164, outputs are 0.0509837\n",
      "epoch: 1, step: 1165, outputs are 0.05182135\n",
      "epoch: 1, step: 1166, outputs are 0.05570621\n",
      "epoch: 1, step: 1167, outputs are 0.054649524\n",
      "epoch: 1, step: 1168, outputs are 0.054403987\n",
      "epoch: 1, step: 1169, outputs are 0.056268476\n",
      "epoch: 1, step: 1170, outputs are 0.050442196\n",
      "epoch: 1, step: 1171, outputs are 0.054492097\n",
      "epoch: 1, step: 1172, outputs are 0.056925338\n",
      "epoch: 1, step: 1173, outputs are 0.052931458\n",
      "epoch: 1, step: 1174, outputs are 0.049149644\n",
      "epoch: 1, step: 1175, outputs are 0.057011105\n",
      "epoch: 1, step: 1176, outputs are 0.052587897\n",
      "epoch: 1, step: 1177, outputs are 0.056055445\n",
      "epoch: 1, step: 1178, outputs are 0.053650435\n",
      "epoch: 1, step: 1179, outputs are 0.05750529\n",
      "epoch: 1, step: 1180, outputs are 0.049101345\n",
      "epoch: 1, step: 1181, outputs are 0.0547767\n",
      "epoch: 1, step: 1182, outputs are 0.050335087\n",
      "epoch: 1, step: 1183, outputs are 0.055581324\n",
      "epoch: 1, step: 1184, outputs are 0.05092255\n",
      "epoch: 1, step: 1185, outputs are 0.05613498\n",
      "epoch: 1, step: 1186, outputs are 0.0508752\n",
      "epoch: 1, step: 1187, outputs are 0.062105827\n",
      "epoch: 1, step: 1188, outputs are 0.059496082\n",
      "epoch: 1, step: 1189, outputs are 0.052755885\n",
      "epoch: 1, step: 1190, outputs are 0.058842286\n",
      "epoch: 1, step: 1191, outputs are 0.05411735\n",
      "epoch: 1, step: 1192, outputs are 0.058377393\n",
      "epoch: 1, step: 1193, outputs are 0.057990126\n",
      "epoch: 1, step: 1194, outputs are 0.052548394\n",
      "epoch: 1, step: 1195, outputs are 0.053563416\n",
      "epoch: 1, step: 1196, outputs are 0.04990584\n",
      "epoch: 1, step: 1197, outputs are 0.05531755\n",
      "epoch: 1, step: 1198, outputs are 0.057983756\n",
      "epoch: 1, step: 1199, outputs are 0.059788555\n",
      "epoch: 1, step: 1200, outputs are 0.04729338\n",
      "epoch: 1, step: 1201, outputs are 0.050641384\n",
      "epoch: 1, step: 1202, outputs are 0.05780886\n",
      "epoch: 1, step: 1203, outputs are 0.05202557\n",
      "epoch: 1, step: 1204, outputs are 0.054412972\n",
      "epoch: 1, step: 1205, outputs are 0.05389555\n",
      "epoch: 1, step: 1206, outputs are 0.060487434\n",
      "epoch: 1, step: 1207, outputs are 0.05039434\n",
      "epoch: 1, step: 1208, outputs are 0.053423733\n",
      "epoch: 1, step: 1209, outputs are 0.049383935\n",
      "epoch: 1, step: 1210, outputs are 0.053882994\n",
      "epoch: 1, step: 1211, outputs are 0.050828185\n",
      "epoch: 1, step: 1212, outputs are 0.054607876\n",
      "epoch: 1, step: 1213, outputs are 0.056386746\n",
      "epoch: 1, step: 1214, outputs are 0.051187623\n",
      "epoch: 1, step: 1215, outputs are 0.05514743\n",
      "epoch: 1, step: 1216, outputs are 0.048043564\n",
      "epoch: 1, step: 1217, outputs are 0.052726425\n",
      "epoch: 1, step: 1218, outputs are 0.051748216\n",
      "epoch: 1, step: 1219, outputs are 0.050677754\n",
      "epoch: 1, step: 1220, outputs are 0.050540477\n",
      "epoch: 1, step: 1221, outputs are 0.053613625\n",
      "epoch: 1, step: 1222, outputs are 0.059523165\n",
      "epoch: 1, step: 1223, outputs are 0.053674404\n",
      "epoch: 1, step: 1224, outputs are 0.048182607\n",
      "epoch: 1, step: 1225, outputs are 0.05320157\n",
      "epoch: 1, step: 1226, outputs are 0.05220384\n",
      "epoch: 1, step: 1227, outputs are 0.05323326\n",
      "epoch: 1, step: 1228, outputs are 0.048323374\n",
      "epoch: 1, step: 1229, outputs are 0.054431707\n",
      "epoch: 1, step: 1230, outputs are 0.056195684\n",
      "epoch: 1, step: 1231, outputs are 0.055793248\n",
      "epoch: 1, step: 1232, outputs are 0.05218637\n",
      "epoch: 1, step: 1233, outputs are 0.05296924\n",
      "epoch: 1, step: 1234, outputs are 0.053665563\n",
      "epoch: 1, step: 1235, outputs are 0.05479861\n",
      "epoch: 1, step: 1236, outputs are 0.05777421\n",
      "epoch: 1, step: 1237, outputs are 0.05552413\n",
      "epoch: 1, step: 1238, outputs are 0.05260452\n",
      "epoch: 1, step: 1239, outputs are 0.049693152\n",
      "epoch: 1, step: 1240, outputs are 0.048561115\n",
      "epoch: 1, step: 1241, outputs are 0.051097207\n",
      "epoch: 1, step: 1242, outputs are 0.05089406\n",
      "epoch: 1, step: 1243, outputs are 0.05352749\n",
      "epoch: 1, step: 1244, outputs are 0.05442922\n",
      "epoch: 1, step: 1245, outputs are 0.05771467\n",
      "epoch: 1, step: 1246, outputs are 0.05474494\n",
      "epoch: 1, step: 1247, outputs are 0.049631923\n",
      "epoch: 1, step: 1248, outputs are 0.055297174\n",
      "epoch: 1, step: 1249, outputs are 0.052196465\n",
      "epoch: 1, step: 1250, outputs are 0.05082088\n",
      "epoch: 1, step: 1251, outputs are 0.054407876\n",
      "epoch: 1, step: 1252, outputs are 0.05725278\n",
      "epoch: 1, step: 1253, outputs are 0.04952482\n",
      "epoch: 1, step: 1254, outputs are 0.049224928\n",
      "epoch: 1, step: 1255, outputs are 0.056632325\n",
      "epoch: 1, step: 1256, outputs are 0.0541867\n",
      "epoch: 1, step: 1257, outputs are 0.050024558\n",
      "epoch: 1, step: 1258, outputs are 0.05784501\n",
      "epoch: 1, step: 1259, outputs are 0.051375456\n",
      "epoch: 1, step: 1260, outputs are 0.053560577\n",
      "epoch: 1, step: 1261, outputs are 0.052952394\n",
      "epoch: 1, step: 1262, outputs are 0.04778257\n",
      "epoch: 1, step: 1263, outputs are 0.05909569\n",
      "epoch: 1, step: 1264, outputs are 0.056283083\n",
      "epoch: 1, step: 1265, outputs are 0.048034273\n",
      "epoch: 1, step: 1266, outputs are 0.05015881\n",
      "epoch: 1, step: 1267, outputs are 0.05235835\n",
      "epoch: 1, step: 1268, outputs are 0.048945256\n",
      "epoch: 1, step: 1269, outputs are 0.046827603\n",
      "epoch: 1, step: 1270, outputs are 0.050197523\n",
      "epoch: 1, step: 1271, outputs are 0.05314777\n",
      "epoch: 1, step: 1272, outputs are 0.05059816\n",
      "epoch: 1, step: 1273, outputs are 0.060965557\n",
      "epoch: 1, step: 1274, outputs are 0.055254996\n",
      "epoch: 1, step: 1275, outputs are 0.050667975\n",
      "epoch: 1, step: 1276, outputs are 0.051466994\n",
      "epoch: 1, step: 1277, outputs are 0.057056308\n",
      "epoch: 1, step: 1278, outputs are 0.052709255\n",
      "epoch: 1, step: 1279, outputs are 0.05280212\n",
      "epoch: 1, step: 1280, outputs are 0.049619026\n",
      "epoch: 1, step: 1281, outputs are 0.05013277\n",
      "epoch: 1, step: 1282, outputs are 0.050210368\n",
      "epoch: 1, step: 1283, outputs are 0.051993825\n",
      "epoch: 1, step: 1284, outputs are 0.053889163\n",
      "epoch: 1, step: 1285, outputs are 0.049268275\n",
      "epoch: 1, step: 1286, outputs are 0.05433532\n",
      "epoch: 1, step: 1287, outputs are 0.05008524\n",
      "epoch: 1, step: 1288, outputs are 0.06310502\n",
      "epoch: 1, step: 1289, outputs are 0.050708905\n",
      "epoch: 1, step: 1290, outputs are 0.053567152\n",
      "epoch: 1, step: 1291, outputs are 0.053518664\n",
      "epoch: 1, step: 1292, outputs are 0.048389994\n",
      "epoch: 1, step: 1293, outputs are 0.05208031\n",
      "epoch: 1, step: 1294, outputs are 0.053721264\n",
      "epoch: 1, step: 1295, outputs are 0.04975656\n",
      "epoch: 1, step: 1296, outputs are 0.05647952\n",
      "epoch: 1, step: 1297, outputs are 0.057069123\n",
      "epoch: 1, step: 1298, outputs are 0.056196615\n",
      "epoch: 1, step: 1299, outputs are 0.049155943\n",
      "epoch: 1, step: 1300, outputs are 0.050350927\n",
      "epoch: 1, step: 1301, outputs are 0.054316133\n",
      "epoch: 1, step: 1302, outputs are 0.048230417\n",
      "epoch: 1, step: 1303, outputs are 0.052994862\n",
      "epoch: 1, step: 1304, outputs are 0.0551518\n",
      "epoch: 1, step: 1305, outputs are 0.054854844\n",
      "epoch: 1, step: 1306, outputs are 0.05882584\n",
      "epoch: 1, step: 1307, outputs are 0.051967274\n",
      "epoch: 1, step: 1308, outputs are 0.05243048\n",
      "epoch: 1, step: 1309, outputs are 0.05115703\n",
      "epoch: 1, step: 1310, outputs are 0.053183846\n",
      "epoch: 1, step: 1311, outputs are 0.056305327\n",
      "epoch: 1, step: 1312, outputs are 0.04905509\n",
      "epoch: 1, step: 1313, outputs are 0.051555056\n",
      "epoch: 1, step: 1314, outputs are 0.05229518\n",
      "epoch: 1, step: 1315, outputs are 0.051101327\n",
      "epoch: 1, step: 1316, outputs are 0.04724092\n",
      "epoch: 1, step: 1317, outputs are 0.055236004\n",
      "epoch: 1, step: 1318, outputs are 0.052414287\n",
      "epoch: 1, step: 1319, outputs are 0.05654128\n",
      "epoch: 1, step: 1320, outputs are 0.0510232\n",
      "epoch: 1, step: 1321, outputs are 0.05646875\n",
      "epoch: 1, step: 1322, outputs are 0.049665518\n",
      "epoch: 1, step: 1323, outputs are 0.048213147\n",
      "epoch: 1, step: 1324, outputs are 0.053055737\n",
      "epoch: 1, step: 1325, outputs are 0.050227694\n",
      "epoch: 1, step: 1326, outputs are 0.045557134\n",
      "epoch: 1, step: 1327, outputs are 0.056764405\n",
      "epoch: 1, step: 1328, outputs are 0.058868013\n",
      "epoch: 1, step: 1329, outputs are 0.05921573\n",
      "epoch: 1, step: 1330, outputs are 0.05449561\n",
      "epoch: 1, step: 1331, outputs are 0.05961794\n",
      "epoch: 1, step: 1332, outputs are 0.053078588\n",
      "epoch: 1, step: 1333, outputs are 0.048916683\n",
      "epoch: 1, step: 1334, outputs are 0.04743219\n",
      "epoch: 1, step: 1335, outputs are 0.05195246\n",
      "epoch: 1, step: 1336, outputs are 0.047253206\n",
      "epoch: 1, step: 1337, outputs are 0.052745853\n",
      "epoch: 1, step: 1338, outputs are 0.05115406\n",
      "epoch: 1, step: 1339, outputs are 0.05467905\n",
      "epoch: 1, step: 1340, outputs are 0.05479932\n",
      "epoch: 1, step: 1341, outputs are 0.051693607\n",
      "epoch: 1, step: 1342, outputs are 0.054740965\n",
      "epoch: 1, step: 1343, outputs are 0.05650899\n",
      "epoch: 1, step: 1344, outputs are 0.04953506\n",
      "epoch: 1, step: 1345, outputs are 0.047994208\n",
      "epoch: 1, step: 1346, outputs are 0.05381157\n",
      "epoch: 1, step: 1347, outputs are 0.05322323\n",
      "epoch: 1, step: 1348, outputs are 0.052492123\n",
      "epoch: 1, step: 1349, outputs are 0.053481173\n",
      "epoch: 1, step: 1350, outputs are 0.051988933\n",
      "epoch: 1, step: 1351, outputs are 0.053530626\n",
      "epoch: 1, step: 1352, outputs are 0.049457155\n",
      "epoch: 1, step: 1353, outputs are 0.057770804\n",
      "epoch: 1, step: 1354, outputs are 0.049963668\n",
      "epoch: 1, step: 1355, outputs are 0.05158888\n",
      "epoch: 1, step: 1356, outputs are 0.048763234\n",
      "epoch: 1, step: 1357, outputs are 0.05550029\n",
      "epoch: 1, step: 1358, outputs are 0.05161003\n",
      "epoch: 1, step: 1359, outputs are 0.051852114\n",
      "epoch: 1, step: 1360, outputs are 0.049282137\n",
      "epoch: 1, step: 1361, outputs are 0.05309821\n",
      "epoch: 1, step: 1362, outputs are 0.05188541\n",
      "epoch: 1, step: 1363, outputs are 0.056559853\n",
      "epoch: 1, step: 1364, outputs are 0.056058466\n",
      "epoch: 1, step: 1365, outputs are 0.056810454\n",
      "epoch: 1, step: 1366, outputs are 0.050772715\n",
      "epoch: 1, step: 1367, outputs are 0.05022771\n",
      "epoch: 1, step: 1368, outputs are 0.05479531\n",
      "epoch: 1, step: 1369, outputs are 0.053770732\n",
      "epoch: 1, step: 1370, outputs are 0.052243058\n",
      "epoch: 1, step: 1371, outputs are 0.05373211\n",
      "epoch: 1, step: 1372, outputs are 0.052642122\n",
      "epoch: 1, step: 1373, outputs are 0.050410654\n",
      "epoch: 1, step: 1374, outputs are 0.052058812\n",
      "epoch: 1, step: 1375, outputs are 0.052529488\n",
      "epoch: 1, step: 1376, outputs are 0.050785854\n",
      "epoch: 1, step: 1377, outputs are 0.051008455\n",
      "epoch: 1, step: 1378, outputs are 0.055411153\n",
      "epoch: 1, step: 1379, outputs are 0.052129634\n",
      "epoch: 1, step: 1380, outputs are 0.049631268\n",
      "epoch: 1, step: 1381, outputs are 0.055618368\n",
      "epoch: 1, step: 1382, outputs are 0.053757235\n",
      "epoch: 1, step: 1383, outputs are 0.04930137\n",
      "epoch: 1, step: 1384, outputs are 0.05295907\n",
      "epoch: 1, step: 1385, outputs are 0.052549914\n",
      "epoch: 1, step: 1386, outputs are 0.049995214\n",
      "epoch: 1, step: 1387, outputs are 0.0466879\n",
      "epoch: 1, step: 1388, outputs are 0.05132235\n",
      "epoch: 1, step: 1389, outputs are 0.054200575\n",
      "epoch: 1, step: 1390, outputs are 0.056241393\n",
      "epoch: 1, step: 1391, outputs are 0.05743525\n",
      "epoch: 1, step: 1392, outputs are 0.048366692\n",
      "epoch: 1, step: 1393, outputs are 0.050334774\n",
      "epoch: 1, step: 1394, outputs are 0.055522554\n",
      "epoch: 1, step: 1395, outputs are 0.052936375\n",
      "epoch: 1, step: 1396, outputs are 0.055922244\n",
      "epoch: 1, step: 1397, outputs are 0.058843117\n",
      "epoch: 1, step: 1398, outputs are 0.05072879\n",
      "epoch: 1, step: 1399, outputs are 0.049868178\n",
      "epoch: 1, step: 1400, outputs are 0.054009385\n",
      "epoch: 1, step: 1401, outputs are 0.059893966\n",
      "epoch: 1, step: 1402, outputs are 0.05559529\n",
      "epoch: 1, step: 1403, outputs are 0.053506635\n",
      "epoch: 1, step: 1404, outputs are 0.05378917\n",
      "epoch: 1, step: 1405, outputs are 0.05385036\n",
      "epoch: 1, step: 1406, outputs are 0.050366115\n",
      "epoch: 1, step: 1407, outputs are 0.05531281\n",
      "epoch: 1, step: 1408, outputs are 0.05177206\n",
      "epoch: 1, step: 1409, outputs are 0.047243576\n",
      "epoch: 1, step: 1410, outputs are 0.054780066\n",
      "epoch: 1, step: 1411, outputs are 0.052244663\n",
      "epoch: 1, step: 1412, outputs are 0.045572713\n",
      "epoch: 1, step: 1413, outputs are 0.056474626\n",
      "epoch: 1, step: 1414, outputs are 0.045102216\n",
      "epoch: 1, step: 1415, outputs are 0.05308955\n",
      "epoch: 1, step: 1416, outputs are 0.04739502\n",
      "epoch: 1, step: 1417, outputs are 0.062434576\n",
      "epoch: 1, step: 1418, outputs are 0.048831858\n",
      "epoch: 1, step: 1419, outputs are 0.05311598\n",
      "epoch: 1, step: 1420, outputs are 0.051079746\n",
      "epoch: 1, step: 1421, outputs are 0.05090457\n",
      "epoch: 1, step: 1422, outputs are 0.051420033\n",
      "epoch: 1, step: 1423, outputs are 0.04984372\n",
      "epoch: 1, step: 1424, outputs are 0.050474726\n",
      "epoch: 1, step: 1425, outputs are 0.05089073\n",
      "epoch: 1, step: 1426, outputs are 0.04938001\n",
      "epoch: 1, step: 1427, outputs are 0.04910756\n",
      "epoch: 1, step: 1428, outputs are 0.052907582\n",
      "epoch: 1, step: 1429, outputs are 0.051735807\n",
      "epoch: 1, step: 1430, outputs are 0.051567752\n",
      "epoch: 1, step: 1431, outputs are 0.050986916\n",
      "epoch: 1, step: 1432, outputs are 0.050162874\n",
      "epoch: 1, step: 1433, outputs are 0.052084208\n",
      "epoch: 1, step: 1434, outputs are 0.049232963\n",
      "epoch: 1, step: 1435, outputs are 0.052098554\n",
      "epoch: 1, step: 1436, outputs are 0.047413573\n",
      "epoch: 1, step: 1437, outputs are 0.052417364\n",
      "epoch: 1, step: 1438, outputs are 0.05657672\n",
      "epoch: 1, step: 1439, outputs are 0.04865148\n",
      "epoch: 1, step: 1440, outputs are 0.04710112\n",
      "epoch: 1, step: 1441, outputs are 0.04665591\n",
      "epoch: 1, step: 1442, outputs are 0.05330167\n",
      "epoch: 1, step: 1443, outputs are 0.053109497\n",
      "epoch: 1, step: 1444, outputs are 0.051453665\n",
      "epoch: 1, step: 1445, outputs are 0.053358104\n",
      "epoch: 1, step: 1446, outputs are 0.055091396\n",
      "epoch: 1, step: 1447, outputs are 0.047940597\n",
      "epoch: 1, step: 1448, outputs are 0.046065927\n",
      "epoch: 1, step: 1449, outputs are 0.054854877\n",
      "epoch: 1, step: 1450, outputs are 0.055402942\n",
      "epoch: 1, step: 1451, outputs are 0.055605687\n",
      "epoch: 1, step: 1452, outputs are 0.04860711\n",
      "epoch: 1, step: 1453, outputs are 0.04671466\n",
      "epoch: 1, step: 1454, outputs are 0.05645731\n",
      "epoch: 1, step: 1455, outputs are 0.050951954\n",
      "epoch: 1, step: 1456, outputs are 0.05617555\n",
      "epoch: 1, step: 1457, outputs are 0.05323162\n",
      "epoch: 1, step: 1458, outputs are 0.04906149\n",
      "epoch: 1, step: 1459, outputs are 0.049214527\n",
      "epoch: 1, step: 1460, outputs are 0.049476404\n",
      "epoch: 1, step: 1461, outputs are 0.049492825\n",
      "epoch: 1, step: 1462, outputs are 0.056350574\n",
      "epoch: 1, step: 1463, outputs are 0.046160292\n",
      "epoch: 1, step: 1464, outputs are 0.048241314\n",
      "epoch: 1, step: 1465, outputs are 0.05106969\n",
      "epoch: 1, step: 1466, outputs are 0.050326366\n",
      "epoch: 1, step: 1467, outputs are 0.04923819\n",
      "epoch: 1, step: 1468, outputs are 0.052344844\n",
      "epoch: 1, step: 1469, outputs are 0.050904665\n",
      "epoch: 1, step: 1470, outputs are 0.047069587\n",
      "epoch: 1, step: 1471, outputs are 0.051856536\n",
      "epoch: 1, step: 1472, outputs are 0.052991986\n",
      "epoch: 1, step: 1473, outputs are 0.046937153\n",
      "epoch: 1, step: 1474, outputs are 0.049762957\n",
      "epoch: 1, step: 1475, outputs are 0.051603287\n",
      "epoch: 1, step: 1476, outputs are 0.05069069\n",
      "epoch: 1, step: 1477, outputs are 0.049479604\n",
      "epoch: 1, step: 1478, outputs are 0.051952545\n",
      "epoch: 1, step: 1479, outputs are 0.051331878\n",
      "epoch: 1, step: 1480, outputs are 0.04477695\n",
      "epoch: 1, step: 1481, outputs are 0.048499767\n",
      "epoch: 1, step: 1482, outputs are 0.05226458\n",
      "epoch: 1, step: 1483, outputs are 0.046519794\n",
      "epoch: 1, step: 1484, outputs are 0.054666944\n",
      "epoch: 1, step: 1485, outputs are 0.049736407\n",
      "epoch: 1, step: 1486, outputs are 0.0520921\n",
      "epoch: 1, step: 1487, outputs are 0.05593969\n",
      "epoch: 1, step: 1488, outputs are 0.04926657\n",
      "epoch: 1, step: 1489, outputs are 0.05174979\n",
      "epoch: 1, step: 1490, outputs are 0.05471234\n",
      "epoch: 1, step: 1491, outputs are 0.052528113\n",
      "epoch: 1, step: 1492, outputs are 0.05454127\n",
      "epoch: 1, step: 1493, outputs are 0.05434497\n",
      "epoch: 1, step: 1494, outputs are 0.050097696\n",
      "epoch: 1, step: 1495, outputs are 0.053190544\n",
      "epoch: 1, step: 1496, outputs are 0.053648993\n",
      "epoch: 1, step: 1497, outputs are 0.05286674\n",
      "epoch: 1, step: 1498, outputs are 0.057264365\n",
      "epoch: 1, step: 1499, outputs are 0.055111356\n",
      "epoch: 1, step: 1500, outputs are 0.05159141\n",
      "epoch: 1, step: 1501, outputs are 0.049073867\n",
      "epoch: 1, step: 1502, outputs are 0.049259983\n",
      "epoch: 1, step: 1503, outputs are 0.050264448\n",
      "epoch: 1, step: 1504, outputs are 0.048032895\n",
      "epoch: 1, step: 1505, outputs are 0.054582864\n",
      "epoch: 1, step: 1506, outputs are 0.05090017\n",
      "epoch: 1, step: 1507, outputs are 0.054056257\n",
      "epoch: 1, step: 1508, outputs are 0.051290188\n",
      "epoch: 1, step: 1509, outputs are 0.0517129\n",
      "epoch: 1, step: 1510, outputs are 0.052188233\n",
      "epoch: 1, step: 1511, outputs are 0.051420044\n",
      "epoch: 1, step: 1512, outputs are 0.055321462\n",
      "epoch: 1, step: 1513, outputs are 0.052272588\n",
      "epoch: 1, step: 1514, outputs are 0.05354429\n",
      "epoch: 1, step: 1515, outputs are 0.048130013\n",
      "epoch: 1, step: 1516, outputs are 0.053780846\n",
      "epoch: 1, step: 1517, outputs are 0.046068035\n",
      "epoch: 1, step: 1518, outputs are 0.05259458\n",
      "epoch: 1, step: 1519, outputs are 0.051714923\n",
      "epoch: 1, step: 1520, outputs are 0.051474065\n",
      "epoch: 1, step: 1521, outputs are 0.055569537\n",
      "epoch: 1, step: 1522, outputs are 0.051490285\n",
      "epoch: 1, step: 1523, outputs are 0.051945385\n",
      "epoch: 1, step: 1524, outputs are 0.054185208\n",
      "epoch: 1, step: 1525, outputs are 0.04902801\n",
      "epoch: 1, step: 1526, outputs are 0.04215304\n",
      "epoch: 1, step: 1527, outputs are 0.049394146\n",
      "epoch: 1, step: 1528, outputs are 0.04785902\n",
      "epoch: 1, step: 1529, outputs are 0.048369743\n",
      "epoch: 1, step: 1530, outputs are 0.053871356\n",
      "epoch: 1, step: 1531, outputs are 0.04707256\n",
      "epoch: 1, step: 1532, outputs are 0.050587486\n",
      "epoch: 1, step: 1533, outputs are 0.052817225\n",
      "epoch: 1, step: 1534, outputs are 0.04886373\n",
      "epoch: 1, step: 1535, outputs are 0.05707746\n",
      "epoch: 1, step: 1536, outputs are 0.054566894\n",
      "epoch: 1, step: 1537, outputs are 0.053909034\n",
      "epoch: 1, step: 1538, outputs are 0.050624162\n",
      "epoch: 1, step: 1539, outputs are 0.048970874\n",
      "epoch: 1, step: 1540, outputs are 0.048897028\n",
      "epoch: 1, step: 1541, outputs are 0.049875993\n",
      "epoch: 1, step: 1542, outputs are 0.050503403\n",
      "epoch: 1, step: 1543, outputs are 0.048008196\n",
      "epoch: 1, step: 1544, outputs are 0.051510684\n",
      "epoch: 1, step: 1545, outputs are 0.05254847\n",
      "epoch: 1, step: 1546, outputs are 0.049573366\n",
      "epoch: 1, step: 1547, outputs are 0.053972326\n",
      "epoch: 1, step: 1548, outputs are 0.047002874\n",
      "epoch: 1, step: 1549, outputs are 0.049543038\n",
      "epoch: 1, step: 1550, outputs are 0.053878404\n",
      "epoch: 1, step: 1551, outputs are 0.0570123\n",
      "epoch: 1, step: 1552, outputs are 0.050569184\n",
      "epoch: 1, step: 1553, outputs are 0.051075913\n",
      "epoch: 1, step: 1554, outputs are 0.054169264\n",
      "epoch: 1, step: 1555, outputs are 0.049795877\n",
      "epoch: 1, step: 1556, outputs are 0.05171603\n",
      "epoch: 1, step: 1557, outputs are 0.050530724\n",
      "epoch: 1, step: 1558, outputs are 0.05674506\n",
      "epoch: 1, step: 1559, outputs are 0.0511488\n",
      "epoch: 1, step: 1560, outputs are 0.050191134\n",
      "epoch: 1, step: 1561, outputs are 0.054301225\n",
      "epoch: 1, step: 1562, outputs are 0.0532944\n",
      "epoch: 1, step: 1563, outputs are 0.053891636\n",
      "epoch: 1, step: 1564, outputs are 0.05397448\n",
      "epoch: 1, step: 1565, outputs are 0.058748893\n",
      "epoch: 1, step: 1566, outputs are 0.049940605\n",
      "epoch: 1, step: 1567, outputs are 0.049654696\n",
      "epoch: 1, step: 1568, outputs are 0.04821605\n",
      "epoch: 1, step: 1569, outputs are 0.05241403\n",
      "epoch: 1, step: 1570, outputs are 0.048963323\n",
      "epoch: 1, step: 1571, outputs are 0.0503089\n",
      "epoch: 1, step: 1572, outputs are 0.054267183\n",
      "epoch: 1, step: 1573, outputs are 0.049502037\n",
      "epoch: 1, step: 1574, outputs are 0.049006276\n",
      "epoch: 1, step: 1575, outputs are 0.051120937\n",
      "epoch: 1, step: 1576, outputs are 0.04683215\n",
      "epoch: 1, step: 1577, outputs are 0.051596045\n",
      "epoch: 1, step: 1578, outputs are 0.04900983\n",
      "epoch: 1, step: 1579, outputs are 0.050218116\n",
      "epoch: 1, step: 1580, outputs are 0.052959707\n",
      "epoch: 1, step: 1581, outputs are 0.04742867\n",
      "epoch: 1, step: 1582, outputs are 0.048691258\n",
      "epoch: 1, step: 1583, outputs are 0.055320773\n",
      "epoch: 1, step: 1584, outputs are 0.05040728\n",
      "epoch: 1, step: 1585, outputs are 0.050500963\n",
      "epoch: 1, step: 1586, outputs are 0.048081458\n",
      "epoch: 1, step: 1587, outputs are 0.055028953\n",
      "epoch: 1, step: 1588, outputs are 0.05427088\n",
      "epoch: 1, step: 1589, outputs are 0.048078783\n",
      "epoch: 1, step: 1590, outputs are 0.052610602\n",
      "epoch: 1, step: 1591, outputs are 0.04861887\n",
      "epoch: 1, step: 1592, outputs are 0.051908\n",
      "epoch: 1, step: 1593, outputs are 0.04685067\n",
      "epoch: 1, step: 1594, outputs are 0.04829855\n",
      "epoch: 1, step: 1595, outputs are 0.049379148\n",
      "epoch: 1, step: 1596, outputs are 0.04847287\n",
      "epoch: 1, step: 1597, outputs are 0.04706519\n",
      "epoch: 1, step: 1598, outputs are 0.049055994\n",
      "epoch: 1, step: 1599, outputs are 0.04992503\n",
      "epoch: 1, step: 1600, outputs are 0.05216178\n",
      "epoch: 1, step: 1601, outputs are 0.054048553\n",
      "epoch: 1, step: 1602, outputs are 0.051012985\n",
      "epoch: 1, step: 1603, outputs are 0.051533416\n",
      "epoch: 1, step: 1604, outputs are 0.052924022\n",
      "epoch: 1, step: 1605, outputs are 0.05708547\n",
      "epoch: 1, step: 1606, outputs are 0.050839033\n",
      "epoch: 1, step: 1607, outputs are 0.053202294\n",
      "epoch: 1, step: 1608, outputs are 0.04921864\n",
      "epoch: 1, step: 1609, outputs are 0.050884612\n",
      "epoch: 1, step: 1610, outputs are 0.05005248\n",
      "epoch: 1, step: 1611, outputs are 0.050127845\n",
      "epoch: 1, step: 1612, outputs are 0.04991219\n",
      "epoch: 1, step: 1613, outputs are 0.050067473\n",
      "epoch: 1, step: 1614, outputs are 0.052362964\n",
      "epoch: 1, step: 1615, outputs are 0.048282064\n",
      "epoch: 1, step: 1616, outputs are 0.045983754\n",
      "epoch: 1, step: 1617, outputs are 0.052128583\n",
      "epoch: 1, step: 1618, outputs are 0.052119166\n",
      "epoch: 1, step: 1619, outputs are 0.054734662\n",
      "epoch: 1, step: 1620, outputs are 0.048238188\n",
      "epoch: 1, step: 1621, outputs are 0.05332137\n",
      "epoch: 1, step: 1622, outputs are 0.05048656\n",
      "epoch: 1, step: 1623, outputs are 0.05322677\n",
      "epoch: 1, step: 1624, outputs are 0.046599504\n",
      "epoch: 1, step: 1625, outputs are 0.04983994\n",
      "epoch: 1, step: 1626, outputs are 0.04846464\n",
      "epoch: 1, step: 1627, outputs are 0.051831175\n",
      "epoch: 1, step: 1628, outputs are 0.051355574\n",
      "epoch: 1, step: 1629, outputs are 0.05056981\n",
      "epoch: 1, step: 1630, outputs are 0.056932207\n",
      "epoch: 1, step: 1631, outputs are 0.04692951\n",
      "epoch: 1, step: 1632, outputs are 0.053965878\n",
      "epoch: 1, step: 1633, outputs are 0.043689825\n",
      "epoch: 1, step: 1634, outputs are 0.05216932\n",
      "epoch: 1, step: 1635, outputs are 0.050764695\n",
      "epoch: 1, step: 1636, outputs are 0.048361354\n",
      "epoch: 1, step: 1637, outputs are 0.050399944\n",
      "epoch: 1, step: 1638, outputs are 0.053683832\n",
      "epoch: 1, step: 1639, outputs are 0.0514873\n",
      "epoch: 1, step: 1640, outputs are 0.053822115\n",
      "epoch: 1, step: 1641, outputs are 0.051952504\n",
      "epoch: 1, step: 1642, outputs are 0.051568806\n",
      "epoch: 1, step: 1643, outputs are 0.0440821\n",
      "epoch: 1, step: 1644, outputs are 0.04846123\n",
      "epoch: 1, step: 1645, outputs are 0.047583826\n",
      "epoch: 1, step: 1646, outputs are 0.05298096\n",
      "epoch: 1, step: 1647, outputs are 0.05415312\n",
      "epoch: 1, step: 1648, outputs are 0.054440144\n",
      "epoch: 1, step: 1649, outputs are 0.048604675\n",
      "epoch: 1, step: 1650, outputs are 0.04633893\n",
      "epoch: 1, step: 1651, outputs are 0.047298342\n",
      "epoch: 1, step: 1652, outputs are 0.050660692\n",
      "epoch: 1, step: 1653, outputs are 0.05119016\n",
      "epoch: 1, step: 1654, outputs are 0.054641064\n",
      "epoch: 1, step: 1655, outputs are 0.050466623\n",
      "epoch: 1, step: 1656, outputs are 0.04816792\n",
      "epoch: 1, step: 1657, outputs are 0.051850338\n",
      "epoch: 1, step: 1658, outputs are 0.048129134\n",
      "epoch: 1, step: 1659, outputs are 0.048056297\n",
      "epoch: 1, step: 1660, outputs are 0.04895843\n",
      "epoch: 1, step: 1661, outputs are 0.052054882\n",
      "epoch: 1, step: 1662, outputs are 0.050345354\n",
      "epoch: 1, step: 1663, outputs are 0.05044067\n",
      "epoch: 1, step: 1664, outputs are 0.048674256\n",
      "epoch: 1, step: 1665, outputs are 0.05508115\n",
      "epoch: 1, step: 1666, outputs are 0.050880328\n",
      "epoch: 1, step: 1667, outputs are 0.05226517\n",
      "epoch: 1, step: 1668, outputs are 0.05228713\n",
      "epoch: 1, step: 1669, outputs are 0.048934285\n",
      "epoch: 1, step: 1670, outputs are 0.062136836\n",
      "epoch: 1, step: 1671, outputs are 0.04868751\n",
      "epoch: 1, step: 1672, outputs are 0.052626997\n",
      "epoch: 1, step: 1673, outputs are 0.048921242\n",
      "epoch: 1, step: 1674, outputs are 0.045943826\n",
      "epoch: 1, step: 1675, outputs are 0.052239127\n",
      "epoch: 1, step: 1676, outputs are 0.052899644\n",
      "epoch: 1, step: 1677, outputs are 0.04874525\n",
      "epoch: 1, step: 1678, outputs are 0.05189508\n",
      "epoch: 1, step: 1679, outputs are 0.05189666\n",
      "epoch: 1, step: 1680, outputs are 0.050670583\n",
      "epoch: 1, step: 1681, outputs are 0.053989466\n",
      "epoch: 1, step: 1682, outputs are 0.049397364\n",
      "epoch: 1, step: 1683, outputs are 0.050501153\n",
      "epoch: 1, step: 1684, outputs are 0.05272801\n",
      "epoch: 1, step: 1685, outputs are 0.05028943\n",
      "epoch: 1, step: 1686, outputs are 0.04887981\n",
      "epoch: 1, step: 1687, outputs are 0.05248559\n",
      "epoch: 1, step: 1688, outputs are 0.05309046\n",
      "epoch: 1, step: 1689, outputs are 0.050780445\n",
      "epoch: 1, step: 1690, outputs are 0.051177904\n",
      "epoch: 1, step: 1691, outputs are 0.050887775\n",
      "epoch: 1, step: 1692, outputs are 0.05195305\n",
      "epoch: 1, step: 1693, outputs are 0.048872404\n",
      "epoch: 1, step: 1694, outputs are 0.05365236\n",
      "epoch: 1, step: 1695, outputs are 0.05419831\n",
      "epoch: 1, step: 1696, outputs are 0.04934325\n",
      "epoch: 1, step: 1697, outputs are 0.04636026\n",
      "epoch: 1, step: 1698, outputs are 0.049247462\n",
      "epoch: 1, step: 1699, outputs are 0.051091302\n",
      "epoch: 1, step: 1700, outputs are 0.05055743\n",
      "epoch: 1, step: 1701, outputs are 0.04738322\n",
      "epoch: 1, step: 1702, outputs are 0.052558657\n",
      "epoch: 1, step: 1703, outputs are 0.049112037\n",
      "epoch: 1, step: 1704, outputs are 0.051986605\n",
      "epoch: 1, step: 1705, outputs are 0.048510827\n",
      "epoch: 1, step: 1706, outputs are 0.053856403\n",
      "epoch: 1, step: 1707, outputs are 0.052881055\n",
      "epoch: 1, step: 1708, outputs are 0.055606917\n",
      "epoch: 1, step: 1709, outputs are 0.052509382\n",
      "epoch: 1, step: 1710, outputs are 0.051992442\n",
      "epoch: 1, step: 1711, outputs are 0.054749124\n",
      "epoch: 1, step: 1712, outputs are 0.05366575\n",
      "epoch: 1, step: 1713, outputs are 0.051927447\n",
      "epoch: 1, step: 1714, outputs are 0.04793223\n",
      "epoch: 1, step: 1715, outputs are 0.049136132\n",
      "epoch: 1, step: 1716, outputs are 0.05213019\n",
      "epoch: 1, step: 1717, outputs are 0.04736812\n",
      "epoch: 1, step: 1718, outputs are 0.055534877\n",
      "epoch: 1, step: 1719, outputs are 0.053057984\n",
      "epoch: 1, step: 1720, outputs are 0.0495257\n",
      "epoch: 1, step: 1721, outputs are 0.047350883\n",
      "epoch: 1, step: 1722, outputs are 0.053671874\n",
      "epoch: 1, step: 1723, outputs are 0.047396556\n",
      "epoch: 1, step: 1724, outputs are 0.045342915\n",
      "epoch: 1, step: 1725, outputs are 0.04821433\n",
      "epoch: 1, step: 1726, outputs are 0.048248954\n",
      "epoch: 1, step: 1727, outputs are 0.053309526\n",
      "epoch: 1, step: 1728, outputs are 0.04853844\n",
      "epoch: 1, step: 1729, outputs are 0.05281394\n",
      "epoch: 1, step: 1730, outputs are 0.04943431\n",
      "epoch: 1, step: 1731, outputs are 0.04775461\n",
      "epoch: 1, step: 1732, outputs are 0.04747191\n",
      "epoch: 1, step: 1733, outputs are 0.045987133\n",
      "epoch: 1, step: 1734, outputs are 0.049878024\n",
      "epoch: 1, step: 1735, outputs are 0.054770153\n",
      "epoch: 1, step: 1736, outputs are 0.05055146\n",
      "epoch: 1, step: 1737, outputs are 0.049064457\n",
      "epoch: 1, step: 1738, outputs are 0.05482947\n",
      "epoch: 1, step: 1739, outputs are 0.048065666\n",
      "epoch: 1, step: 1740, outputs are 0.050869368\n",
      "epoch: 1, step: 1741, outputs are 0.049523424\n",
      "epoch: 1, step: 1742, outputs are 0.051037107\n",
      "epoch: 1, step: 1743, outputs are 0.046106406\n",
      "epoch: 1, step: 1744, outputs are 0.05078714\n",
      "epoch: 1, step: 1745, outputs are 0.04981676\n",
      "epoch: 1, step: 1746, outputs are 0.051002853\n",
      "epoch: 1, step: 1747, outputs are 0.050073512\n",
      "epoch: 1, step: 1748, outputs are 0.0501638\n",
      "epoch: 1, step: 1749, outputs are 0.053642645\n",
      "epoch: 1, step: 1750, outputs are 0.050677393\n",
      "epoch: 1, step: 1751, outputs are 0.04602924\n",
      "epoch: 1, step: 1752, outputs are 0.050577994\n",
      "epoch: 1, step: 1753, outputs are 0.051004227\n",
      "epoch: 1, step: 1754, outputs are 0.046366964\n",
      "epoch: 1, step: 1755, outputs are 0.047629945\n",
      "epoch: 1, step: 1756, outputs are 0.047935575\n",
      "epoch: 1, step: 1757, outputs are 0.050010644\n",
      "epoch: 1, step: 1758, outputs are 0.04749658\n",
      "epoch: 1, step: 1759, outputs are 0.049517356\n",
      "epoch: 1, step: 1760, outputs are 0.052715376\n",
      "epoch: 1, step: 1761, outputs are 0.049968533\n",
      "epoch: 1, step: 1762, outputs are 0.04587055\n",
      "epoch: 1, step: 1763, outputs are 0.049461015\n",
      "epoch: 1, step: 1764, outputs are 0.051055714\n",
      "epoch: 1, step: 1765, outputs are 0.048940595\n",
      "epoch: 1, step: 1766, outputs are 0.04671198\n",
      "epoch: 1, step: 1767, outputs are 0.049912658\n",
      "epoch: 1, step: 1768, outputs are 0.048857942\n",
      "epoch: 1, step: 1769, outputs are 0.05075921\n",
      "epoch: 1, step: 1770, outputs are 0.054429524\n",
      "epoch: 1, step: 1771, outputs are 0.052192435\n",
      "epoch: 1, step: 1772, outputs are 0.048589163\n",
      "epoch: 1, step: 1773, outputs are 0.04878839\n",
      "epoch: 1, step: 1774, outputs are 0.05257806\n",
      "epoch: 1, step: 1775, outputs are 0.045489527\n",
      "epoch: 1, step: 1776, outputs are 0.053895213\n",
      "epoch: 1, step: 1777, outputs are 0.050833143\n",
      "epoch: 1, step: 1778, outputs are 0.053550117\n",
      "epoch: 1, step: 1779, outputs are 0.050705332\n",
      "epoch: 1, step: 1780, outputs are 0.050618816\n",
      "epoch: 1, step: 1781, outputs are 0.054391354\n",
      "epoch: 1, step: 1782, outputs are 0.04958465\n",
      "epoch: 1, step: 1783, outputs are 0.052151352\n",
      "epoch: 1, step: 1784, outputs are 0.05251464\n",
      "epoch: 1, step: 1785, outputs are 0.051114958\n",
      "epoch: 1, step: 1786, outputs are 0.052236266\n",
      "epoch: 1, step: 1787, outputs are 0.046928473\n",
      "epoch: 1, step: 1788, outputs are 0.049094614\n",
      "epoch: 1, step: 1789, outputs are 0.05013486\n",
      "epoch: 1, step: 1790, outputs are 0.04780621\n",
      "epoch: 1, step: 1791, outputs are 0.05473624\n",
      "epoch: 1, step: 1792, outputs are 0.05214313\n",
      "epoch: 1, step: 1793, outputs are 0.050126016\n",
      "epoch: 1, step: 1794, outputs are 0.04621718\n",
      "epoch: 1, step: 1795, outputs are 0.055290498\n",
      "epoch: 1, step: 1796, outputs are 0.05261477\n",
      "epoch: 1, step: 1797, outputs are 0.048407894\n",
      "epoch: 1, step: 1798, outputs are 0.04746\n",
      "epoch: 1, step: 1799, outputs are 0.046970706\n",
      "epoch: 1, step: 1800, outputs are 0.054560285\n",
      "epoch: 1, step: 1801, outputs are 0.049830925\n",
      "epoch: 1, step: 1802, outputs are 0.05106557\n",
      "epoch: 1, step: 1803, outputs are 0.052533813\n",
      "epoch: 1, step: 1804, outputs are 0.04911766\n",
      "epoch: 1, step: 1805, outputs are 0.049568012\n",
      "epoch: 1, step: 1806, outputs are 0.048186332\n",
      "epoch: 1, step: 1807, outputs are 0.04894705\n",
      "epoch: 1, step: 1808, outputs are 0.046354074\n",
      "epoch: 1, step: 1809, outputs are 0.052077465\n",
      "epoch: 1, step: 1810, outputs are 0.050818887\n",
      "epoch: 1, step: 1811, outputs are 0.048476044\n",
      "epoch: 1, step: 1812, outputs are 0.049899608\n",
      "epoch: 1, step: 1813, outputs are 0.050026763\n",
      "epoch: 1, step: 1814, outputs are 0.04991409\n",
      "epoch: 1, step: 1815, outputs are 0.046131913\n",
      "epoch: 1, step: 1816, outputs are 0.04841144\n",
      "epoch: 1, step: 1817, outputs are 0.050980754\n",
      "epoch: 1, step: 1818, outputs are 0.04681693\n",
      "epoch: 1, step: 1819, outputs are 0.051630996\n",
      "epoch: 1, step: 1820, outputs are 0.054030422\n",
      "epoch: 1, step: 1821, outputs are 0.054749027\n",
      "epoch: 1, step: 1822, outputs are 0.04628993\n",
      "epoch: 1, step: 1823, outputs are 0.05365154\n",
      "epoch: 1, step: 1824, outputs are 0.04905165\n",
      "epoch: 1, step: 1825, outputs are 0.05368137\n",
      "epoch: 1, step: 1826, outputs are 0.049670905\n",
      "epoch: 1, step: 1827, outputs are 0.051257286\n",
      "epoch: 1, step: 1828, outputs are 0.05414774\n",
      "epoch: 1, step: 1829, outputs are 0.049003482\n",
      "epoch: 1, step: 1830, outputs are 0.049400695\n",
      "epoch: 1, step: 1831, outputs are 0.057121016\n",
      "epoch: 1, step: 1832, outputs are 0.050222207\n",
      "epoch: 1, step: 1833, outputs are 0.052323118\n",
      "epoch: 1, step: 1834, outputs are 0.050366066\n",
      "epoch: 1, step: 1835, outputs are 0.049043264\n",
      "epoch: 1, step: 1836, outputs are 0.04839321\n",
      "epoch: 1, step: 1837, outputs are 0.049133938\n",
      "epoch: 1, step: 1838, outputs are 0.044838455\n",
      "epoch: 1, step: 1839, outputs are 0.04903525\n",
      "epoch: 1, step: 1840, outputs are 0.050205547\n",
      "epoch: 1, step: 1841, outputs are 0.051991176\n",
      "epoch: 1, step: 1842, outputs are 0.04414755\n",
      "epoch: 1, step: 1843, outputs are 0.050624944\n",
      "epoch: 1, step: 1844, outputs are 0.04914381\n",
      "epoch: 1, step: 1845, outputs are 0.049299862\n",
      "epoch: 1, step: 1846, outputs are 0.049097747\n",
      "epoch: 1, step: 1847, outputs are 0.052070815\n",
      "epoch: 1, step: 1848, outputs are 0.051685527\n",
      "epoch: 1, step: 1849, outputs are 0.048350327\n",
      "epoch: 1, step: 1850, outputs are 0.05042094\n",
      "epoch: 1, step: 1851, outputs are 0.056062933\n",
      "epoch: 1, step: 1852, outputs are 0.056448765\n",
      "epoch: 1, step: 1853, outputs are 0.05094017\n",
      "epoch: 1, step: 1854, outputs are 0.04922011\n",
      "epoch: 1, step: 1855, outputs are 0.04886924\n",
      "epoch: 1, step: 1856, outputs are 0.052518845\n",
      "epoch: 1, step: 1857, outputs are 0.04855926\n",
      "epoch: 1, step: 1858, outputs are 0.051041707\n",
      "epoch: 1, step: 1859, outputs are 0.05165631\n",
      "epoch: 1, step: 1860, outputs are 0.049342733\n",
      "epoch: 1, step: 1861, outputs are 0.049831044\n",
      "epoch: 1, step: 1862, outputs are 0.046561576\n",
      "epoch: 1, step: 1863, outputs are 0.053642273\n",
      "epoch: 1, step: 1864, outputs are 0.046595633\n",
      "epoch: 1, step: 1865, outputs are 0.04806975\n",
      "epoch: 1, step: 1866, outputs are 0.049050365\n",
      "epoch: 1, step: 1867, outputs are 0.049147576\n",
      "epoch: 1, step: 1868, outputs are 0.055021375\n",
      "epoch: 1, step: 1869, outputs are 0.04704493\n",
      "epoch: 1, step: 1870, outputs are 0.051378377\n",
      "epoch: 1, step: 1871, outputs are 0.048851453\n",
      "epoch: 1, step: 1872, outputs are 0.05377437\n",
      "epoch: 1, step: 1873, outputs are 0.047445204\n",
      "epoch: 1, step: 1874, outputs are 0.047953904\n",
      "epoch: 1, step: 1875, outputs are 0.058673017\n",
      "epoch: 1, step: 1876, outputs are 0.04882062\n",
      "epoch: 1, step: 1877, outputs are 0.045480408\n",
      "epoch: 1, step: 1878, outputs are 0.05095487\n",
      "epoch: 1, step: 1879, outputs are 0.051254064\n",
      "epoch: 1, step: 1880, outputs are 0.049286593\n",
      "epoch: 1, step: 1881, outputs are 0.05279622\n",
      "epoch: 1, step: 1882, outputs are 0.04840804\n",
      "epoch: 1, step: 1883, outputs are 0.055373196\n",
      "epoch: 1, step: 1884, outputs are 0.050515607\n",
      "epoch: 1, step: 1885, outputs are 0.05166978\n",
      "epoch: 1, step: 1886, outputs are 0.048119858\n",
      "epoch: 1, step: 1887, outputs are 0.051332396\n",
      "epoch: 1, step: 1888, outputs are 0.043286964\n",
      "epoch: 1, step: 1889, outputs are 0.04728049\n",
      "epoch: 1, step: 1890, outputs are 0.04790659\n",
      "epoch: 1, step: 1891, outputs are 0.048736975\n",
      "epoch: 1, step: 1892, outputs are 0.046054296\n",
      "epoch: 1, step: 1893, outputs are 0.04597459\n",
      "epoch: 1, step: 1894, outputs are 0.05233748\n",
      "epoch: 1, step: 1895, outputs are 0.0487136\n",
      "epoch: 1, step: 1896, outputs are 0.05413757\n",
      "epoch: 1, step: 1897, outputs are 0.05010648\n",
      "epoch: 1, step: 1898, outputs are 0.048666053\n",
      "epoch: 1, step: 1899, outputs are 0.047020927\n",
      "epoch: 1, step: 1900, outputs are 0.05149491\n",
      "epoch: 1, step: 1901, outputs are 0.049351387\n",
      "epoch: 1, step: 1902, outputs are 0.047335934\n",
      "epoch: 1, step: 1903, outputs are 0.052112788\n",
      "epoch: 1, step: 1904, outputs are 0.045962516\n",
      "epoch: 1, step: 1905, outputs are 0.04769513\n",
      "epoch: 1, step: 1906, outputs are 0.04820762\n",
      "epoch: 1, step: 1907, outputs are 0.052552037\n",
      "epoch: 1, step: 1908, outputs are 0.04935918\n",
      "epoch: 1, step: 1909, outputs are 0.051984176\n",
      "epoch: 1, step: 1910, outputs are 0.04661993\n",
      "epoch: 1, step: 1911, outputs are 0.049605016\n",
      "epoch: 1, step: 1912, outputs are 0.048898503\n",
      "epoch: 1, step: 1913, outputs are 0.049627557\n",
      "epoch: 1, step: 1914, outputs are 0.051878266\n",
      "epoch: 1, step: 1915, outputs are 0.051677078\n",
      "epoch: 1, step: 1916, outputs are 0.048159882\n",
      "epoch: 1, step: 1917, outputs are 0.04964029\n",
      "epoch: 1, step: 1918, outputs are 0.052572403\n",
      "epoch: 1, step: 1919, outputs are 0.050114766\n",
      "epoch: 1, step: 1920, outputs are 0.046014506\n",
      "epoch: 1, step: 1921, outputs are 0.047506295\n",
      "epoch: 1, step: 1922, outputs are 0.047468133\n",
      "epoch: 1, step: 1923, outputs are 0.050681084\n",
      "epoch: 1, step: 1924, outputs are 0.052357268\n",
      "epoch: 1, step: 1925, outputs are 0.050168328\n",
      "epoch: 1, step: 1926, outputs are 0.054080132\n",
      "epoch: 1, step: 1927, outputs are 0.048156433\n",
      "epoch: 1, step: 1928, outputs are 0.04741091\n",
      "epoch: 1, step: 1929, outputs are 0.050902322\n",
      "epoch: 1, step: 1930, outputs are 0.05053143\n",
      "epoch: 1, step: 1931, outputs are 0.048524994\n",
      "epoch: 1, step: 1932, outputs are 0.049721863\n",
      "epoch: 1, step: 1933, outputs are 0.044369385\n",
      "epoch: 1, step: 1934, outputs are 0.05065629\n",
      "epoch: 1, step: 1935, outputs are 0.05128756\n",
      "epoch: 1, step: 1936, outputs are 0.047693852\n",
      "epoch: 1, step: 1937, outputs are 0.04845281\n",
      "epoch: 1, step: 1938, outputs are 0.045977317\n",
      "epoch: 1, step: 1939, outputs are 0.043731034\n",
      "epoch: 1, step: 1940, outputs are 0.04846017\n",
      "epoch: 1, step: 1941, outputs are 0.050200827\n",
      "epoch: 1, step: 1942, outputs are 0.050825335\n",
      "epoch: 1, step: 1943, outputs are 0.05188253\n",
      "epoch: 1, step: 1944, outputs are 0.04989381\n",
      "epoch: 1, step: 1945, outputs are 0.051542528\n",
      "epoch: 1, step: 1946, outputs are 0.05045016\n",
      "epoch: 1, step: 1947, outputs are 0.05198277\n",
      "epoch: 1, step: 1948, outputs are 0.0475706\n",
      "epoch: 1, step: 1949, outputs are 0.05166918\n",
      "epoch: 1, step: 1950, outputs are 0.049185604\n",
      "epoch: 1, step: 1951, outputs are 0.048381038\n",
      "epoch: 1, step: 1952, outputs are 0.04953878\n",
      "epoch: 1, step: 1953, outputs are 0.047400177\n",
      "epoch: 1, step: 1954, outputs are 0.050459422\n",
      "epoch: 1, step: 1955, outputs are 0.049026012\n",
      "epoch: 1, step: 1956, outputs are 0.04993806\n",
      "epoch: 1, step: 1957, outputs are 0.049322907\n",
      "epoch: 1, step: 1958, outputs are 0.052574188\n",
      "epoch: 1, step: 1959, outputs are 0.049015228\n",
      "epoch: 1, step: 1960, outputs are 0.055650968\n",
      "epoch: 1, step: 1961, outputs are 0.046129018\n",
      "epoch: 1, step: 1962, outputs are 0.048822504\n",
      "epoch: 1, step: 1963, outputs are 0.044971067\n",
      "epoch: 1, step: 1964, outputs are 0.04721883\n",
      "epoch: 1, step: 1965, outputs are 0.046964556\n",
      "epoch: 1, step: 1966, outputs are 0.046539985\n",
      "epoch: 1, step: 1967, outputs are 0.051509406\n",
      "epoch: 1, step: 1968, outputs are 0.045595787\n",
      "epoch: 1, step: 1969, outputs are 0.0480708\n",
      "epoch: 1, step: 1970, outputs are 0.048158925\n",
      "epoch: 1, step: 1971, outputs are 0.05080179\n",
      "epoch: 1, step: 1972, outputs are 0.050303765\n",
      "epoch: 1, step: 1973, outputs are 0.048716668\n",
      "epoch: 1, step: 1974, outputs are 0.047987655\n",
      "epoch: 1, step: 1975, outputs are 0.053675838\n",
      "epoch: 1, step: 1976, outputs are 0.04890348\n",
      "epoch: 1, step: 1977, outputs are 0.044720948\n",
      "epoch: 1, step: 1978, outputs are 0.046407335\n",
      "epoch: 1, step: 1979, outputs are 0.050618008\n",
      "epoch: 1, step: 1980, outputs are 0.04932035\n",
      "epoch: 1, step: 1981, outputs are 0.049226716\n",
      "epoch: 1, step: 1982, outputs are 0.04914309\n",
      "epoch: 1, step: 1983, outputs are 0.050726026\n",
      "epoch: 1, step: 1984, outputs are 0.04723583\n",
      "epoch: 1, step: 1985, outputs are 0.05466894\n",
      "epoch: 1, step: 1986, outputs are 0.05156769\n",
      "epoch: 1, step: 1987, outputs are 0.050203484\n",
      "epoch: 1, step: 1988, outputs are 0.048366804\n",
      "epoch: 1, step: 1989, outputs are 0.052576393\n",
      "epoch: 1, step: 1990, outputs are 0.050254747\n",
      "epoch: 1, step: 1991, outputs are 0.05723797\n",
      "epoch: 1, step: 1992, outputs are 0.05313998\n",
      "epoch: 1, step: 1993, outputs are 0.05043073\n",
      "epoch: 1, step: 1994, outputs are 0.04984989\n",
      "epoch: 1, step: 1995, outputs are 0.045353133\n",
      "epoch: 1, step: 1996, outputs are 0.04667646\n",
      "epoch: 1, step: 1997, outputs are 0.04671024\n",
      "epoch: 1, step: 1998, outputs are 0.05052277\n",
      "epoch: 1, step: 1999, outputs are 0.04881544\n",
      "epoch: 1, step: 2000, outputs are 0.04997217\n",
      "epoch: 1, step: 2001, outputs are 0.055949412\n",
      "epoch: 1, step: 2002, outputs are 0.05185767\n",
      "epoch: 1, step: 2003, outputs are 0.054968614\n",
      "epoch: 1, step: 2004, outputs are 0.051181942\n",
      "epoch: 1, step: 2005, outputs are 0.051867016\n",
      "epoch: 1, step: 2006, outputs are 0.04737579\n",
      "epoch: 1, step: 2007, outputs are 0.056269582\n",
      "epoch: 1, step: 2008, outputs are 0.048340257\n",
      "epoch: 1, step: 2009, outputs are 0.049732175\n",
      "epoch: 1, step: 2010, outputs are 0.050664637\n",
      "epoch: 1, step: 2011, outputs are 0.05347908\n",
      "epoch: 1, step: 2012, outputs are 0.047154993\n",
      "epoch: 1, step: 2013, outputs are 0.04934443\n",
      "epoch: 1, step: 2014, outputs are 0.047189\n",
      "epoch: 1, step: 2015, outputs are 0.04814826\n",
      "epoch: 1, step: 2016, outputs are 0.05084268\n",
      "epoch: 1, step: 2017, outputs are 0.05248092\n",
      "epoch: 1, step: 2018, outputs are 0.04846776\n",
      "epoch: 1, step: 2019, outputs are 0.04595673\n",
      "epoch: 1, step: 2020, outputs are 0.050372988\n",
      "epoch: 1, step: 2021, outputs are 0.04760582\n",
      "epoch: 1, step: 2022, outputs are 0.049375545\n",
      "epoch: 1, step: 2023, outputs are 0.05584853\n",
      "epoch: 1, step: 2024, outputs are 0.04770506\n",
      "epoch: 1, step: 2025, outputs are 0.04632132\n",
      "epoch: 1, step: 2026, outputs are 0.05073411\n",
      "epoch: 1, step: 2027, outputs are 0.04909636\n",
      "epoch: 1, step: 2028, outputs are 0.047515336\n",
      "epoch: 1, step: 2029, outputs are 0.04941976\n",
      "epoch: 1, step: 2030, outputs are 0.048118025\n",
      "epoch: 1, step: 2031, outputs are 0.05306262\n",
      "epoch: 1, step: 2032, outputs are 0.042796\n",
      "epoch: 1, step: 2033, outputs are 0.04637648\n",
      "epoch: 1, step: 2034, outputs are 0.048430763\n",
      "epoch: 1, step: 2035, outputs are 0.050053325\n",
      "epoch: 1, step: 2036, outputs are 0.05507008\n",
      "epoch: 1, step: 2037, outputs are 0.052501142\n",
      "epoch: 1, step: 2038, outputs are 0.050390165\n",
      "epoch: 1, step: 2039, outputs are 0.054234017\n",
      "epoch: 1, step: 2040, outputs are 0.050745197\n",
      "epoch: 1, step: 2041, outputs are 0.04753717\n",
      "epoch: 1, step: 2042, outputs are 0.048873067\n",
      "epoch: 1, step: 2043, outputs are 0.046432372\n",
      "epoch: 1, step: 2044, outputs are 0.04865913\n",
      "epoch: 1, step: 2045, outputs are 0.050114267\n",
      "epoch: 1, step: 2046, outputs are 0.05088795\n",
      "epoch: 1, step: 2047, outputs are 0.050235756\n",
      "epoch: 1, step: 2048, outputs are 0.04942939\n",
      "epoch: 1, step: 2049, outputs are 0.046632864\n",
      "epoch: 1, step: 2050, outputs are 0.047880538\n",
      "epoch: 1, step: 2051, outputs are 0.048944652\n",
      "epoch: 1, step: 2052, outputs are 0.051348973\n",
      "epoch: 1, step: 2053, outputs are 0.051083736\n",
      "epoch: 1, step: 2054, outputs are 0.049592216\n",
      "epoch: 1, step: 2055, outputs are 0.055304356\n",
      "epoch: 1, step: 2056, outputs are 0.04781437\n",
      "epoch: 1, step: 2057, outputs are 0.049620055\n",
      "epoch: 1, step: 2058, outputs are 0.046761177\n",
      "epoch: 1, step: 2059, outputs are 0.044229448\n",
      "epoch: 1, step: 2060, outputs are 0.049691975\n",
      "epoch: 1, step: 2061, outputs are 0.051549174\n",
      "epoch: 1, step: 2062, outputs are 0.046141654\n",
      "epoch: 1, step: 2063, outputs are 0.053864695\n",
      "epoch: 1, step: 2064, outputs are 0.04985708\n",
      "epoch: 1, step: 2065, outputs are 0.052376352\n",
      "epoch: 1, step: 2066, outputs are 0.04937589\n",
      "epoch: 1, step: 2067, outputs are 0.045028433\n",
      "epoch: 1, step: 2068, outputs are 0.050925914\n",
      "epoch: 1, step: 2069, outputs are 0.050288297\n",
      "epoch: 1, step: 2070, outputs are 0.052023903\n",
      "epoch: 1, step: 2071, outputs are 0.048785556\n",
      "epoch: 1, step: 2072, outputs are 0.048479162\n",
      "epoch: 1, step: 2073, outputs are 0.047330383\n",
      "epoch: 1, step: 2074, outputs are 0.057194836\n",
      "epoch: 1, step: 2075, outputs are 0.047423758\n",
      "epoch: 1, step: 2076, outputs are 0.047365613\n",
      "epoch: 1, step: 2077, outputs are 0.04842417\n",
      "epoch: 1, step: 2078, outputs are 0.053744934\n",
      "epoch: 1, step: 2079, outputs are 0.05296293\n",
      "epoch: 1, step: 2080, outputs are 0.051582627\n",
      "epoch: 1, step: 2081, outputs are 0.050631344\n",
      "epoch: 1, step: 2082, outputs are 0.047799174\n",
      "epoch: 1, step: 2083, outputs are 0.044395883\n",
      "epoch: 1, step: 2084, outputs are 0.054010123\n",
      "epoch: 1, step: 2085, outputs are 0.052956503\n",
      "epoch: 1, step: 2086, outputs are 0.048531648\n",
      "epoch: 1, step: 2087, outputs are 0.049567927\n",
      "epoch: 1, step: 2088, outputs are 0.048542965\n",
      "epoch: 1, step: 2089, outputs are 0.049853433\n",
      "epoch: 1, step: 2090, outputs are 0.056268003\n",
      "epoch: 1, step: 2091, outputs are 0.048663743\n",
      "epoch: 1, step: 2092, outputs are 0.05032092\n",
      "epoch: 1, step: 2093, outputs are 0.04925915\n",
      "epoch: 1, step: 2094, outputs are 0.04629587\n",
      "epoch: 1, step: 2095, outputs are 0.051511254\n",
      "epoch: 1, step: 2096, outputs are 0.049981058\n",
      "epoch: 1, step: 2097, outputs are 0.050980225\n",
      "epoch: 1, step: 2098, outputs are 0.04982341\n",
      "epoch: 1, step: 2099, outputs are 0.0509267\n",
      "epoch: 1, step: 2100, outputs are 0.048590604\n",
      "epoch: 1, step: 2101, outputs are 0.04729335\n",
      "epoch: 1, step: 2102, outputs are 0.051704086\n",
      "epoch: 1, step: 2103, outputs are 0.048968248\n",
      "epoch: 1, step: 2104, outputs are 0.048234705\n",
      "epoch: 1, step: 2105, outputs are 0.048786115\n",
      "epoch: 1, step: 2106, outputs are 0.045765128\n",
      "epoch: 1, step: 2107, outputs are 0.05082683\n",
      "epoch: 1, step: 2108, outputs are 0.050561693\n",
      "epoch: 1, step: 2109, outputs are 0.05230391\n",
      "epoch: 1, step: 2110, outputs are 0.05052732\n",
      "epoch: 1, step: 2111, outputs are 0.048131257\n",
      "epoch: 1, step: 2112, outputs are 0.04997864\n",
      "epoch: 1, step: 2113, outputs are 0.052353576\n",
      "epoch: 1, step: 2114, outputs are 0.04899907\n",
      "epoch: 1, step: 2115, outputs are 0.047963575\n",
      "epoch: 1, step: 2116, outputs are 0.050910875\n",
      "epoch: 1, step: 2117, outputs are 0.048666105\n",
      "epoch: 1, step: 2118, outputs are 0.048356008\n",
      "epoch: 1, step: 2119, outputs are 0.04455404\n",
      "epoch: 1, step: 2120, outputs are 0.04964105\n",
      "epoch: 1, step: 2121, outputs are 0.05201863\n",
      "epoch: 1, step: 2122, outputs are 0.05112151\n",
      "epoch: 1, step: 2123, outputs are 0.050056078\n",
      "epoch: 1, step: 2124, outputs are 0.049799427\n",
      "epoch: 1, step: 2125, outputs are 0.05148657\n",
      "epoch: 1, step: 2126, outputs are 0.04560058\n",
      "epoch: 1, step: 2127, outputs are 0.051382255\n",
      "epoch: 1, step: 2128, outputs are 0.047449823\n",
      "epoch: 1, step: 2129, outputs are 0.045654275\n",
      "epoch: 1, step: 2130, outputs are 0.044889446\n",
      "epoch: 1, step: 2131, outputs are 0.04973436\n",
      "epoch: 1, step: 2132, outputs are 0.051896736\n",
      "epoch: 1, step: 2133, outputs are 0.05154568\n",
      "epoch: 1, step: 2134, outputs are 0.04837078\n",
      "epoch: 1, step: 2135, outputs are 0.04915346\n",
      "epoch: 1, step: 2136, outputs are 0.04794517\n",
      "epoch: 1, step: 2137, outputs are 0.05290851\n",
      "epoch: 1, step: 2138, outputs are 0.048303336\n",
      "epoch: 1, step: 2139, outputs are 0.052743312\n",
      "epoch: 1, step: 2140, outputs are 0.052480996\n",
      "epoch: 1, step: 2141, outputs are 0.04769305\n",
      "epoch: 1, step: 2142, outputs are 0.05364238\n",
      "epoch: 1, step: 2143, outputs are 0.048765294\n",
      "epoch: 1, step: 2144, outputs are 0.0490771\n",
      "epoch: 1, step: 2145, outputs are 0.048056126\n",
      "epoch: 1, step: 2146, outputs are 0.05386293\n",
      "epoch: 1, step: 2147, outputs are 0.04614311\n",
      "epoch: 1, step: 2148, outputs are 0.050828196\n",
      "epoch: 1, step: 2149, outputs are 0.04911133\n",
      "epoch: 1, step: 2150, outputs are 0.048787773\n",
      "epoch: 1, step: 2151, outputs are 0.04952419\n",
      "epoch: 1, step: 2152, outputs are 0.052868683\n",
      "epoch: 1, step: 2153, outputs are 0.05093205\n",
      "epoch: 1, step: 2154, outputs are 0.04833813\n",
      "epoch: 1, step: 2155, outputs are 0.049311507\n",
      "epoch: 1, step: 2156, outputs are 0.048763733\n",
      "epoch: 1, step: 2157, outputs are 0.047061265\n",
      "epoch: 1, step: 2158, outputs are 0.04166131\n",
      "epoch: 1, step: 2159, outputs are 0.047421142\n",
      "epoch: 1, step: 2160, outputs are 0.04579357\n",
      "epoch: 1, step: 2161, outputs are 0.04841973\n",
      "epoch: 1, step: 2162, outputs are 0.04788081\n",
      "epoch: 1, step: 2163, outputs are 0.054041617\n",
      "epoch: 1, step: 2164, outputs are 0.051033292\n",
      "epoch: 1, step: 2165, outputs are 0.04873534\n",
      "epoch: 1, step: 2166, outputs are 0.049269773\n",
      "epoch: 1, step: 2167, outputs are 0.044478327\n",
      "epoch: 1, step: 2168, outputs are 0.052327603\n",
      "epoch: 1, step: 2169, outputs are 0.049463287\n",
      "epoch: 1, step: 2170, outputs are 0.047238104\n",
      "epoch: 1, step: 2171, outputs are 0.048986454\n",
      "epoch: 1, step: 2172, outputs are 0.04897715\n",
      "epoch: 1, step: 2173, outputs are 0.051390048\n",
      "epoch: 1, step: 2174, outputs are 0.05217374\n",
      "epoch: 1, step: 2175, outputs are 0.049277607\n",
      "epoch: 1, step: 2176, outputs are 0.050473318\n",
      "epoch: 1, step: 2177, outputs are 0.04947052\n",
      "epoch: 1, step: 2178, outputs are 0.04611647\n",
      "epoch: 1, step: 2179, outputs are 0.04927319\n",
      "epoch: 1, step: 2180, outputs are 0.048300683\n",
      "epoch: 1, step: 2181, outputs are 0.048918948\n",
      "epoch: 1, step: 2182, outputs are 0.04887464\n",
      "epoch: 1, step: 2183, outputs are 0.048628226\n",
      "epoch: 1, step: 2184, outputs are 0.048771694\n",
      "epoch: 1, step: 2185, outputs are 0.05414389\n",
      "epoch: 1, step: 2186, outputs are 0.048760086\n",
      "epoch: 1, step: 2187, outputs are 0.054014765\n",
      "epoch: 1, step: 2188, outputs are 0.045147486\n",
      "epoch: 1, step: 2189, outputs are 0.049553417\n",
      "epoch: 1, step: 2190, outputs are 0.050623972\n",
      "epoch: 1, step: 2191, outputs are 0.04742401\n",
      "epoch: 1, step: 2192, outputs are 0.048574045\n",
      "epoch: 1, step: 2193, outputs are 0.045006882\n",
      "epoch: 1, step: 2194, outputs are 0.048281178\n",
      "epoch: 1, step: 2195, outputs are 0.045898274\n",
      "epoch: 1, step: 2196, outputs are 0.04809975\n",
      "epoch: 1, step: 2197, outputs are 0.048475996\n",
      "epoch: 1, step: 2198, outputs are 0.049818482\n",
      "epoch: 1, step: 2199, outputs are 0.048795298\n",
      "epoch: 1, step: 2200, outputs are 0.05365328\n",
      "epoch: 1, step: 2201, outputs are 0.053467564\n",
      "epoch: 1, step: 2202, outputs are 0.053394057\n",
      "epoch: 1, step: 2203, outputs are 0.04954068\n",
      "epoch: 1, step: 2204, outputs are 0.049721252\n",
      "epoch: 1, step: 2205, outputs are 0.055212207\n",
      "epoch: 1, step: 2206, outputs are 0.05059518\n",
      "epoch: 1, step: 2207, outputs are 0.050441027\n",
      "epoch: 1, step: 2208, outputs are 0.048366785\n",
      "epoch: 1, step: 2209, outputs are 0.04629469\n",
      "epoch: 1, step: 2210, outputs are 0.0483585\n",
      "epoch: 1, step: 2211, outputs are 0.052936286\n",
      "epoch: 1, step: 2212, outputs are 0.05078926\n",
      "epoch: 1, step: 2213, outputs are 0.049542498\n",
      "epoch: 1, step: 2214, outputs are 0.055536922\n",
      "epoch: 1, step: 2215, outputs are 0.05171363\n",
      "epoch: 1, step: 2216, outputs are 0.051817775\n",
      "epoch: 1, step: 2217, outputs are 0.05238228\n",
      "epoch: 1, step: 2218, outputs are 0.046237327\n",
      "epoch: 1, step: 2219, outputs are 0.048471063\n",
      "epoch: 1, step: 2220, outputs are 0.045486446\n",
      "epoch: 1, step: 2221, outputs are 0.04737551\n",
      "epoch: 1, step: 2222, outputs are 0.052402135\n",
      "epoch: 1, step: 2223, outputs are 0.04993322\n",
      "epoch: 1, step: 2224, outputs are 0.047312763\n",
      "epoch: 1, step: 2225, outputs are 0.049286418\n",
      "epoch: 1, step: 2226, outputs are 0.049704764\n",
      "epoch: 1, step: 2227, outputs are 0.055890016\n",
      "epoch: 1, step: 2228, outputs are 0.051197655\n",
      "epoch: 1, step: 2229, outputs are 0.04796137\n",
      "epoch: 1, step: 2230, outputs are 0.049957678\n",
      "epoch: 1, step: 2231, outputs are 0.046571855\n",
      "epoch: 1, step: 2232, outputs are 0.049587034\n",
      "epoch: 1, step: 2233, outputs are 0.044337254\n",
      "epoch: 1, step: 2234, outputs are 0.047978677\n",
      "epoch: 1, step: 2235, outputs are 0.043977275\n",
      "epoch: 1, step: 2236, outputs are 0.04638092\n",
      "epoch: 1, step: 2237, outputs are 0.045042653\n",
      "epoch: 1, step: 2238, outputs are 0.050454155\n",
      "epoch: 1, step: 2239, outputs are 0.04994721\n",
      "epoch: 1, step: 2240, outputs are 0.046483137\n",
      "epoch: 1, step: 2241, outputs are 0.05288045\n",
      "epoch: 1, step: 2242, outputs are 0.04897561\n",
      "epoch: 1, step: 2243, outputs are 0.054461405\n",
      "epoch: 1, step: 2244, outputs are 0.04683169\n",
      "epoch: 1, step: 2245, outputs are 0.047206532\n",
      "epoch: 1, step: 2246, outputs are 0.05002909\n",
      "epoch: 1, step: 2247, outputs are 0.049678463\n",
      "epoch: 1, step: 2248, outputs are 0.046978027\n",
      "epoch: 1, step: 2249, outputs are 0.055936\n",
      "epoch: 1, step: 2250, outputs are 0.04823628\n",
      "epoch: 1, step: 2251, outputs are 0.05025022\n",
      "epoch: 1, step: 2252, outputs are 0.045656793\n",
      "epoch: 1, step: 2253, outputs are 0.048199117\n",
      "epoch: 1, step: 2254, outputs are 0.04648122\n",
      "epoch: 1, step: 2255, outputs are 0.05068259\n",
      "epoch: 1, step: 2256, outputs are 0.045688234\n",
      "epoch: 1, step: 2257, outputs are 0.04768167\n",
      "epoch: 1, step: 2258, outputs are 0.048351154\n",
      "epoch: 1, step: 2259, outputs are 0.045716777\n",
      "epoch: 1, step: 2260, outputs are 0.050812643\n",
      "epoch: 1, step: 2261, outputs are 0.049229153\n",
      "epoch: 1, step: 2262, outputs are 0.047007404\n",
      "epoch: 1, step: 2263, outputs are 0.04756816\n",
      "epoch: 1, step: 2264, outputs are 0.04842697\n",
      "epoch: 1, step: 2265, outputs are 0.050521683\n",
      "epoch: 1, step: 2266, outputs are 0.044874817\n",
      "epoch: 1, step: 2267, outputs are 0.049011964\n",
      "epoch: 1, step: 2268, outputs are 0.047671877\n",
      "epoch: 1, step: 2269, outputs are 0.048879776\n",
      "epoch: 1, step: 2270, outputs are 0.05231936\n",
      "epoch: 1, step: 2271, outputs are 0.052999265\n",
      "epoch: 1, step: 2272, outputs are 0.046561085\n",
      "epoch: 1, step: 2273, outputs are 0.04972913\n",
      "epoch: 1, step: 2274, outputs are 0.049016003\n",
      "epoch: 1, step: 2275, outputs are 0.04677607\n",
      "epoch: 1, step: 2276, outputs are 0.053774334\n",
      "epoch: 1, step: 2277, outputs are 0.04943522\n",
      "epoch: 1, step: 2278, outputs are 0.05320633\n",
      "epoch: 1, step: 2279, outputs are 0.05093505\n",
      "epoch: 1, step: 2280, outputs are 0.049238585\n",
      "epoch: 1, step: 2281, outputs are 0.048318572\n",
      "epoch: 1, step: 2282, outputs are 0.048365787\n",
      "epoch: 1, step: 2283, outputs are 0.054535437\n",
      "epoch: 1, step: 2284, outputs are 0.046663962\n",
      "epoch: 1, step: 2285, outputs are 0.04448107\n",
      "epoch: 1, step: 2286, outputs are 0.04438352\n",
      "epoch: 1, step: 2287, outputs are 0.05107761\n",
      "epoch: 1, step: 2288, outputs are 0.050964355\n",
      "epoch: 1, step: 2289, outputs are 0.044506863\n",
      "epoch: 1, step: 2290, outputs are 0.049911454\n",
      "epoch: 1, step: 2291, outputs are 0.045682333\n",
      "epoch: 1, step: 2292, outputs are 0.05214002\n",
      "epoch: 1, step: 2293, outputs are 0.046305597\n",
      "epoch: 1, step: 2294, outputs are 0.051374458\n",
      "epoch: 1, step: 2295, outputs are 0.057192378\n",
      "epoch: 1, step: 2296, outputs are 0.04795845\n",
      "epoch: 1, step: 2297, outputs are 0.046626933\n",
      "epoch: 1, step: 2298, outputs are 0.048634857\n",
      "epoch: 1, step: 2299, outputs are 0.047493607\n",
      "epoch: 1, step: 2300, outputs are 0.04870254\n",
      "epoch: 1, step: 2301, outputs are 0.052403558\n",
      "epoch: 1, step: 2302, outputs are 0.05219514\n",
      "epoch: 1, step: 2303, outputs are 0.04896703\n",
      "epoch: 1, step: 2304, outputs are 0.05130473\n",
      "epoch: 1, step: 2305, outputs are 0.05122997\n",
      "epoch: 1, step: 2306, outputs are 0.054904867\n",
      "epoch: 1, step: 2307, outputs are 0.051721264\n",
      "epoch: 1, step: 2308, outputs are 0.049462534\n",
      "epoch: 1, step: 2309, outputs are 0.050032377\n",
      "epoch: 1, step: 2310, outputs are 0.04356201\n",
      "epoch: 1, step: 2311, outputs are 0.048282526\n",
      "epoch: 1, step: 2312, outputs are 0.049555827\n",
      "epoch: 1, step: 2313, outputs are 0.048646647\n",
      "epoch: 1, step: 2314, outputs are 0.050766747\n",
      "epoch: 1, step: 2315, outputs are 0.046589445\n",
      "epoch: 1, step: 2316, outputs are 0.045853227\n",
      "epoch: 1, step: 2317, outputs are 0.051695686\n",
      "epoch: 1, step: 2318, outputs are 0.048652634\n",
      "epoch: 1, step: 2319, outputs are 0.051798083\n",
      "epoch: 1, step: 2320, outputs are 0.048884742\n",
      "epoch: 1, step: 2321, outputs are 0.05010586\n",
      "epoch: 1, step: 2322, outputs are 0.047470223\n",
      "epoch: 1, step: 2323, outputs are 0.04756494\n",
      "epoch: 1, step: 2324, outputs are 0.047185577\n",
      "epoch: 1, step: 2325, outputs are 0.052798033\n",
      "epoch: 1, step: 2326, outputs are 0.047517862\n",
      "epoch: 1, step: 2327, outputs are 0.053124696\n",
      "epoch: 1, step: 2328, outputs are 0.048074834\n",
      "epoch: 1, step: 2329, outputs are 0.048844967\n",
      "epoch: 1, step: 2330, outputs are 0.048018295\n",
      "epoch: 1, step: 2331, outputs are 0.04927489\n",
      "epoch: 1, step: 2332, outputs are 0.051084556\n",
      "epoch: 1, step: 2333, outputs are 0.050596304\n",
      "epoch: 1, step: 2334, outputs are 0.052806407\n",
      "epoch: 1, step: 2335, outputs are 0.053101107\n",
      "epoch: 1, step: 2336, outputs are 0.04867096\n",
      "epoch: 1, step: 2337, outputs are 0.048610236\n",
      "epoch: 1, step: 2338, outputs are 0.047665566\n",
      "epoch: 1, step: 2339, outputs are 0.050244723\n",
      "epoch: 1, step: 2340, outputs are 0.050825328\n",
      "epoch: 1, step: 2341, outputs are 0.04780984\n",
      "epoch: 1, step: 2342, outputs are 0.05036684\n",
      "epoch: 1, step: 2343, outputs are 0.048179947\n",
      "epoch: 1, step: 2344, outputs are 0.049099773\n",
      "epoch: 1, step: 2345, outputs are 0.04616992\n",
      "epoch: 1, step: 2346, outputs are 0.047418065\n",
      "epoch: 1, step: 2347, outputs are 0.047750294\n",
      "epoch: 1, step: 2348, outputs are 0.05037649\n",
      "epoch: 1, step: 2349, outputs are 0.04798563\n",
      "epoch: 1, step: 2350, outputs are 0.051147904\n",
      "epoch: 1, step: 2351, outputs are 0.051066026\n",
      "epoch: 1, step: 2352, outputs are 0.04805122\n",
      "epoch: 1, step: 2353, outputs are 0.053407975\n",
      "epoch: 1, step: 2354, outputs are 0.051365215\n",
      "epoch: 1, step: 2355, outputs are 0.047511015\n",
      "epoch: 1, step: 2356, outputs are 0.049238067\n",
      "epoch: 1, step: 2357, outputs are 0.050137732\n",
      "epoch: 1, step: 2358, outputs are 0.050718285\n",
      "epoch: 1, step: 2359, outputs are 0.053133953\n",
      "epoch: 1, step: 2360, outputs are 0.05371714\n",
      "epoch: 1, step: 2361, outputs are 0.049652334\n",
      "epoch: 1, step: 2362, outputs are 0.053796772\n",
      "epoch: 1, step: 2363, outputs are 0.056147974\n",
      "epoch: 1, step: 2364, outputs are 0.049926784\n",
      "epoch: 1, step: 2365, outputs are 0.05400517\n",
      "epoch: 1, step: 2366, outputs are 0.04909554\n",
      "epoch: 1, step: 2367, outputs are 0.05055312\n",
      "epoch: 1, step: 2368, outputs are 0.050176814\n",
      "epoch: 1, step: 2369, outputs are 0.05106801\n",
      "epoch: 1, step: 2370, outputs are 0.045616586\n",
      "epoch: 1, step: 2371, outputs are 0.046732347\n",
      "epoch: 1, step: 2372, outputs are 0.048658717\n",
      "epoch: 1, step: 2373, outputs are 0.04815369\n",
      "epoch: 1, step: 2374, outputs are 0.050944727\n",
      "epoch: 1, step: 2375, outputs are 0.050229214\n",
      "epoch: 1, step: 2376, outputs are 0.04717225\n",
      "epoch: 1, step: 2377, outputs are 0.048524953\n",
      "epoch: 1, step: 2378, outputs are 0.053518504\n",
      "epoch: 1, step: 2379, outputs are 0.045045607\n",
      "epoch: 1, step: 2380, outputs are 0.04961038\n",
      "epoch: 1, step: 2381, outputs are 0.046927195\n",
      "epoch: 1, step: 2382, outputs are 0.05270099\n",
      "epoch: 1, step: 2383, outputs are 0.050845332\n",
      "epoch: 1, step: 2384, outputs are 0.040797777\n",
      "epoch: 1, step: 2385, outputs are 0.053090118\n",
      "epoch: 1, step: 2386, outputs are 0.04934243\n",
      "epoch: 1, step: 2387, outputs are 0.04960075\n",
      "epoch: 1, step: 2388, outputs are 0.044897534\n",
      "epoch: 1, step: 2389, outputs are 0.049673058\n",
      "epoch: 1, step: 2390, outputs are 0.04927529\n",
      "epoch: 1, step: 2391, outputs are 0.05152349\n",
      "epoch: 1, step: 2392, outputs are 0.04384062\n",
      "epoch: 1, step: 2393, outputs are 0.051463906\n",
      "epoch: 1, step: 2394, outputs are 0.04796473\n",
      "epoch: 1, step: 2395, outputs are 0.04880637\n",
      "epoch: 1, step: 2396, outputs are 0.04957635\n",
      "epoch: 1, step: 2397, outputs are 0.050521806\n",
      "epoch: 1, step: 2398, outputs are 0.048711415\n",
      "epoch: 1, step: 2399, outputs are 0.046549045\n",
      "epoch: 1, step: 2400, outputs are 0.046444725\n",
      "epoch: 1, step: 2401, outputs are 0.04871599\n",
      "epoch: 1, step: 2402, outputs are 0.056497984\n",
      "epoch: 1, step: 2403, outputs are 0.05164068\n",
      "epoch: 1, step: 2404, outputs are 0.052469235\n",
      "epoch: 1, step: 2405, outputs are 0.04976449\n",
      "epoch: 1, step: 2406, outputs are 0.04832757\n",
      "epoch: 1, step: 2407, outputs are 0.056573115\n",
      "epoch: 1, step: 2408, outputs are 0.04888618\n",
      "epoch: 1, step: 2409, outputs are 0.04938851\n",
      "epoch: 1, step: 2410, outputs are 0.052656785\n",
      "epoch: 1, step: 2411, outputs are 0.051069748\n",
      "epoch: 1, step: 2412, outputs are 0.046137273\n",
      "epoch: 1, step: 2413, outputs are 0.049654707\n",
      "epoch: 1, step: 2414, outputs are 0.056796506\n",
      "epoch: 1, step: 2415, outputs are 0.048544183\n",
      "epoch: 1, step: 2416, outputs are 0.05673825\n",
      "epoch: 1, step: 2417, outputs are 0.05069661\n",
      "epoch: 1, step: 2418, outputs are 0.046306875\n",
      "epoch: 1, step: 2419, outputs are 0.04376514\n",
      "epoch: 1, step: 2420, outputs are 0.05237483\n",
      "epoch: 1, step: 2421, outputs are 0.045560326\n",
      "epoch: 1, step: 2422, outputs are 0.048555173\n",
      "epoch: 1, step: 2423, outputs are 0.05114729\n",
      "epoch: 1, step: 2424, outputs are 0.050820168\n",
      "epoch: 1, step: 2425, outputs are 0.049203552\n",
      "epoch: 1, step: 2426, outputs are 0.051612936\n",
      "epoch: 1, step: 2427, outputs are 0.04870496\n",
      "epoch: 1, step: 2428, outputs are 0.051956773\n",
      "epoch: 1, step: 2429, outputs are 0.05147563\n",
      "epoch: 1, step: 2430, outputs are 0.046450067\n",
      "epoch: 1, step: 2431, outputs are 0.0461871\n",
      "epoch: 1, step: 2432, outputs are 0.046845037\n",
      "epoch: 1, step: 2433, outputs are 0.047616947\n",
      "epoch: 1, step: 2434, outputs are 0.050195806\n",
      "epoch: 1, step: 2435, outputs are 0.047752813\n",
      "epoch: 1, step: 2436, outputs are 0.049803924\n",
      "epoch: 1, step: 2437, outputs are 0.050023943\n",
      "epoch: 1, step: 2438, outputs are 0.04889619\n",
      "epoch: 1, step: 2439, outputs are 0.053533953\n",
      "epoch: 1, step: 2440, outputs are 0.053850126\n",
      "epoch: 1, step: 2441, outputs are 0.048549525\n",
      "epoch: 1, step: 2442, outputs are 0.051676564\n",
      "epoch: 1, step: 2443, outputs are 0.047956567\n",
      "epoch: 1, step: 2444, outputs are 0.05181347\n",
      "epoch: 1, step: 2445, outputs are 0.049123574\n",
      "epoch: 1, step: 2446, outputs are 0.048834614\n",
      "epoch: 1, step: 2447, outputs are 0.049610265\n",
      "epoch: 1, step: 2448, outputs are 0.05078129\n",
      "epoch: 1, step: 2449, outputs are 0.04943082\n",
      "epoch: 1, step: 2450, outputs are 0.05091647\n",
      "epoch: 1, step: 2451, outputs are 0.051372245\n",
      "epoch: 1, step: 2452, outputs are 0.046890497\n",
      "epoch: 1, step: 2453, outputs are 0.049387656\n",
      "epoch: 1, step: 2454, outputs are 0.0490761\n",
      "epoch: 1, step: 2455, outputs are 0.05886659\n",
      "epoch: 1, step: 2456, outputs are 0.04397149\n",
      "epoch: 1, step: 2457, outputs are 0.049577735\n",
      "epoch: 1, step: 2458, outputs are 0.050144043\n",
      "epoch: 1, step: 2459, outputs are 0.049732096\n",
      "epoch: 1, step: 2460, outputs are 0.051541835\n",
      "epoch: 1, step: 2461, outputs are 0.05020757\n",
      "epoch: 1, step: 2462, outputs are 0.04945246\n",
      "epoch: 1, step: 2463, outputs are 0.04936263\n",
      "epoch: 1, step: 2464, outputs are 0.047378168\n",
      "epoch: 1, step: 2465, outputs are 0.04639695\n",
      "epoch: 1, step: 2466, outputs are 0.048910215\n",
      "epoch: 1, step: 2467, outputs are 0.054973587\n",
      "epoch: 1, step: 2468, outputs are 0.051352672\n",
      "epoch: 1, step: 2469, outputs are 0.05092938\n",
      "epoch: 1, step: 2470, outputs are 0.051230356\n",
      "epoch: 1, step: 2471, outputs are 0.050297845\n",
      "epoch: 1, step: 2472, outputs are 0.048752837\n",
      "epoch: 1, step: 2473, outputs are 0.053013872\n",
      "epoch: 1, step: 2474, outputs are 0.052510194\n",
      "epoch: 1, step: 2475, outputs are 0.0446332\n",
      "epoch: 1, step: 2476, outputs are 0.045243476\n",
      "epoch: 1, step: 2477, outputs are 0.049195837\n",
      "epoch: 1, step: 2478, outputs are 0.05112698\n",
      "epoch: 1, step: 2479, outputs are 0.050392143\n",
      "epoch: 1, step: 2480, outputs are 0.050317504\n",
      "epoch: 1, step: 2481, outputs are 0.05422825\n",
      "epoch: 1, step: 2482, outputs are 0.0550407\n",
      "epoch: 1, step: 2483, outputs are 0.048835248\n",
      "epoch: 1, step: 2484, outputs are 0.05160328\n",
      "epoch: 1, step: 2485, outputs are 0.04794802\n",
      "epoch: 1, step: 2486, outputs are 0.048958473\n",
      "epoch: 1, step: 2487, outputs are 0.049020793\n",
      "epoch: 1, step: 2488, outputs are 0.048351776\n",
      "epoch: 1, step: 2489, outputs are 0.04895911\n",
      "epoch: 1, step: 2490, outputs are 0.050507933\n",
      "epoch: 1, step: 2491, outputs are 0.053653356\n",
      "epoch: 1, step: 2492, outputs are 0.049375366\n",
      "epoch: 1, step: 2493, outputs are 0.050693452\n",
      "epoch: 1, step: 2494, outputs are 0.054574396\n",
      "epoch: 1, step: 2495, outputs are 0.04906495\n",
      "epoch: 1, step: 2496, outputs are 0.050294414\n",
      "epoch: 1, step: 2497, outputs are 0.048265226\n",
      "epoch: 1, step: 2498, outputs are 0.052168008\n",
      "epoch: 1, step: 2499, outputs are 0.05095\n",
      "epoch: 1, step: 2500, outputs are 0.048878364\n",
      "epoch: 1, step: 2501, outputs are 0.049669527\n",
      "epoch: 1, step: 2502, outputs are 0.046250947\n",
      "epoch: 1, step: 2503, outputs are 0.049649976\n",
      "epoch: 1, step: 2504, outputs are 0.050904177\n",
      "epoch: 1, step: 2505, outputs are 0.046180215\n",
      "epoch: 1, step: 2506, outputs are 0.050214946\n",
      "epoch: 1, step: 2507, outputs are 0.052389883\n",
      "epoch: 1, step: 2508, outputs are 0.05149756\n",
      "epoch: 1, step: 2509, outputs are 0.0477987\n",
      "epoch: 1, step: 2510, outputs are 0.05244406\n",
      "epoch: 1, step: 2511, outputs are 0.04721807\n",
      "epoch: 1, step: 2512, outputs are 0.04704095\n",
      "epoch: 1, step: 2513, outputs are 0.04545233\n",
      "epoch: 1, step: 2514, outputs are 0.048019625\n",
      "epoch: 1, step: 2515, outputs are 0.047593698\n",
      "epoch: 1, step: 2516, outputs are 0.051551443\n",
      "epoch: 1, step: 2517, outputs are 0.047537554\n",
      "epoch: 1, step: 2518, outputs are 0.047554567\n",
      "epoch: 1, step: 2519, outputs are 0.05199758\n",
      "epoch: 1, step: 2520, outputs are 0.054014936\n",
      "epoch: 1, step: 2521, outputs are 0.04981547\n",
      "epoch: 1, step: 2522, outputs are 0.05069205\n",
      "epoch: 1, step: 2523, outputs are 0.049387492\n",
      "epoch: 1, step: 2524, outputs are 0.050279353\n",
      "epoch: 1, step: 2525, outputs are 0.049754404\n",
      "epoch: 1, step: 2526, outputs are 0.049757417\n",
      "epoch: 1, step: 2527, outputs are 0.049638707\n",
      "epoch: 1, step: 2528, outputs are 0.05014842\n",
      "epoch: 1, step: 2529, outputs are 0.051377084\n",
      "epoch: 1, step: 2530, outputs are 0.048188575\n",
      "epoch: 1, step: 2531, outputs are 0.04888244\n",
      "epoch: 1, step: 2532, outputs are 0.052234724\n",
      "epoch: 1, step: 2533, outputs are 0.04978421\n",
      "epoch: 1, step: 2534, outputs are 0.04866912\n",
      "epoch: 1, step: 2535, outputs are 0.051086545\n",
      "epoch: 1, step: 2536, outputs are 0.053369794\n",
      "epoch: 1, step: 2537, outputs are 0.051988035\n",
      "epoch: 1, step: 2538, outputs are 0.047479693\n",
      "epoch: 1, step: 2539, outputs are 0.047822535\n",
      "epoch: 1, step: 2540, outputs are 0.051265247\n",
      "epoch: 1, step: 2541, outputs are 0.05117124\n",
      "epoch: 1, step: 2542, outputs are 0.049636863\n",
      "epoch: 1, step: 2543, outputs are 0.051024552\n",
      "epoch: 1, step: 2544, outputs are 0.045483727\n",
      "epoch: 1, step: 2545, outputs are 0.050303638\n",
      "epoch: 1, step: 2546, outputs are 0.047875453\n",
      "epoch: 1, step: 2547, outputs are 0.04770347\n",
      "epoch: 1, step: 2548, outputs are 0.0492746\n",
      "epoch: 1, step: 2549, outputs are 0.054277875\n",
      "epoch: 1, step: 2550, outputs are 0.04986746\n",
      "epoch: 1, step: 2551, outputs are 0.048525967\n",
      "epoch: 1, step: 2552, outputs are 0.047977567\n",
      "epoch: 1, step: 2553, outputs are 0.05158569\n",
      "epoch: 1, step: 2554, outputs are 0.049145136\n",
      "epoch: 1, step: 2555, outputs are 0.051831517\n",
      "epoch: 1, step: 2556, outputs are 0.04892379\n",
      "epoch: 1, step: 2557, outputs are 0.049182996\n",
      "epoch: 1, step: 2558, outputs are 0.055635825\n",
      "epoch: 1, step: 2559, outputs are 0.051017623\n",
      "epoch: 1, step: 2560, outputs are 0.047642276\n",
      "epoch: 1, step: 2561, outputs are 0.055244435\n",
      "epoch: 1, step: 2562, outputs are 0.04816438\n",
      "epoch: 1, step: 2563, outputs are 0.048631497\n",
      "epoch: 1, step: 2564, outputs are 0.046715826\n",
      "epoch: 1, step: 2565, outputs are 0.049000815\n",
      "epoch: 1, step: 2566, outputs are 0.049945436\n",
      "epoch: 1, step: 2567, outputs are 0.04786693\n",
      "epoch: 1, step: 2568, outputs are 0.051721968\n",
      "epoch: 1, step: 2569, outputs are 0.04935611\n",
      "epoch: 1, step: 2570, outputs are 0.051327854\n",
      "epoch: 1, step: 2571, outputs are 0.047629006\n",
      "epoch: 1, step: 2572, outputs are 0.05258338\n",
      "epoch: 1, step: 2573, outputs are 0.047672994\n",
      "epoch: 1, step: 2574, outputs are 0.04995706\n",
      "epoch: 1, step: 2575, outputs are 0.051556375\n",
      "epoch: 1, step: 2576, outputs are 0.05431956\n",
      "epoch: 1, step: 2577, outputs are 0.0450053\n",
      "epoch: 1, step: 2578, outputs are 0.05257882\n",
      "epoch: 1, step: 2579, outputs are 0.050387617\n",
      "epoch: 1, step: 2580, outputs are 0.050880175\n",
      "epoch: 1, step: 2581, outputs are 0.044868536\n",
      "epoch: 1, step: 2582, outputs are 0.04899141\n",
      "epoch: 1, step: 2583, outputs are 0.052494474\n",
      "epoch: 1, step: 2584, outputs are 0.0500042\n",
      "epoch: 1, step: 2585, outputs are 0.05013308\n",
      "epoch: 1, step: 2586, outputs are 0.050403282\n",
      "epoch: 1, step: 2587, outputs are 0.049388908\n",
      "epoch: 1, step: 2588, outputs are 0.050136533\n",
      "epoch: 1, step: 2589, outputs are 0.050926875\n",
      "epoch: 1, step: 2590, outputs are 0.044961985\n",
      "epoch: 1, step: 2591, outputs are 0.042864025\n",
      "epoch: 1, step: 2592, outputs are 0.048575446\n",
      "epoch: 1, step: 2593, outputs are 0.047981642\n",
      "epoch: 1, step: 2594, outputs are 0.05528376\n",
      "epoch: 1, step: 2595, outputs are 0.0509211\n",
      "epoch: 1, step: 2596, outputs are 0.04840596\n",
      "epoch: 1, step: 2597, outputs are 0.054480374\n",
      "epoch: 1, step: 2598, outputs are 0.051535916\n",
      "epoch: 1, step: 2599, outputs are 0.04855963\n",
      "epoch: 1, step: 2600, outputs are 0.0444092\n",
      "epoch: 1, step: 2601, outputs are 0.04760347\n",
      "epoch: 1, step: 2602, outputs are 0.04717171\n",
      "epoch: 1, step: 2603, outputs are 0.048691444\n",
      "epoch: 1, step: 2604, outputs are 0.049989864\n",
      "epoch: 1, step: 2605, outputs are 0.050945166\n",
      "epoch: 1, step: 2606, outputs are 0.04755644\n",
      "epoch: 1, step: 2607, outputs are 0.05108624\n",
      "epoch: 1, step: 2608, outputs are 0.047906928\n",
      "epoch: 1, step: 2609, outputs are 0.047623105\n",
      "epoch: 1, step: 2610, outputs are 0.053114362\n",
      "epoch: 1, step: 2611, outputs are 0.048738547\n",
      "epoch: 1, step: 2612, outputs are 0.04802626\n",
      "epoch: 1, step: 2613, outputs are 0.05163534\n",
      "epoch: 1, step: 2614, outputs are 0.04720682\n",
      "epoch: 1, step: 2615, outputs are 0.05062397\n",
      "epoch: 1, step: 2616, outputs are 0.047936425\n",
      "epoch: 1, step: 2617, outputs are 0.05227366\n",
      "epoch: 1, step: 2618, outputs are 0.05052392\n",
      "epoch: 1, step: 2619, outputs are 0.047638506\n",
      "epoch: 1, step: 2620, outputs are 0.05077366\n",
      "epoch: 1, step: 2621, outputs are 0.050133504\n",
      "epoch: 1, step: 2622, outputs are 0.051209077\n",
      "epoch: 1, step: 2623, outputs are 0.051872734\n",
      "epoch: 1, step: 2624, outputs are 0.048683133\n",
      "epoch: 1, step: 2625, outputs are 0.049214307\n",
      "epoch: 1, step: 2626, outputs are 0.04779992\n",
      "epoch: 1, step: 2627, outputs are 0.050031886\n",
      "epoch: 1, step: 2628, outputs are 0.04760134\n",
      "epoch: 1, step: 2629, outputs are 0.05223497\n",
      "epoch: 1, step: 2630, outputs are 0.052215397\n",
      "epoch: 1, step: 2631, outputs are 0.048758652\n",
      "epoch: 1, step: 2632, outputs are 0.0467996\n",
      "epoch: 1, step: 2633, outputs are 0.047797393\n",
      "epoch: 1, step: 2634, outputs are 0.05172931\n",
      "epoch: 1, step: 2635, outputs are 0.045557793\n",
      "epoch: 1, step: 2636, outputs are 0.04938302\n",
      "epoch: 1, step: 2637, outputs are 0.051452585\n",
      "epoch: 1, step: 2638, outputs are 0.05230875\n",
      "epoch: 1, step: 2639, outputs are 0.054118372\n",
      "epoch: 1, step: 2640, outputs are 0.048537664\n",
      "epoch: 1, step: 2641, outputs are 0.05120061\n",
      "epoch: 1, step: 2642, outputs are 0.05484554\n",
      "epoch: 1, step: 2643, outputs are 0.04582925\n",
      "epoch: 1, step: 2644, outputs are 0.046820104\n",
      "epoch: 1, step: 2645, outputs are 0.049104862\n",
      "epoch: 1, step: 2646, outputs are 0.05249653\n",
      "epoch: 1, step: 2647, outputs are 0.047110625\n",
      "epoch: 1, step: 2648, outputs are 0.052502137\n",
      "epoch: 1, step: 2649, outputs are 0.050380304\n",
      "epoch: 1, step: 2650, outputs are 0.04953079\n",
      "epoch: 1, step: 2651, outputs are 0.044220395\n",
      "epoch: 1, step: 2652, outputs are 0.047882035\n",
      "epoch: 1, step: 2653, outputs are 0.05192447\n",
      "epoch: 1, step: 2654, outputs are 0.053709667\n",
      "epoch: 1, step: 2655, outputs are 0.050357237\n",
      "epoch: 1, step: 2656, outputs are 0.04855904\n",
      "epoch: 1, step: 2657, outputs are 0.0502551\n",
      "epoch: 1, step: 2658, outputs are 0.05020819\n",
      "epoch: 1, step: 2659, outputs are 0.046887547\n",
      "epoch: 1, step: 2660, outputs are 0.048378684\n",
      "epoch: 1, step: 2661, outputs are 0.053596046\n",
      "epoch: 1, step: 2662, outputs are 0.05385781\n",
      "epoch: 1, step: 2663, outputs are 0.05129061\n",
      "epoch: 1, step: 2664, outputs are 0.04775996\n",
      "epoch: 1, step: 2665, outputs are 0.05070111\n",
      "epoch: 1, step: 2666, outputs are 0.048385408\n",
      "epoch: 1, step: 2667, outputs are 0.05104039\n",
      "epoch: 1, step: 2668, outputs are 0.04819472\n",
      "epoch: 1, step: 2669, outputs are 0.05078894\n",
      "epoch: 1, step: 2670, outputs are 0.048110925\n",
      "epoch: 1, step: 2671, outputs are 0.050046515\n",
      "epoch: 1, step: 2672, outputs are 0.05107779\n",
      "epoch: 1, step: 2673, outputs are 0.049331766\n",
      "epoch: 1, step: 2674, outputs are 0.054162413\n",
      "epoch: 1, step: 2675, outputs are 0.050657965\n",
      "epoch: 1, step: 2676, outputs are 0.04920344\n",
      "epoch: 1, step: 2677, outputs are 0.048161946\n",
      "epoch: 1, step: 2678, outputs are 0.046523735\n",
      "epoch: 1, step: 2679, outputs are 0.048135135\n",
      "epoch: 1, step: 2680, outputs are 0.050374143\n",
      "epoch: 1, step: 2681, outputs are 0.04679724\n",
      "epoch: 1, step: 2682, outputs are 0.04882995\n",
      "epoch: 1, step: 2683, outputs are 0.04867462\n",
      "epoch: 1, step: 2684, outputs are 0.04505284\n",
      "epoch: 1, step: 2685, outputs are 0.050748017\n",
      "epoch: 1, step: 2686, outputs are 0.058599018\n",
      "epoch: 1, step: 2687, outputs are 0.04884486\n",
      "epoch: 1, step: 2688, outputs are 0.047232643\n",
      "epoch: 1, step: 2689, outputs are 0.04983467\n",
      "epoch: 1, step: 2690, outputs are 0.048662096\n",
      "epoch: 1, step: 2691, outputs are 0.04811449\n",
      "epoch: 1, step: 2692, outputs are 0.05379674\n",
      "epoch: 1, step: 2693, outputs are 0.04711844\n",
      "epoch: 1, step: 2694, outputs are 0.048734892\n",
      "epoch: 1, step: 2695, outputs are 0.052332994\n",
      "epoch: 1, step: 2696, outputs are 0.04492575\n",
      "epoch: 1, step: 2697, outputs are 0.0512568\n",
      "epoch: 1, step: 2698, outputs are 0.049497336\n",
      "epoch: 1, step: 2699, outputs are 0.04843902\n",
      "epoch: 1, step: 2700, outputs are 0.05078102\n",
      "epoch: 1, step: 2701, outputs are 0.04653941\n",
      "epoch: 1, step: 2702, outputs are 0.05117149\n",
      "epoch: 1, step: 2703, outputs are 0.047055915\n",
      "epoch: 1, step: 2704, outputs are 0.049836107\n",
      "epoch: 1, step: 2705, outputs are 0.050467156\n",
      "epoch: 1, step: 2706, outputs are 0.051498845\n",
      "epoch: 1, step: 2707, outputs are 0.053088967\n",
      "epoch: 1, step: 2708, outputs are 0.051868845\n",
      "epoch: 1, step: 2709, outputs are 0.049067438\n",
      "epoch: 1, step: 2710, outputs are 0.049282644\n",
      "epoch: 1, step: 2711, outputs are 0.04925437\n",
      "epoch: 1, step: 2712, outputs are 0.04665935\n",
      "epoch: 1, step: 2713, outputs are 0.045336753\n",
      "epoch: 1, step: 2714, outputs are 0.048927404\n",
      "epoch: 1, step: 2715, outputs are 0.049152385\n",
      "epoch: 1, step: 2716, outputs are 0.045093123\n",
      "epoch: 1, step: 2717, outputs are 0.04792094\n",
      "epoch: 1, step: 2718, outputs are 0.05424123\n",
      "epoch: 1, step: 2719, outputs are 0.054847084\n",
      "epoch: 1, step: 2720, outputs are 0.048566993\n",
      "epoch: 1, step: 2721, outputs are 0.046239905\n",
      "epoch: 1, step: 2722, outputs are 0.054239392\n",
      "epoch: 1, step: 2723, outputs are 0.049206153\n",
      "epoch: 1, step: 2724, outputs are 0.052795034\n",
      "epoch: 1, step: 2725, outputs are 0.049157787\n",
      "epoch: 1, step: 2726, outputs are 0.044998422\n",
      "epoch: 1, step: 2727, outputs are 0.05062611\n",
      "epoch: 1, step: 2728, outputs are 0.050694987\n",
      "epoch: 1, step: 2729, outputs are 0.05134608\n",
      "epoch: 1, step: 2730, outputs are 0.053934485\n",
      "epoch: 1, step: 2731, outputs are 0.045433193\n",
      "epoch: 1, step: 2732, outputs are 0.047806352\n",
      "epoch: 1, step: 2733, outputs are 0.05126954\n",
      "epoch: 1, step: 2734, outputs are 0.05173458\n",
      "epoch: 1, step: 2735, outputs are 0.04817983\n",
      "epoch: 1, step: 2736, outputs are 0.048834648\n",
      "epoch: 1, step: 2737, outputs are 0.050297324\n",
      "epoch: 1, step: 2738, outputs are 0.047946297\n",
      "epoch: 1, step: 2739, outputs are 0.049439035\n",
      "epoch: 1, step: 2740, outputs are 0.049946588\n",
      "epoch: 1, step: 2741, outputs are 0.049052443\n",
      "epoch: 1, step: 2742, outputs are 0.04854624\n",
      "epoch: 1, step: 2743, outputs are 0.048725568\n",
      "epoch: 1, step: 2744, outputs are 0.047358427\n",
      "epoch: 1, step: 2745, outputs are 0.043866046\n",
      "epoch: 1, step: 2746, outputs are 0.052772492\n",
      "epoch: 1, step: 2747, outputs are 0.05074667\n",
      "epoch: 1, step: 2748, outputs are 0.049021464\n",
      "epoch: 1, step: 2749, outputs are 0.04671991\n",
      "epoch: 1, step: 2750, outputs are 0.050829757\n",
      "epoch: 1, step: 2751, outputs are 0.052839\n",
      "epoch: 1, step: 2752, outputs are 0.052157804\n",
      "epoch: 1, step: 2753, outputs are 0.054787822\n",
      "epoch: 1, step: 2754, outputs are 0.051745355\n",
      "epoch: 1, step: 2755, outputs are 0.050028782\n",
      "epoch: 1, step: 2756, outputs are 0.04385299\n",
      "epoch: 1, step: 2757, outputs are 0.052963946\n",
      "epoch: 1, step: 2758, outputs are 0.04596158\n",
      "epoch: 1, step: 2759, outputs are 0.05078383\n",
      "epoch: 1, step: 2760, outputs are 0.048732396\n",
      "epoch: 1, step: 2761, outputs are 0.050599642\n",
      "epoch: 1, step: 2762, outputs are 0.04809707\n",
      "epoch: 1, step: 2763, outputs are 0.050490364\n",
      "epoch: 1, step: 2764, outputs are 0.051902182\n",
      "epoch: 1, step: 2765, outputs are 0.046533003\n",
      "epoch: 1, step: 2766, outputs are 0.044597663\n",
      "epoch: 1, step: 2767, outputs are 0.04886396\n",
      "epoch: 1, step: 2768, outputs are 0.050695643\n",
      "epoch: 1, step: 2769, outputs are 0.049229823\n",
      "epoch: 1, step: 2770, outputs are 0.04717024\n",
      "epoch: 1, step: 2771, outputs are 0.051381476\n",
      "epoch: 1, step: 2772, outputs are 0.051613886\n",
      "epoch: 1, step: 2773, outputs are 0.04631179\n",
      "epoch: 1, step: 2774, outputs are 0.04891073\n",
      "epoch: 1, step: 2775, outputs are 0.04960013\n",
      "epoch: 1, step: 2776, outputs are 0.049491167\n",
      "epoch: 1, step: 2777, outputs are 0.046366923\n",
      "epoch: 1, step: 2778, outputs are 0.05204588\n",
      "epoch: 1, step: 2779, outputs are 0.049421392\n",
      "epoch: 1, step: 2780, outputs are 0.048157804\n",
      "epoch: 1, step: 2781, outputs are 0.0551011\n",
      "epoch: 1, step: 2782, outputs are 0.04879954\n",
      "epoch: 1, step: 2783, outputs are 0.05189936\n",
      "epoch: 1, step: 2784, outputs are 0.051913396\n",
      "epoch: 1, step: 2785, outputs are 0.050009213\n",
      "epoch: 1, step: 2786, outputs are 0.052455243\n",
      "epoch: 1, step: 2787, outputs are 0.05293276\n",
      "epoch: 1, step: 2788, outputs are 0.043843076\n",
      "epoch: 1, step: 2789, outputs are 0.050812285\n",
      "epoch: 1, step: 2790, outputs are 0.05010446\n",
      "epoch: 1, step: 2791, outputs are 0.04557392\n",
      "epoch: 1, step: 2792, outputs are 0.049360413\n",
      "epoch: 1, step: 2793, outputs are 0.04970362\n",
      "epoch: 1, step: 2794, outputs are 0.0495732\n",
      "epoch: 1, step: 2795, outputs are 0.046060894\n",
      "epoch: 1, step: 2796, outputs are 0.04670179\n",
      "epoch: 1, step: 2797, outputs are 0.052329242\n",
      "epoch: 1, step: 2798, outputs are 0.047377136\n",
      "epoch: 1, step: 2799, outputs are 0.050559442\n",
      "epoch: 1, step: 2800, outputs are 0.04608181\n",
      "epoch: 1, step: 2801, outputs are 0.04528733\n",
      "epoch: 1, step: 2802, outputs are 0.047002066\n",
      "epoch: 1, step: 2803, outputs are 0.05380707\n",
      "epoch: 1, step: 2804, outputs are 0.049008984\n",
      "epoch: 1, step: 2805, outputs are 0.051353067\n",
      "epoch: 1, step: 2806, outputs are 0.0505848\n",
      "epoch: 1, step: 2807, outputs are 0.05042311\n",
      "epoch: 1, step: 2808, outputs are 0.05449039\n",
      "epoch: 1, step: 2809, outputs are 0.052085623\n",
      "epoch: 1, step: 2810, outputs are 0.047896374\n",
      "epoch: 1, step: 2811, outputs are 0.0445235\n",
      "epoch: 1, step: 2812, outputs are 0.04726228\n",
      "epoch: 1, step: 2813, outputs are 0.045144666\n",
      "epoch: 1, step: 2814, outputs are 0.049886882\n",
      "epoch: 1, step: 2815, outputs are 0.050400507\n",
      "epoch: 1, step: 2816, outputs are 0.046495836\n",
      "epoch: 1, step: 2817, outputs are 0.049448796\n",
      "epoch: 1, step: 2818, outputs are 0.05218911\n",
      "epoch: 1, step: 2819, outputs are 0.0484735\n",
      "epoch: 1, step: 2820, outputs are 0.04522563\n",
      "epoch: 1, step: 2821, outputs are 0.0484797\n",
      "epoch: 1, step: 2822, outputs are 0.05010511\n",
      "epoch: 1, step: 2823, outputs are 0.053237163\n",
      "epoch: 1, step: 2824, outputs are 0.04644492\n",
      "epoch: 1, step: 2825, outputs are 0.05421576\n",
      "epoch: 1, step: 2826, outputs are 0.05381891\n",
      "epoch: 1, step: 2827, outputs are 0.054858036\n",
      "epoch: 1, step: 2828, outputs are 0.046253048\n",
      "epoch: 1, step: 2829, outputs are 0.05442728\n",
      "epoch: 1, step: 2830, outputs are 0.049215954\n",
      "epoch: 1, step: 2831, outputs are 0.048340373\n",
      "epoch: 1, step: 2832, outputs are 0.047630556\n",
      "epoch: 1, step: 2833, outputs are 0.050097167\n",
      "epoch: 1, step: 2834, outputs are 0.046540637\n",
      "epoch: 1, step: 2835, outputs are 0.045609113\n",
      "epoch: 1, step: 2836, outputs are 0.04771869\n",
      "epoch: 1, step: 2837, outputs are 0.047171347\n",
      "epoch: 1, step: 2838, outputs are 0.043727346\n",
      "epoch: 1, step: 2839, outputs are 0.04804461\n",
      "epoch: 1, step: 2840, outputs are 0.049812883\n",
      "epoch: 1, step: 2841, outputs are 0.049402043\n",
      "epoch: 1, step: 2842, outputs are 0.04498414\n",
      "epoch: 1, step: 2843, outputs are 0.047644693\n",
      "epoch: 1, step: 2844, outputs are 0.04936048\n",
      "epoch: 1, step: 2845, outputs are 0.048534002\n",
      "epoch: 1, step: 2846, outputs are 0.048235886\n",
      "epoch: 1, step: 2847, outputs are 0.04989802\n",
      "epoch: 1, step: 2848, outputs are 0.049933936\n",
      "epoch: 1, step: 2849, outputs are 0.05003316\n",
      "epoch: 1, step: 2850, outputs are 0.047898803\n",
      "epoch: 1, step: 2851, outputs are 0.0527657\n",
      "epoch: 1, step: 2852, outputs are 0.04901391\n",
      "epoch: 1, step: 2853, outputs are 0.04992603\n",
      "epoch: 1, step: 2854, outputs are 0.049329713\n",
      "epoch: 1, step: 2855, outputs are 0.054366387\n",
      "epoch: 1, step: 2856, outputs are 0.049271755\n",
      "epoch: 1, step: 2857, outputs are 0.057131767\n",
      "epoch: 1, step: 2858, outputs are 0.050774045\n",
      "epoch: 1, step: 2859, outputs are 0.046987936\n",
      "epoch: 1, step: 2860, outputs are 0.045523163\n",
      "epoch: 1, step: 2861, outputs are 0.051797114\n",
      "epoch: 1, step: 2862, outputs are 0.052150644\n",
      "epoch: 1, step: 2863, outputs are 0.052542232\n",
      "epoch: 1, step: 2864, outputs are 0.049287207\n",
      "epoch: 1, step: 2865, outputs are 0.05158368\n",
      "epoch: 1, step: 2866, outputs are 0.049887102\n",
      "epoch: 1, step: 2867, outputs are 0.04781784\n",
      "epoch: 1, step: 2868, outputs are 0.04823669\n",
      "epoch: 1, step: 2869, outputs are 0.0508368\n",
      "epoch: 1, step: 2870, outputs are 0.046997562\n",
      "epoch: 1, step: 2871, outputs are 0.051036697\n",
      "epoch: 1, step: 2872, outputs are 0.04564141\n",
      "epoch: 1, step: 2873, outputs are 0.048504733\n",
      "epoch: 1, step: 2874, outputs are 0.047848478\n",
      "epoch: 1, step: 2875, outputs are 0.04830004\n",
      "epoch: 1, step: 2876, outputs are 0.05517172\n",
      "epoch: 1, step: 2877, outputs are 0.050485767\n",
      "epoch: 1, step: 2878, outputs are 0.046455543\n",
      "epoch: 1, step: 2879, outputs are 0.04684721\n",
      "epoch: 1, step: 2880, outputs are 0.05059622\n",
      "epoch: 1, step: 2881, outputs are 0.04838027\n",
      "epoch: 1, step: 2882, outputs are 0.048164833\n",
      "epoch: 1, step: 2883, outputs are 0.052330878\n",
      "epoch: 1, step: 2884, outputs are 0.04679138\n",
      "epoch: 1, step: 2885, outputs are 0.049181674\n",
      "epoch: 1, step: 2886, outputs are 0.046197787\n",
      "epoch: 1, step: 2887, outputs are 0.04925202\n",
      "epoch: 1, step: 2888, outputs are 0.04616853\n",
      "epoch: 1, step: 2889, outputs are 0.04851117\n",
      "epoch: 1, step: 2890, outputs are 0.04721598\n",
      "epoch: 1, step: 2891, outputs are 0.047367647\n",
      "epoch: 1, step: 2892, outputs are 0.051138908\n",
      "epoch: 1, step: 2893, outputs are 0.048261248\n",
      "epoch: 1, step: 2894, outputs are 0.044787366\n",
      "epoch: 1, step: 2895, outputs are 0.051474385\n",
      "epoch: 1, step: 2896, outputs are 0.046124153\n",
      "epoch: 1, step: 2897, outputs are 0.046191104\n",
      "epoch: 1, step: 2898, outputs are 0.047755573\n",
      "epoch: 1, step: 2899, outputs are 0.047687985\n",
      "epoch: 1, step: 2900, outputs are 0.054823805\n",
      "epoch: 1, step: 2901, outputs are 0.050418723\n",
      "epoch: 1, step: 2902, outputs are 0.043895762\n",
      "epoch: 1, step: 2903, outputs are 0.05100188\n",
      "epoch: 1, step: 2904, outputs are 0.05169449\n",
      "epoch: 1, step: 2905, outputs are 0.055765726\n",
      "epoch: 1, step: 2906, outputs are 0.046184905\n",
      "epoch: 1, step: 2907, outputs are 0.050608024\n",
      "epoch: 1, step: 2908, outputs are 0.04518029\n",
      "epoch: 1, step: 2909, outputs are 0.0519212\n",
      "epoch: 1, step: 2910, outputs are 0.049784962\n",
      "epoch: 1, step: 2911, outputs are 0.047210064\n",
      "epoch: 1, step: 2912, outputs are 0.046913028\n",
      "epoch: 1, step: 2913, outputs are 0.05220665\n",
      "epoch: 1, step: 2914, outputs are 0.045023665\n",
      "epoch: 1, step: 2915, outputs are 0.04941479\n",
      "epoch: 1, step: 2916, outputs are 0.05098348\n",
      "epoch: 1, step: 2917, outputs are 0.04913032\n",
      "epoch: 1, step: 2918, outputs are 0.045354273\n",
      "epoch: 1, step: 2919, outputs are 0.04600889\n",
      "epoch: 1, step: 2920, outputs are 0.052755676\n",
      "epoch: 1, step: 2921, outputs are 0.049902305\n",
      "epoch: 1, step: 2922, outputs are 0.046671893\n",
      "epoch: 1, step: 2923, outputs are 0.04828284\n",
      "epoch: 1, step: 2924, outputs are 0.047023665\n",
      "epoch: 1, step: 2925, outputs are 0.05594479\n",
      "epoch: 1, step: 2926, outputs are 0.047874533\n",
      "epoch: 1, step: 2927, outputs are 0.0479124\n",
      "epoch: 1, step: 2928, outputs are 0.0465433\n",
      "epoch: 1, step: 2929, outputs are 0.051929403\n",
      "epoch: 1, step: 2930, outputs are 0.050430827\n",
      "epoch: 1, step: 2931, outputs are 0.047653314\n",
      "epoch: 1, step: 2932, outputs are 0.051003113\n",
      "epoch: 1, step: 2933, outputs are 0.049209602\n",
      "epoch: 1, step: 2934, outputs are 0.04849352\n",
      "epoch: 1, step: 2935, outputs are 0.051258035\n",
      "epoch: 1, step: 2936, outputs are 0.04927156\n",
      "epoch: 1, step: 2937, outputs are 0.049853273\n",
      "epoch: 1, step: 2938, outputs are 0.052526385\n",
      "epoch: 1, step: 2939, outputs are 0.05205448\n",
      "epoch: 1, step: 2940, outputs are 0.048555125\n",
      "epoch: 1, step: 2941, outputs are 0.046752673\n",
      "epoch: 1, step: 2942, outputs are 0.048902523\n",
      "epoch: 1, step: 2943, outputs are 0.051073864\n",
      "epoch: 1, step: 2944, outputs are 0.046532508\n",
      "epoch: 1, step: 2945, outputs are 0.049528275\n",
      "epoch: 1, step: 2946, outputs are 0.05179385\n",
      "epoch: 1, step: 2947, outputs are 0.04567711\n",
      "epoch: 1, step: 2948, outputs are 0.058101572\n",
      "epoch: 1, step: 2949, outputs are 0.04930974\n",
      "epoch: 1, step: 2950, outputs are 0.050607827\n",
      "epoch: 1, step: 2951, outputs are 0.046945862\n",
      "epoch: 1, step: 2952, outputs are 0.047027208\n",
      "epoch: 1, step: 2953, outputs are 0.043835323\n",
      "epoch: 1, step: 2954, outputs are 0.04981055\n",
      "epoch: 1, step: 2955, outputs are 0.047247835\n",
      "epoch: 1, step: 2956, outputs are 0.05079813\n",
      "epoch: 1, step: 2957, outputs are 0.050311558\n",
      "epoch: 1, step: 2958, outputs are 0.051797103\n",
      "epoch: 1, step: 2959, outputs are 0.047294557\n",
      "epoch: 1, step: 2960, outputs are 0.04665925\n",
      "epoch: 1, step: 2961, outputs are 0.0478646\n",
      "epoch: 1, step: 2962, outputs are 0.05005971\n",
      "epoch: 1, step: 2963, outputs are 0.04724396\n",
      "epoch: 1, step: 2964, outputs are 0.047822326\n",
      "epoch: 1, step: 2965, outputs are 0.049379326\n",
      "epoch: 1, step: 2966, outputs are 0.05144845\n",
      "epoch: 1, step: 2967, outputs are 0.046311833\n",
      "epoch: 1, step: 2968, outputs are 0.050643694\n",
      "epoch: 1, step: 2969, outputs are 0.045723222\n",
      "epoch: 1, step: 2970, outputs are 0.04787844\n",
      "epoch: 1, step: 2971, outputs are 0.047369696\n",
      "epoch: 1, step: 2972, outputs are 0.050225977\n",
      "epoch: 1, step: 2973, outputs are 0.049617305\n",
      "epoch: 1, step: 2974, outputs are 0.047554564\n",
      "epoch: 1, step: 2975, outputs are 0.047396414\n",
      "epoch: 1, step: 2976, outputs are 0.045859892\n",
      "epoch: 1, step: 2977, outputs are 0.048012953\n",
      "epoch: 1, step: 2978, outputs are 0.049037024\n",
      "epoch: 1, step: 2979, outputs are 0.051520042\n",
      "epoch: 1, step: 2980, outputs are 0.048584875\n",
      "epoch: 1, step: 2981, outputs are 0.047709387\n",
      "epoch: 1, step: 2982, outputs are 0.049346317\n",
      "epoch: 1, step: 2983, outputs are 0.05149343\n",
      "epoch: 1, step: 2984, outputs are 0.04454929\n",
      "epoch: 1, step: 2985, outputs are 0.044083297\n",
      "epoch: 1, step: 2986, outputs are 0.048564285\n",
      "epoch: 1, step: 2987, outputs are 0.047021717\n",
      "epoch: 1, step: 2988, outputs are 0.04720068\n",
      "epoch: 1, step: 2989, outputs are 0.046708353\n",
      "epoch: 1, step: 2990, outputs are 0.051054467\n",
      "epoch: 1, step: 2991, outputs are 0.046850976\n",
      "epoch: 1, step: 2992, outputs are 0.047078706\n",
      "epoch: 1, step: 2993, outputs are 0.05119386\n",
      "epoch: 1, step: 2994, outputs are 0.046692744\n",
      "epoch: 1, step: 2995, outputs are 0.045956466\n",
      "epoch: 1, step: 2996, outputs are 0.04932133\n",
      "epoch: 1, step: 2997, outputs are 0.046844125\n",
      "epoch: 1, step: 2998, outputs are 0.04606615\n",
      "epoch: 1, step: 2999, outputs are 0.05010457\n",
      "epoch: 1, step: 3000, outputs are 0.048300687\n",
      "epoch: 1, step: 3001, outputs are 0.049045566\n",
      "epoch: 1, step: 3002, outputs are 0.051533736\n",
      "epoch: 1, step: 3003, outputs are 0.050987657\n",
      "epoch: 1, step: 3004, outputs are 0.0491834\n",
      "epoch: 1, step: 3005, outputs are 0.04422663\n",
      "epoch: 1, step: 3006, outputs are 0.049895473\n",
      "epoch: 1, step: 3007, outputs are 0.045262232\n",
      "epoch: 1, step: 3008, outputs are 0.049427748\n",
      "epoch: 1, step: 3009, outputs are 0.049621753\n",
      "epoch: 1, step: 3010, outputs are 0.04590558\n",
      "epoch: 1, step: 3011, outputs are 0.04680563\n",
      "epoch: 1, step: 3012, outputs are 0.05393198\n",
      "epoch: 1, step: 3013, outputs are 0.04449436\n",
      "epoch: 1, step: 3014, outputs are 0.050288323\n",
      "epoch: 1, step: 3015, outputs are 0.05293992\n",
      "epoch: 1, step: 3016, outputs are 0.04708089\n",
      "epoch: 1, step: 3017, outputs are 0.046945825\n",
      "epoch: 1, step: 3018, outputs are 0.047840476\n",
      "epoch: 1, step: 3019, outputs are 0.048736133\n",
      "epoch: 1, step: 3020, outputs are 0.048795514\n",
      "epoch: 1, step: 3021, outputs are 0.050618403\n",
      "epoch: 1, step: 3022, outputs are 0.05028846\n",
      "epoch: 1, step: 3023, outputs are 0.053872235\n",
      "epoch: 1, step: 3024, outputs are 0.046485495\n",
      "epoch: 1, step: 3025, outputs are 0.046144005\n",
      "epoch: 1, step: 3026, outputs are 0.0523365\n",
      "epoch: 1, step: 3027, outputs are 0.048089884\n",
      "epoch: 1, step: 3028, outputs are 0.04753893\n",
      "epoch: 1, step: 3029, outputs are 0.049190123\n",
      "epoch: 1, step: 3030, outputs are 0.047683217\n",
      "epoch: 1, step: 3031, outputs are 0.048171148\n",
      "epoch: 1, step: 3032, outputs are 0.04722685\n",
      "epoch: 1, step: 3033, outputs are 0.047431964\n",
      "epoch: 1, step: 3034, outputs are 0.048255503\n",
      "epoch: 1, step: 3035, outputs are 0.048140146\n",
      "epoch: 1, step: 3036, outputs are 0.046712402\n",
      "epoch: 1, step: 3037, outputs are 0.054165706\n",
      "epoch: 1, step: 3038, outputs are 0.05354643\n",
      "epoch: 1, step: 3039, outputs are 0.048106518\n",
      "epoch: 1, step: 3040, outputs are 0.04903814\n",
      "epoch: 1, step: 3041, outputs are 0.049307313\n",
      "epoch: 1, step: 3042, outputs are 0.046501324\n",
      "epoch: 1, step: 3043, outputs are 0.04712078\n",
      "epoch: 1, step: 3044, outputs are 0.048910826\n",
      "epoch: 1, step: 3045, outputs are 0.04492212\n",
      "epoch: 1, step: 3046, outputs are 0.04953669\n",
      "epoch: 1, step: 3047, outputs are 0.049526393\n",
      "epoch: 1, step: 3048, outputs are 0.04866454\n",
      "epoch: 1, step: 3049, outputs are 0.048256766\n",
      "epoch: 1, step: 3050, outputs are 0.0452463\n",
      "epoch: 1, step: 3051, outputs are 0.048524167\n",
      "epoch: 1, step: 3052, outputs are 0.04750941\n",
      "epoch: 1, step: 3053, outputs are 0.047207657\n",
      "epoch: 1, step: 3054, outputs are 0.054316215\n",
      "epoch: 1, step: 3055, outputs are 0.04799768\n",
      "epoch: 1, step: 3056, outputs are 0.054615535\n",
      "epoch: 1, step: 3057, outputs are 0.04710694\n",
      "epoch: 1, step: 3058, outputs are 0.047679417\n",
      "epoch: 1, step: 3059, outputs are 0.05085109\n",
      "epoch: 1, step: 3060, outputs are 0.05032231\n",
      "epoch: 1, step: 3061, outputs are 0.047958527\n",
      "epoch: 1, step: 3062, outputs are 0.045317207\n",
      "epoch: 1, step: 3063, outputs are 0.04829746\n",
      "epoch: 1, step: 3064, outputs are 0.046311222\n",
      "epoch: 1, step: 3065, outputs are 0.046632722\n",
      "epoch: 1, step: 3066, outputs are 0.050459825\n",
      "epoch: 1, step: 3067, outputs are 0.04815842\n",
      "epoch: 1, step: 3068, outputs are 0.05139236\n",
      "epoch: 1, step: 3069, outputs are 0.046526514\n",
      "epoch: 1, step: 3070, outputs are 0.04867027\n",
      "epoch: 1, step: 3071, outputs are 0.045389857\n",
      "epoch: 1, step: 3072, outputs are 0.051381875\n",
      "epoch: 1, step: 3073, outputs are 0.050416887\n",
      "epoch: 1, step: 3074, outputs are 0.04901662\n",
      "epoch: 1, step: 3075, outputs are 0.050502017\n",
      "epoch: 1, step: 3076, outputs are 0.048301786\n",
      "epoch: 1, step: 3077, outputs are 0.049218725\n",
      "epoch: 1, step: 3078, outputs are 0.048845463\n",
      "epoch: 1, step: 3079, outputs are 0.051562235\n",
      "epoch: 1, step: 3080, outputs are 0.04655127\n",
      "epoch: 1, step: 3081, outputs are 0.0466057\n",
      "epoch: 1, step: 3082, outputs are 0.04716809\n",
      "epoch: 1, step: 3083, outputs are 0.045413196\n",
      "epoch: 1, step: 3084, outputs are 0.05102372\n",
      "epoch: 1, step: 3085, outputs are 0.04977785\n",
      "epoch: 1, step: 3086, outputs are 0.051368687\n",
      "epoch: 1, step: 3087, outputs are 0.04992019\n",
      "epoch: 1, step: 3088, outputs are 0.048764233\n",
      "epoch: 1, step: 3089, outputs are 0.05438562\n",
      "epoch: 1, step: 3090, outputs are 0.0477925\n",
      "epoch: 1, step: 3091, outputs are 0.047622476\n",
      "epoch: 1, step: 3092, outputs are 0.04639642\n",
      "epoch: 1, step: 3093, outputs are 0.048896834\n",
      "epoch: 1, step: 3094, outputs are 0.046528492\n",
      "epoch: 1, step: 3095, outputs are 0.05160527\n",
      "epoch: 1, step: 3096, outputs are 0.0480949\n",
      "epoch: 1, step: 3097, outputs are 0.04760932\n",
      "epoch: 1, step: 3098, outputs are 0.053565547\n",
      "epoch: 1, step: 3099, outputs are 0.053356916\n",
      "epoch: 1, step: 3100, outputs are 0.04759295\n",
      "epoch: 1, step: 3101, outputs are 0.046516024\n",
      "epoch: 1, step: 3102, outputs are 0.04622335\n",
      "epoch: 1, step: 3103, outputs are 0.050950907\n",
      "epoch: 1, step: 3104, outputs are 0.04538636\n",
      "epoch: 1, step: 3105, outputs are 0.042802844\n",
      "epoch: 1, step: 3106, outputs are 0.05019763\n",
      "epoch: 1, step: 3107, outputs are 0.04919382\n",
      "epoch: 1, step: 3108, outputs are 0.053379178\n",
      "epoch: 1, step: 3109, outputs are 0.05127931\n",
      "epoch: 1, step: 3110, outputs are 0.052087374\n",
      "epoch: 1, step: 3111, outputs are 0.045750692\n",
      "epoch: 1, step: 3112, outputs are 0.045475468\n",
      "epoch: 1, step: 3113, outputs are 0.048353467\n",
      "epoch: 1, step: 3114, outputs are 0.043255165\n",
      "epoch: 1, step: 3115, outputs are 0.04989523\n",
      "epoch: 1, step: 3116, outputs are 0.050076086\n",
      "epoch: 1, step: 3117, outputs are 0.047679953\n",
      "epoch: 1, step: 3118, outputs are 0.050378904\n",
      "epoch: 1, step: 3119, outputs are 0.048080213\n",
      "epoch: 1, step: 3120, outputs are 0.047298793\n",
      "epoch: 1, step: 3121, outputs are 0.04921551\n",
      "epoch: 1, step: 3122, outputs are 0.04697065\n",
      "epoch: 1, step: 3123, outputs are 0.04610367\n",
      "epoch: 1, step: 3124, outputs are 0.049653806\n",
      "epoch: 1, step: 3125, outputs are 0.053644955\n",
      "epoch: 1, step: 3126, outputs are 0.052587952\n",
      "epoch: 1, step: 3127, outputs are 0.04384605\n",
      "epoch: 1, step: 3128, outputs are 0.04638332\n",
      "epoch: 1, step: 3129, outputs are 0.04522596\n",
      "epoch: 1, step: 3130, outputs are 0.050133906\n",
      "epoch: 1, step: 3131, outputs are 0.0482713\n",
      "epoch: 1, step: 3132, outputs are 0.048211455\n",
      "epoch: 1, step: 3133, outputs are 0.04550851\n",
      "epoch: 1, step: 3134, outputs are 0.05245243\n",
      "epoch: 1, step: 3135, outputs are 0.04656802\n",
      "epoch: 1, step: 3136, outputs are 0.04972142\n",
      "epoch: 1, step: 3137, outputs are 0.054419465\n",
      "epoch: 1, step: 3138, outputs are 0.048591726\n",
      "epoch: 1, step: 3139, outputs are 0.050016746\n",
      "epoch: 1, step: 3140, outputs are 0.04563474\n",
      "epoch: 1, step: 3141, outputs are 0.050105717\n",
      "epoch: 1, step: 3142, outputs are 0.04562736\n",
      "epoch: 1, step: 3143, outputs are 0.04680769\n",
      "epoch: 1, step: 3144, outputs are 0.049629487\n",
      "epoch: 1, step: 3145, outputs are 0.04873748\n",
      "epoch: 1, step: 3146, outputs are 0.049921695\n",
      "epoch: 1, step: 3147, outputs are 0.04772179\n",
      "epoch: 1, step: 3148, outputs are 0.047568075\n",
      "epoch: 1, step: 3149, outputs are 0.055748887\n",
      "epoch: 1, step: 3150, outputs are 0.048713107\n",
      "epoch: 1, step: 3151, outputs are 0.054045454\n",
      "epoch: 1, step: 3152, outputs are 0.05050512\n",
      "epoch: 1, step: 3153, outputs are 0.04781427\n",
      "epoch: 1, step: 3154, outputs are 0.049400397\n",
      "epoch: 1, step: 3155, outputs are 0.044010155\n",
      "epoch: 1, step: 3156, outputs are 0.04782971\n",
      "epoch: 1, step: 3157, outputs are 0.050548956\n",
      "epoch: 1, step: 3158, outputs are 0.046399813\n",
      "epoch: 1, step: 3159, outputs are 0.048421197\n",
      "epoch: 1, step: 3160, outputs are 0.048445288\n",
      "epoch: 1, step: 3161, outputs are 0.05058427\n",
      "epoch: 1, step: 3162, outputs are 0.043276913\n",
      "epoch: 1, step: 3163, outputs are 0.04791823\n",
      "epoch: 1, step: 3164, outputs are 0.04561992\n",
      "epoch: 1, step: 3165, outputs are 0.047410443\n",
      "epoch: 1, step: 3166, outputs are 0.05063493\n",
      "epoch: 1, step: 3167, outputs are 0.048168063\n",
      "epoch: 1, step: 3168, outputs are 0.046002187\n",
      "epoch: 1, step: 3169, outputs are 0.053569697\n",
      "epoch: 1, step: 3170, outputs are 0.043822743\n",
      "epoch: 1, step: 3171, outputs are 0.050940488\n",
      "epoch: 1, step: 3172, outputs are 0.048769753\n",
      "epoch: 1, step: 3173, outputs are 0.046802565\n",
      "epoch: 1, step: 3174, outputs are 0.05013365\n",
      "epoch: 1, step: 3175, outputs are 0.049302485\n",
      "epoch: 1, step: 3176, outputs are 0.044376485\n",
      "epoch: 1, step: 3177, outputs are 0.05053049\n",
      "epoch: 1, step: 3178, outputs are 0.05188679\n",
      "epoch: 1, step: 3179, outputs are 0.04702596\n",
      "epoch: 1, step: 3180, outputs are 0.049362138\n",
      "epoch: 1, step: 3181, outputs are 0.051331524\n",
      "epoch: 1, step: 3182, outputs are 0.053356424\n",
      "epoch: 1, step: 3183, outputs are 0.052953225\n",
      "epoch: 1, step: 3184, outputs are 0.051935103\n",
      "epoch: 1, step: 3185, outputs are 0.046687223\n",
      "epoch: 1, step: 3186, outputs are 0.047444664\n",
      "epoch: 1, step: 3187, outputs are 0.047594316\n",
      "epoch: 1, step: 3188, outputs are 0.049635977\n",
      "epoch: 1, step: 3189, outputs are 0.04934362\n",
      "epoch: 1, step: 3190, outputs are 0.04762596\n",
      "epoch: 1, step: 3191, outputs are 0.047879107\n",
      "epoch: 1, step: 3192, outputs are 0.048707835\n",
      "epoch: 1, step: 3193, outputs are 0.050419472\n",
      "epoch: 1, step: 3194, outputs are 0.047824595\n",
      "epoch: 1, step: 3195, outputs are 0.053104136\n",
      "epoch: 1, step: 3196, outputs are 0.04555466\n",
      "epoch: 1, step: 3197, outputs are 0.050189115\n",
      "epoch: 1, step: 3198, outputs are 0.054295704\n",
      "epoch: 1, step: 3199, outputs are 0.04570671\n",
      "epoch: 1, step: 3200, outputs are 0.04960194\n",
      "epoch: 1, step: 3201, outputs are 0.043900132\n",
      "epoch: 1, step: 3202, outputs are 0.052500278\n",
      "epoch: 1, step: 3203, outputs are 0.050091464\n",
      "epoch: 1, step: 3204, outputs are 0.052619483\n",
      "epoch: 1, step: 3205, outputs are 0.04547149\n",
      "epoch: 1, step: 3206, outputs are 0.04473667\n",
      "epoch: 1, step: 3207, outputs are 0.044919267\n",
      "epoch: 1, step: 3208, outputs are 0.046987955\n",
      "epoch: 1, step: 3209, outputs are 0.04630203\n",
      "epoch: 1, step: 3210, outputs are 0.047267467\n",
      "epoch: 1, step: 3211, outputs are 0.0482151\n",
      "epoch: 1, step: 3212, outputs are 0.049110726\n",
      "epoch: 1, step: 3213, outputs are 0.04642869\n",
      "epoch: 1, step: 3214, outputs are 0.04672895\n",
      "epoch: 1, step: 3215, outputs are 0.044784006\n",
      "epoch: 1, step: 3216, outputs are 0.05093494\n",
      "epoch: 1, step: 3217, outputs are 0.0465879\n",
      "epoch: 1, step: 3218, outputs are 0.047739767\n",
      "epoch: 1, step: 3219, outputs are 0.046079606\n",
      "epoch: 1, step: 3220, outputs are 0.049765147\n",
      "epoch: 1, step: 3221, outputs are 0.051271804\n",
      "epoch: 1, step: 3222, outputs are 0.049129568\n",
      "epoch: 1, step: 3223, outputs are 0.047071904\n",
      "epoch: 1, step: 3224, outputs are 0.04927852\n",
      "epoch: 1, step: 3225, outputs are 0.049734566\n",
      "epoch: 1, step: 3226, outputs are 0.049453408\n",
      "epoch: 1, step: 3227, outputs are 0.051520646\n",
      "epoch: 1, step: 3228, outputs are 0.0540767\n",
      "epoch: 1, step: 3229, outputs are 0.04254555\n",
      "epoch: 1, step: 3230, outputs are 0.04558023\n",
      "epoch: 1, step: 3231, outputs are 0.046246894\n",
      "epoch: 1, step: 3232, outputs are 0.048521608\n",
      "epoch: 1, step: 3233, outputs are 0.04692967\n",
      "epoch: 1, step: 3234, outputs are 0.04982079\n",
      "epoch: 1, step: 3235, outputs are 0.041500293\n",
      "epoch: 1, step: 3236, outputs are 0.048337426\n",
      "epoch: 1, step: 3237, outputs are 0.049709924\n",
      "epoch: 1, step: 3238, outputs are 0.049789146\n",
      "epoch: 1, step: 3239, outputs are 0.047649525\n",
      "epoch: 1, step: 3240, outputs are 0.048747312\n",
      "epoch: 1, step: 3241, outputs are 0.046595287\n",
      "epoch: 1, step: 3242, outputs are 0.04381246\n",
      "epoch: 1, step: 3243, outputs are 0.0506624\n",
      "epoch: 1, step: 3244, outputs are 0.045076225\n",
      "epoch: 1, step: 3245, outputs are 0.046361297\n",
      "epoch: 1, step: 3246, outputs are 0.049701214\n",
      "epoch: 1, step: 3247, outputs are 0.049541935\n",
      "epoch: 1, step: 3248, outputs are 0.04803554\n",
      "epoch: 1, step: 3249, outputs are 0.052413125\n",
      "epoch: 1, step: 3250, outputs are 0.049335323\n",
      "epoch: 1, step: 3251, outputs are 0.05234255\n",
      "epoch: 1, step: 3252, outputs are 0.045745827\n",
      "epoch: 1, step: 3253, outputs are 0.047094844\n",
      "epoch: 1, step: 3254, outputs are 0.047915865\n",
      "epoch: 1, step: 3255, outputs are 0.04698176\n",
      "epoch: 1, step: 3256, outputs are 0.050145935\n",
      "epoch: 1, step: 3257, outputs are 0.04692732\n",
      "epoch: 1, step: 3258, outputs are 0.046025664\n",
      "epoch: 1, step: 3259, outputs are 0.047020134\n",
      "epoch: 1, step: 3260, outputs are 0.050442934\n",
      "epoch: 1, step: 3261, outputs are 0.048980832\n",
      "epoch: 1, step: 3262, outputs are 0.04625156\n",
      "epoch: 1, step: 3263, outputs are 0.05091584\n",
      "epoch: 1, step: 3264, outputs are 0.05016399\n",
      "epoch: 1, step: 3265, outputs are 0.04623032\n",
      "epoch: 1, step: 3266, outputs are 0.047487143\n",
      "epoch: 1, step: 3267, outputs are 0.048377756\n",
      "epoch: 1, step: 3268, outputs are 0.04686555\n",
      "epoch: 1, step: 3269, outputs are 0.046304986\n",
      "epoch: 1, step: 3270, outputs are 0.042293873\n",
      "epoch: 1, step: 3271, outputs are 0.05138867\n",
      "epoch: 1, step: 3272, outputs are 0.04889993\n",
      "epoch: 1, step: 3273, outputs are 0.043560192\n",
      "epoch: 1, step: 3274, outputs are 0.05451462\n",
      "epoch: 1, step: 3275, outputs are 0.04753709\n",
      "epoch: 1, step: 3276, outputs are 0.04775982\n",
      "epoch: 1, step: 3277, outputs are 0.054838903\n",
      "epoch: 1, step: 3278, outputs are 0.055363137\n",
      "epoch: 1, step: 3279, outputs are 0.04447026\n",
      "epoch: 1, step: 3280, outputs are 0.050471336\n",
      "epoch: 1, step: 3281, outputs are 0.048104003\n",
      "epoch: 1, step: 3282, outputs are 0.047709133\n",
      "epoch: 1, step: 3283, outputs are 0.046391673\n",
      "epoch: 1, step: 3284, outputs are 0.04967456\n",
      "epoch: 1, step: 3285, outputs are 0.045200374\n",
      "epoch: 1, step: 3286, outputs are 0.05032289\n",
      "epoch: 1, step: 3287, outputs are 0.05199162\n",
      "epoch: 1, step: 3288, outputs are 0.053653605\n",
      "epoch: 1, step: 3289, outputs are 0.04826714\n",
      "epoch: 1, step: 3290, outputs are 0.045758277\n",
      "epoch: 1, step: 3291, outputs are 0.045715827\n",
      "epoch: 1, step: 3292, outputs are 0.050665338\n",
      "epoch: 1, step: 3293, outputs are 0.048596866\n",
      "epoch: 1, step: 3294, outputs are 0.051267814\n",
      "epoch: 1, step: 3295, outputs are 0.047057778\n",
      "epoch: 1, step: 3296, outputs are 0.055490583\n",
      "epoch: 1, step: 3297, outputs are 0.045768715\n",
      "epoch: 1, step: 3298, outputs are 0.052313566\n",
      "epoch: 1, step: 3299, outputs are 0.046308022\n",
      "epoch: 1, step: 3300, outputs are 0.045160547\n",
      "epoch: 1, step: 3301, outputs are 0.045699213\n",
      "epoch: 1, step: 3302, outputs are 0.047874503\n",
      "epoch: 1, step: 3303, outputs are 0.04715361\n",
      "epoch: 1, step: 3304, outputs are 0.04594545\n",
      "epoch: 1, step: 3305, outputs are 0.048565283\n",
      "epoch: 1, step: 3306, outputs are 0.048774336\n",
      "epoch: 1, step: 3307, outputs are 0.045794927\n",
      "epoch: 1, step: 3308, outputs are 0.05222462\n",
      "epoch: 1, step: 3309, outputs are 0.046371583\n",
      "epoch: 1, step: 3310, outputs are 0.05158489\n",
      "epoch: 1, step: 3311, outputs are 0.04388621\n",
      "epoch: 1, step: 3312, outputs are 0.04999047\n",
      "epoch: 1, step: 3313, outputs are 0.04588478\n",
      "epoch: 1, step: 3314, outputs are 0.047153868\n",
      "epoch: 1, step: 3315, outputs are 0.047232866\n",
      "epoch: 1, step: 3316, outputs are 0.047704484\n",
      "epoch: 1, step: 3317, outputs are 0.047544498\n",
      "epoch: 1, step: 3318, outputs are 0.04971851\n",
      "epoch: 1, step: 3319, outputs are 0.050515488\n",
      "epoch: 1, step: 3320, outputs are 0.05528551\n",
      "epoch: 1, step: 3321, outputs are 0.048816662\n",
      "epoch: 1, step: 3322, outputs are 0.04898911\n",
      "epoch: 1, step: 3323, outputs are 0.045688093\n",
      "epoch: 1, step: 3324, outputs are 0.047682546\n",
      "epoch: 1, step: 3325, outputs are 0.049413502\n",
      "epoch: 1, step: 3326, outputs are 0.048345923\n",
      "epoch: 1, step: 3327, outputs are 0.052507102\n",
      "epoch: 1, step: 3328, outputs are 0.050000764\n",
      "epoch: 1, step: 3329, outputs are 0.044412937\n",
      "epoch: 1, step: 3330, outputs are 0.048806585\n",
      "epoch: 1, step: 3331, outputs are 0.04970389\n",
      "epoch: 1, step: 3332, outputs are 0.047889218\n",
      "epoch: 1, step: 3333, outputs are 0.04756932\n",
      "epoch: 1, step: 3334, outputs are 0.04736495\n",
      "epoch: 1, step: 3335, outputs are 0.0476641\n",
      "epoch: 1, step: 3336, outputs are 0.041562535\n",
      "epoch: 1, step: 3337, outputs are 0.050728045\n",
      "epoch: 1, step: 3338, outputs are 0.047956806\n",
      "epoch: 1, step: 3339, outputs are 0.04616613\n",
      "epoch: 1, step: 3340, outputs are 0.049006946\n",
      "epoch: 1, step: 3341, outputs are 0.048018176\n",
      "epoch: 1, step: 3342, outputs are 0.044502728\n",
      "epoch: 1, step: 3343, outputs are 0.04954441\n",
      "epoch: 1, step: 3344, outputs are 0.049336787\n",
      "epoch: 1, step: 3345, outputs are 0.04859472\n",
      "epoch: 1, step: 3346, outputs are 0.046193615\n",
      "epoch: 1, step: 3347, outputs are 0.0489649\n",
      "epoch: 1, step: 3348, outputs are 0.047674943\n",
      "epoch: 1, step: 3349, outputs are 0.046928775\n",
      "epoch: 1, step: 3350, outputs are 0.05161165\n",
      "epoch: 1, step: 3351, outputs are 0.05135305\n",
      "epoch: 1, step: 3352, outputs are 0.052019507\n",
      "epoch: 1, step: 3353, outputs are 0.048000515\n",
      "epoch: 1, step: 3354, outputs are 0.051234223\n",
      "epoch: 1, step: 3355, outputs are 0.05022989\n",
      "epoch: 1, step: 3356, outputs are 0.047460612\n",
      "epoch: 1, step: 3357, outputs are 0.04698512\n",
      "epoch: 1, step: 3358, outputs are 0.046413444\n",
      "epoch: 1, step: 3359, outputs are 0.048124965\n",
      "epoch: 1, step: 3360, outputs are 0.047136936\n",
      "epoch: 1, step: 3361, outputs are 0.043848217\n",
      "epoch: 1, step: 3362, outputs are 0.045350567\n",
      "epoch: 1, step: 3363, outputs are 0.047965866\n",
      "epoch: 1, step: 3364, outputs are 0.048373826\n",
      "epoch: 1, step: 3365, outputs are 0.046461593\n",
      "epoch: 1, step: 3366, outputs are 0.042804614\n",
      "epoch: 1, step: 3367, outputs are 0.047174163\n",
      "epoch: 1, step: 3368, outputs are 0.04549966\n",
      "epoch: 1, step: 3369, outputs are 0.045138843\n",
      "epoch: 1, step: 3370, outputs are 0.04858072\n",
      "epoch: 1, step: 3371, outputs are 0.051396698\n",
      "epoch: 1, step: 3372, outputs are 0.048830505\n",
      "epoch: 1, step: 3373, outputs are 0.04616496\n",
      "epoch: 1, step: 3374, outputs are 0.044229575\n",
      "epoch: 1, step: 3375, outputs are 0.057977956\n",
      "epoch: 1, step: 3376, outputs are 0.04485655\n",
      "epoch: 1, step: 3377, outputs are 0.04652597\n",
      "epoch: 1, step: 3378, outputs are 0.05200021\n",
      "epoch: 1, step: 3379, outputs are 0.047834024\n",
      "epoch: 1, step: 3380, outputs are 0.05197886\n",
      "epoch: 1, step: 3381, outputs are 0.046344884\n",
      "epoch: 1, step: 3382, outputs are 0.04755564\n",
      "epoch: 1, step: 3383, outputs are 0.045605965\n",
      "epoch: 1, step: 3384, outputs are 0.049542807\n",
      "epoch: 1, step: 3385, outputs are 0.051032335\n",
      "epoch: 1, step: 3386, outputs are 0.04606338\n",
      "epoch: 1, step: 3387, outputs are 0.051422417\n",
      "epoch: 1, step: 3388, outputs are 0.050694678\n",
      "epoch: 1, step: 3389, outputs are 0.04400636\n",
      "epoch: 1, step: 3390, outputs are 0.049675576\n",
      "epoch: 1, step: 3391, outputs are 0.04922445\n",
      "epoch: 1, step: 3392, outputs are 0.0507553\n",
      "epoch: 1, step: 3393, outputs are 0.049602356\n",
      "epoch: 1, step: 3394, outputs are 0.05115827\n",
      "epoch: 1, step: 3395, outputs are 0.047348235\n",
      "epoch: 1, step: 3396, outputs are 0.049962856\n",
      "epoch: 1, step: 3397, outputs are 0.047312066\n",
      "epoch: 1, step: 3398, outputs are 0.04559315\n",
      "epoch: 1, step: 3399, outputs are 0.048880283\n",
      "epoch: 1, step: 3400, outputs are 0.05370085\n",
      "epoch: 1, step: 3401, outputs are 0.044739645\n",
      "epoch: 1, step: 3402, outputs are 0.048104603\n",
      "epoch: 1, step: 3403, outputs are 0.046170086\n",
      "epoch: 1, step: 3404, outputs are 0.047546275\n",
      "epoch: 1, step: 3405, outputs are 0.046259597\n",
      "epoch: 1, step: 3406, outputs are 0.04345936\n",
      "epoch: 1, step: 3407, outputs are 0.048302464\n",
      "epoch: 1, step: 3408, outputs are 0.04776731\n",
      "epoch: 1, step: 3409, outputs are 0.04944788\n",
      "epoch: 1, step: 3410, outputs are 0.049506158\n",
      "epoch: 1, step: 3411, outputs are 0.04964672\n",
      "epoch: 1, step: 3412, outputs are 0.046451002\n",
      "epoch: 1, step: 3413, outputs are 0.047886655\n",
      "epoch: 1, step: 3414, outputs are 0.04178259\n",
      "epoch: 1, step: 3415, outputs are 0.048329197\n",
      "epoch: 1, step: 3416, outputs are 0.051011764\n",
      "epoch: 1, step: 3417, outputs are 0.048452828\n",
      "epoch: 1, step: 3418, outputs are 0.050047234\n",
      "epoch: 1, step: 3419, outputs are 0.045004204\n",
      "epoch: 1, step: 3420, outputs are 0.051310357\n",
      "epoch: 1, step: 3421, outputs are 0.04589089\n",
      "epoch: 1, step: 3422, outputs are 0.047579743\n",
      "epoch: 1, step: 3423, outputs are 0.04666048\n",
      "epoch: 1, step: 3424, outputs are 0.0484497\n",
      "epoch: 1, step: 3425, outputs are 0.048817288\n",
      "epoch: 1, step: 3426, outputs are 0.04802666\n",
      "epoch: 1, step: 3427, outputs are 0.047970276\n",
      "epoch: 1, step: 3428, outputs are 0.050245494\n",
      "epoch: 1, step: 3429, outputs are 0.049491394\n",
      "epoch: 1, step: 3430, outputs are 0.045121472\n",
      "epoch: 1, step: 3431, outputs are 0.04506163\n",
      "epoch: 1, step: 3432, outputs are 0.052031223\n",
      "epoch: 1, step: 3433, outputs are 0.049661368\n",
      "epoch: 1, step: 3434, outputs are 0.04864101\n",
      "epoch: 1, step: 3435, outputs are 0.048588328\n",
      "epoch: 1, step: 3436, outputs are 0.04900867\n",
      "epoch: 1, step: 3437, outputs are 0.04719481\n",
      "epoch: 1, step: 3438, outputs are 0.045644976\n",
      "epoch: 1, step: 3439, outputs are 0.05105555\n",
      "epoch: 1, step: 3440, outputs are 0.04977953\n",
      "epoch: 1, step: 3441, outputs are 0.04635554\n",
      "epoch: 1, step: 3442, outputs are 0.051852856\n",
      "epoch: 1, step: 3443, outputs are 0.044597443\n",
      "Train epoch time: 3286680.373 ms, per step time: 954.598 ms\n"
     ]
    }
   ],
   "source": [
    "netwithgrads = BertEvaluationCell(netwithloss, optimizer=optimizer)\n",
    "\n",
    "model = Model(netwithgrads)\n",
    "model.train(repeat_count, dataset, callbacks=callback,\n",
    "            dataset_sink_mode=(args_opt.enable_data_sink == 'true'),\n",
    "            sink_size=args_opt.data_sink_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92d74c8-528e-4b34-927b-87d0e1293cf4",
   "metadata": {},
   "source": [
    "## 二阶段\n",
    "\n",
    " 二阶段我们载入一阶段保存的模型， 调整超参，开始新的训练，其他与一阶段相似。\n",
    " \n",
    " \n",
    " ### 载入最新一阶段模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c156681f-2e1e-4bb4-b02a-bccd8d7fa87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = os.listdir(td_phase1_save_ckpt_dir)\n",
    "if lists:\n",
    "    lists.sort(key=lambda fn: os.path.getmtime(td_phase1_save_ckpt_dir + '/' + fn))\n",
    "    name_ext = os.path.splitext(lists[-1])\n",
    "    assert name_ext[-1] == \".ckpt\", \"Invalid file, checkpoint file should be .ckpt file\"\n",
    "    ckpt_file = os.path.join(td_phase1_save_ckpt_dir, lists[-1])\n",
    "    if ckpt_file == '':\n",
    "        raise ValueError(\"Student ckpt file should not be None\")\n",
    "    cfg = phase2_cfg\n",
    "    \n",
    "    \n",
    "load_teacher_checkpoint_path = args_opt.load_teacher_ckpt_path\n",
    "load_student_checkpoint_path = ckpt_file\n",
    "netwithloss = BertNetworkWithLoss_td(teacher_config=td_teacher_net_cfg, teacher_ckpt=load_teacher_checkpoint_path,\n",
    "                                     student_config=td_student_net_cfg, student_ckpt=load_student_checkpoint_path,\n",
    "                                     is_training=True, task_type=args_opt.task_type,\n",
    "                                     num_labels=args_opt.num_labels, is_predistill=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfc5d18-6580-4a6e-bf8d-84c35e6b7735",
   "metadata": {},
   "source": [
    "### 重新读入数据集\n",
    "\n",
    "在二阶段 我们需要对模型的结果进行验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a66ca05-59be-42f0-9716-92c8d922c92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "td2 train dataset size:  3443\n",
      "td2 train dataset repeatcount:  1\n",
      "td2 eval dataset size:  3443\n"
     ]
    }
   ],
   "source": [
    "rank = 0\n",
    "device_num = 1\n",
    "train_dataset = create_tinybert_dataset('td', cfg.batch_size,\n",
    "                                        device_num, rank, args_opt.do_shuffle,\n",
    "                                        args_opt.train_data_dir, args_opt.schema_dir,\n",
    "                                        data_type=dataset_type)\n",
    "\n",
    "dataset_size = train_dataset.get_dataset_size()\n",
    "print('td2 train dataset size: ', dataset_size)\n",
    "print('td2 train dataset repeatcount: ', train_dataset.get_repeat_count())\n",
    "\n",
    "repeat_count = args_opt.td_phase2_epoch_size\n",
    "\n",
    "time_monitor_steps = dataset_size\n",
    "\n",
    "eval_dataset = create_tinybert_dataset('td', eval_cfg.batch_size,\n",
    "                                       device_num, rank, args_opt.do_shuffle,\n",
    "                                       args_opt.eval_data_dir, args_opt.schema_dir,\n",
    "                                       data_type=dataset_type)\n",
    "print('td2 eval dataset size: ', eval_dataset.get_dataset_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15be077b-9e00-46de-9e77-a9ecd9c8456c",
   "metadata": {},
   "source": [
    "### 二阶段学习率与优化器  \n",
    "\n",
    "注意在callback中传入了测试集。\n",
    "在EvalCallBack 中规定了模型的保存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8791cfc-e122-467f-9b91-7ad15a035b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_cfg = cfg.optimizer_cfg\n",
    "\n",
    "lr_schedule = BertLearningRate(learning_rate=optimizer_cfg.AdamWeightDecay.learning_rate,\n",
    "                               end_learning_rate=optimizer_cfg.AdamWeightDecay.end_learning_rate,\n",
    "                               warmup_steps=int(dataset_size * args_opt.td_phase2_epoch_size / 10),\n",
    "                               decay_steps=int(dataset_size * args_opt.td_phase2_epoch_size),\n",
    "                               power=optimizer_cfg.AdamWeightDecay.power)\n",
    "\n",
    "params = netwithloss.trainable_params()\n",
    "decay_params = list(filter(optimizer_cfg.AdamWeightDecay.decay_filter, params))\n",
    "other_params = list(filter(lambda x: not optimizer_cfg.AdamWeightDecay.decay_filter(x), params))\n",
    "group_params = [{'params': decay_params, 'weight_decay': optimizer_cfg.AdamWeightDecay.weight_decay},\n",
    "                {'params': other_params, 'weight_decay': 0.0},\n",
    "                {'order_params': params}]\n",
    "\n",
    "optimizer = AdamWeightDecay(group_params, learning_rate=lr_schedule, eps=optimizer_cfg.AdamWeightDecay.eps)\n",
    "\n",
    "callback = [TimeMonitor(time_monitor_steps), LossCallBack(),\n",
    "            EvalCallBack(netwithloss.bert, eval_dataset)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f1d749-fc5e-4fe2-a037-74309611a142",
   "metadata": {},
   "source": [
    "### 训练\n",
    "\n",
    "注意 这里训练时， 如果没有屏蔽一阶段的训练，或者一阶段中途停止。 可能会出现报错:C++ Call Stack。 如果出现报错，可以先屏蔽一阶段训练的代码段。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4baca7b-9cdb-481e-aa0f-b3201b9285f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(95050:281472998983552,MainProcess):2022-10-17-20:58:51.904.344 [mindspore/train/model.py:1077] For LossCallBack callback, {'step_end'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.\n",
      "[WARNING] ME(95050:281472998983552,MainProcess):2022-10-17-20:58:51.905.670 [mindspore/train/model.py:1077] For EvalCallBack callback, {'epoch_end'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:58:53.151.122 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op18569] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:58:53.574.510 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op18600] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:58:53.715.676 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op18618] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:58:53.933.311 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op18636] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:58:54.292.249 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op18668] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:58:54.445.901 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op18686] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:58:54.664.989 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op18704] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:58:54.997.247 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op18736] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:58:55.133.578 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op18754] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:58:55.342.498 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op18772] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:58:55.650.445 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op18804] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:58:55.767.764 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op18822] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:58:55.972.548 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op18840] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] OPTIMIZER(95050,ffff8a1e4780,python):2022-10-17-20:58:56.913.981 [mindspore/ccsrc/frontend/optimizer/ad/kpynative.cc:1088] SetOutput] Weight is not used in network, weight: fit_dense.weight\n",
      "[WARNING] OPTIMIZER(95050,ffff8a1e4780,python):2022-10-17-20:58:56.915.517 [mindspore/ccsrc/frontend/optimizer/ad/kpynative.cc:1088] SetOutput] Weight is not used in network, weight: fit_dense.bias\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:58:59.211.856 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/gradStridedSlice/StridedSliceGrad-op19182] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(95050,ffff8a1e4780,python):2022-10-17-20:59:00.396.099 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/gradStridedSlice/StridedSliceGrad-op19413] don't support int64, reduce precision from int64 to int32.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, step: 1, outputs are 0.34506848\n",
      "epoch: 1, step: 2, outputs are 0.34500617\n",
      "epoch: 1, step: 3, outputs are 0.34492475\n",
      "epoch: 1, step: 4, outputs are 0.34461087\n",
      "epoch: 1, step: 5, outputs are 0.3443854\n",
      "epoch: 1, step: 6, outputs are 0.34384793\n",
      "epoch: 1, step: 7, outputs are 0.3433299\n",
      "epoch: 1, step: 8, outputs are 0.34268647\n",
      "epoch: 1, step: 9, outputs are 0.34155688\n",
      "epoch: 1, step: 10, outputs are 0.34076613\n",
      "epoch: 1, step: 11, outputs are 0.33978412\n",
      "epoch: 1, step: 12, outputs are 0.33741572\n",
      "epoch: 1, step: 13, outputs are 0.33684337\n",
      "epoch: 1, step: 14, outputs are 0.33472025\n",
      "epoch: 1, step: 15, outputs are 0.33327097\n",
      "epoch: 1, step: 16, outputs are 0.3312292\n",
      "epoch: 1, step: 17, outputs are 0.32820725\n",
      "epoch: 1, step: 18, outputs are 0.32503647\n",
      "epoch: 1, step: 19, outputs are 0.3230511\n",
      "epoch: 1, step: 20, outputs are 0.3192774\n",
      "epoch: 1, step: 21, outputs are 0.31737298\n",
      "epoch: 1, step: 22, outputs are 0.31302541\n",
      "epoch: 1, step: 23, outputs are 0.31051105\n",
      "epoch: 1, step: 24, outputs are 0.30823928\n",
      "epoch: 1, step: 25, outputs are 0.30264005\n",
      "epoch: 1, step: 26, outputs are 0.3002489\n",
      "epoch: 1, step: 27, outputs are 0.29466522\n",
      "epoch: 1, step: 28, outputs are 0.2924029\n",
      "epoch: 1, step: 29, outputs are 0.28930297\n",
      "epoch: 1, step: 30, outputs are 0.28362644\n",
      "epoch: 1, step: 31, outputs are 0.2817887\n",
      "epoch: 1, step: 32, outputs are 0.27788606\n",
      "epoch: 1, step: 33, outputs are 0.27279228\n",
      "epoch: 1, step: 34, outputs are 0.2679552\n",
      "epoch: 1, step: 35, outputs are 0.26647577\n",
      "epoch: 1, step: 36, outputs are 0.26154166\n",
      "epoch: 1, step: 37, outputs are 0.25769615\n",
      "epoch: 1, step: 38, outputs are 0.25518346\n",
      "epoch: 1, step: 39, outputs are 0.2496221\n",
      "epoch: 1, step: 40, outputs are 0.247174\n",
      "epoch: 1, step: 41, outputs are 0.24255408\n",
      "epoch: 1, step: 42, outputs are 0.23808116\n",
      "epoch: 1, step: 43, outputs are 0.23356798\n",
      "epoch: 1, step: 44, outputs are 0.23184252\n",
      "epoch: 1, step: 45, outputs are 0.2282561\n",
      "epoch: 1, step: 46, outputs are 0.2239392\n",
      "epoch: 1, step: 47, outputs are 0.22093156\n",
      "epoch: 1, step: 48, outputs are 0.21761188\n",
      "epoch: 1, step: 49, outputs are 0.21299484\n",
      "epoch: 1, step: 50, outputs are 0.21193822\n",
      "epoch: 1, step: 51, outputs are 0.20928696\n",
      "epoch: 1, step: 52, outputs are 0.20561394\n",
      "epoch: 1, step: 53, outputs are 0.20169802\n",
      "epoch: 1, step: 54, outputs are 0.19791682\n",
      "epoch: 1, step: 55, outputs are 0.19637284\n",
      "epoch: 1, step: 56, outputs are 0.19294085\n",
      "epoch: 1, step: 57, outputs are 0.18867248\n",
      "epoch: 1, step: 58, outputs are 0.18698913\n",
      "epoch: 1, step: 59, outputs are 0.18417835\n",
      "epoch: 1, step: 60, outputs are 0.18046089\n",
      "epoch: 1, step: 61, outputs are 0.17832679\n",
      "epoch: 1, step: 62, outputs are 0.17643231\n",
      "epoch: 1, step: 63, outputs are 0.17380607\n",
      "epoch: 1, step: 64, outputs are 0.17237942\n",
      "epoch: 1, step: 65, outputs are 0.16840947\n",
      "epoch: 1, step: 66, outputs are 0.16629423\n",
      "epoch: 1, step: 67, outputs are 0.1634772\n",
      "epoch: 1, step: 68, outputs are 0.16049702\n",
      "epoch: 1, step: 69, outputs are 0.159816\n",
      "epoch: 1, step: 70, outputs are 0.1571636\n",
      "epoch: 1, step: 71, outputs are 0.15408307\n",
      "epoch: 1, step: 72, outputs are 0.15172341\n",
      "epoch: 1, step: 73, outputs are 0.1501075\n",
      "epoch: 1, step: 74, outputs are 0.1470856\n",
      "epoch: 1, step: 75, outputs are 0.1456385\n",
      "epoch: 1, step: 76, outputs are 0.1435112\n",
      "epoch: 1, step: 77, outputs are 0.14135629\n",
      "epoch: 1, step: 78, outputs are 0.14140067\n",
      "epoch: 1, step: 79, outputs are 0.13861948\n",
      "epoch: 1, step: 80, outputs are 0.13580687\n",
      "epoch: 1, step: 81, outputs are 0.13304701\n",
      "epoch: 1, step: 82, outputs are 0.13207638\n",
      "epoch: 1, step: 83, outputs are 0.13073918\n",
      "epoch: 1, step: 84, outputs are 0.12928659\n",
      "epoch: 1, step: 85, outputs are 0.12604289\n",
      "epoch: 1, step: 86, outputs are 0.12530774\n",
      "epoch: 1, step: 87, outputs are 0.123081714\n",
      "epoch: 1, step: 88, outputs are 0.12108039\n",
      "epoch: 1, step: 89, outputs are 0.12018826\n",
      "epoch: 1, step: 90, outputs are 0.118944645\n",
      "epoch: 1, step: 91, outputs are 0.11691032\n",
      "epoch: 1, step: 92, outputs are 0.11559066\n",
      "epoch: 1, step: 93, outputs are 0.11460552\n",
      "epoch: 1, step: 94, outputs are 0.11191028\n",
      "epoch: 1, step: 95, outputs are 0.110812485\n",
      "epoch: 1, step: 96, outputs are 0.10992816\n",
      "epoch: 1, step: 97, outputs are 0.108595595\n",
      "epoch: 1, step: 98, outputs are 0.10718077\n",
      "epoch: 1, step: 99, outputs are 0.10556478\n",
      "epoch: 1, step: 100, outputs are 0.10446866\n",
      "epoch: 1, step: 101, outputs are 0.10303557\n",
      "epoch: 1, step: 102, outputs are 0.10247441\n",
      "epoch: 1, step: 103, outputs are 0.100568995\n",
      "epoch: 1, step: 104, outputs are 0.10036362\n",
      "epoch: 1, step: 105, outputs are 0.09918763\n",
      "epoch: 1, step: 106, outputs are 0.09735572\n",
      "epoch: 1, step: 107, outputs are 0.09658269\n",
      "epoch: 1, step: 108, outputs are 0.09494255\n",
      "epoch: 1, step: 109, outputs are 0.09395993\n",
      "epoch: 1, step: 110, outputs are 0.093422234\n",
      "epoch: 1, step: 111, outputs are 0.09288837\n",
      "epoch: 1, step: 112, outputs are 0.09140007\n",
      "epoch: 1, step: 113, outputs are 0.0901995\n",
      "epoch: 1, step: 114, outputs are 0.08965979\n",
      "epoch: 1, step: 115, outputs are 0.08833307\n",
      "epoch: 1, step: 116, outputs are 0.08808066\n",
      "epoch: 1, step: 117, outputs are 0.0870693\n",
      "epoch: 1, step: 118, outputs are 0.085756846\n",
      "epoch: 1, step: 119, outputs are 0.08435824\n",
      "epoch: 1, step: 120, outputs are 0.08408359\n",
      "epoch: 1, step: 121, outputs are 0.08322534\n",
      "epoch: 1, step: 122, outputs are 0.08222342\n",
      "epoch: 1, step: 123, outputs are 0.08214365\n",
      "epoch: 1, step: 124, outputs are 0.08066352\n",
      "epoch: 1, step: 125, outputs are 0.080503106\n",
      "epoch: 1, step: 126, outputs are 0.07953184\n",
      "epoch: 1, step: 127, outputs are 0.07870437\n",
      "epoch: 1, step: 128, outputs are 0.07787338\n",
      "epoch: 1, step: 129, outputs are 0.07719781\n",
      "epoch: 1, step: 130, outputs are 0.07621257\n",
      "epoch: 1, step: 131, outputs are 0.075777255\n",
      "epoch: 1, step: 132, outputs are 0.07499707\n",
      "epoch: 1, step: 133, outputs are 0.07446416\n",
      "epoch: 1, step: 134, outputs are 0.07386616\n",
      "epoch: 1, step: 135, outputs are 0.073337175\n",
      "epoch: 1, step: 136, outputs are 0.07268168\n",
      "epoch: 1, step: 137, outputs are 0.071433194\n",
      "epoch: 1, step: 138, outputs are 0.07160737\n",
      "epoch: 1, step: 139, outputs are 0.07034654\n",
      "epoch: 1, step: 140, outputs are 0.069795385\n",
      "epoch: 1, step: 141, outputs are 0.06869326\n",
      "epoch: 1, step: 142, outputs are 0.06836037\n",
      "epoch: 1, step: 143, outputs are 0.06778377\n",
      "epoch: 1, step: 144, outputs are 0.066784814\n",
      "epoch: 1, step: 145, outputs are 0.06643551\n",
      "epoch: 1, step: 146, outputs are 0.06608105\n",
      "epoch: 1, step: 147, outputs are 0.06536387\n",
      "epoch: 1, step: 148, outputs are 0.06408295\n",
      "epoch: 1, step: 149, outputs are 0.064486705\n",
      "epoch: 1, step: 150, outputs are 0.06363099\n",
      "epoch: 1, step: 151, outputs are 0.06301274\n",
      "epoch: 1, step: 152, outputs are 0.06230189\n",
      "epoch: 1, step: 153, outputs are 0.061908413\n",
      "epoch: 1, step: 154, outputs are 0.06120943\n",
      "epoch: 1, step: 155, outputs are 0.060690634\n",
      "epoch: 1, step: 156, outputs are 0.06024854\n",
      "epoch: 1, step: 157, outputs are 0.059717566\n",
      "epoch: 1, step: 158, outputs are 0.059206083\n",
      "epoch: 1, step: 159, outputs are 0.058652468\n",
      "epoch: 1, step: 160, outputs are 0.058074363\n",
      "epoch: 1, step: 161, outputs are 0.057957623\n",
      "epoch: 1, step: 162, outputs are 0.057092234\n",
      "epoch: 1, step: 163, outputs are 0.056650676\n",
      "epoch: 1, step: 164, outputs are 0.056459256\n",
      "epoch: 1, step: 165, outputs are 0.055893697\n",
      "epoch: 1, step: 166, outputs are 0.05545138\n",
      "epoch: 1, step: 167, outputs are 0.054975167\n",
      "epoch: 1, step: 168, outputs are 0.054302685\n",
      "epoch: 1, step: 169, outputs are 0.05425199\n",
      "epoch: 1, step: 170, outputs are 0.05380674\n",
      "epoch: 1, step: 171, outputs are 0.053293645\n",
      "epoch: 1, step: 172, outputs are 0.052723095\n",
      "epoch: 1, step: 173, outputs are 0.052602906\n",
      "epoch: 1, step: 174, outputs are 0.05175542\n",
      "epoch: 1, step: 175, outputs are 0.051623255\n",
      "epoch: 1, step: 176, outputs are 0.051173743\n",
      "epoch: 1, step: 177, outputs are 0.050754406\n",
      "epoch: 1, step: 178, outputs are 0.050487787\n",
      "epoch: 1, step: 179, outputs are 0.050070927\n",
      "epoch: 1, step: 180, outputs are 0.049541645\n",
      "epoch: 1, step: 181, outputs are 0.049319886\n",
      "epoch: 1, step: 182, outputs are 0.049258165\n",
      "epoch: 1, step: 183, outputs are 0.04847134\n",
      "epoch: 1, step: 184, outputs are 0.048381012\n",
      "epoch: 1, step: 185, outputs are 0.04810132\n",
      "epoch: 1, step: 186, outputs are 0.047763854\n",
      "epoch: 1, step: 187, outputs are 0.047160298\n",
      "epoch: 1, step: 188, outputs are 0.046856627\n",
      "epoch: 1, step: 189, outputs are 0.04682692\n",
      "epoch: 1, step: 190, outputs are 0.046516642\n",
      "epoch: 1, step: 191, outputs are 0.04601398\n",
      "epoch: 1, step: 192, outputs are 0.045448013\n",
      "epoch: 1, step: 193, outputs are 0.045471877\n",
      "epoch: 1, step: 194, outputs are 0.04512505\n",
      "epoch: 1, step: 195, outputs are 0.044709593\n",
      "epoch: 1, step: 196, outputs are 0.044448733\n",
      "epoch: 1, step: 197, outputs are 0.043976873\n",
      "epoch: 1, step: 198, outputs are 0.043872662\n",
      "epoch: 1, step: 199, outputs are 0.043585222\n",
      "epoch: 1, step: 200, outputs are 0.043414496\n",
      "epoch: 1, step: 201, outputs are 0.0429233\n",
      "epoch: 1, step: 202, outputs are 0.042621713\n",
      "epoch: 1, step: 203, outputs are 0.042482868\n",
      "epoch: 1, step: 204, outputs are 0.042195566\n",
      "epoch: 1, step: 205, outputs are 0.041686222\n",
      "epoch: 1, step: 206, outputs are 0.04156219\n",
      "epoch: 1, step: 207, outputs are 0.04129856\n",
      "epoch: 1, step: 208, outputs are 0.04115298\n",
      "epoch: 1, step: 209, outputs are 0.040835533\n",
      "epoch: 1, step: 210, outputs are 0.040338006\n",
      "epoch: 1, step: 211, outputs are 0.040215127\n",
      "epoch: 1, step: 212, outputs are 0.040003538\n",
      "epoch: 1, step: 213, outputs are 0.039778665\n",
      "epoch: 1, step: 214, outputs are 0.039538227\n",
      "epoch: 1, step: 215, outputs are 0.039160904\n",
      "epoch: 1, step: 216, outputs are 0.039020084\n",
      "epoch: 1, step: 217, outputs are 0.0387507\n",
      "epoch: 1, step: 218, outputs are 0.038569942\n",
      "epoch: 1, step: 219, outputs are 0.038389146\n",
      "epoch: 1, step: 220, outputs are 0.038082838\n",
      "epoch: 1, step: 221, outputs are 0.037851032\n",
      "epoch: 1, step: 222, outputs are 0.037646525\n",
      "epoch: 1, step: 223, outputs are 0.03748058\n",
      "epoch: 1, step: 224, outputs are 0.037222702\n",
      "epoch: 1, step: 225, outputs are 0.036993325\n",
      "epoch: 1, step: 226, outputs are 0.036989793\n",
      "epoch: 1, step: 227, outputs are 0.036592297\n",
      "epoch: 1, step: 228, outputs are 0.036159173\n",
      "epoch: 1, step: 229, outputs are 0.03628955\n",
      "epoch: 1, step: 230, outputs are 0.035899222\n",
      "epoch: 1, step: 231, outputs are 0.03570558\n",
      "epoch: 1, step: 232, outputs are 0.035405897\n",
      "epoch: 1, step: 233, outputs are 0.035257183\n",
      "epoch: 1, step: 234, outputs are 0.035027266\n",
      "epoch: 1, step: 235, outputs are 0.034929395\n",
      "epoch: 1, step: 236, outputs are 0.034571003\n",
      "epoch: 1, step: 237, outputs are 0.034353677\n",
      "epoch: 1, step: 238, outputs are 0.034307208\n",
      "epoch: 1, step: 239, outputs are 0.034030363\n",
      "epoch: 1, step: 240, outputs are 0.03380315\n",
      "epoch: 1, step: 241, outputs are 0.03367148\n",
      "epoch: 1, step: 242, outputs are 0.033552844\n",
      "epoch: 1, step: 243, outputs are 0.03316562\n",
      "epoch: 1, step: 244, outputs are 0.033157006\n",
      "epoch: 1, step: 245, outputs are 0.032895833\n",
      "epoch: 1, step: 246, outputs are 0.032689266\n",
      "epoch: 1, step: 247, outputs are 0.03241556\n",
      "epoch: 1, step: 248, outputs are 0.03242719\n",
      "epoch: 1, step: 249, outputs are 0.032105364\n",
      "epoch: 1, step: 250, outputs are 0.031983424\n",
      "epoch: 1, step: 251, outputs are 0.03171207\n",
      "epoch: 1, step: 252, outputs are 0.0315074\n",
      "epoch: 1, step: 253, outputs are 0.03147022\n",
      "epoch: 1, step: 254, outputs are 0.031079397\n",
      "epoch: 1, step: 255, outputs are 0.030969033\n",
      "epoch: 1, step: 256, outputs are 0.030874982\n",
      "epoch: 1, step: 257, outputs are 0.030663654\n",
      "epoch: 1, step: 258, outputs are 0.030487359\n",
      "epoch: 1, step: 259, outputs are 0.030319136\n",
      "epoch: 1, step: 260, outputs are 0.03020947\n",
      "epoch: 1, step: 261, outputs are 0.029865135\n",
      "epoch: 1, step: 262, outputs are 0.02970446\n",
      "epoch: 1, step: 263, outputs are 0.029627152\n",
      "epoch: 1, step: 264, outputs are 0.029443685\n",
      "epoch: 1, step: 265, outputs are 0.0294394\n",
      "epoch: 1, step: 266, outputs are 0.02904506\n",
      "epoch: 1, step: 267, outputs are 0.02894998\n",
      "epoch: 1, step: 268, outputs are 0.028793123\n",
      "epoch: 1, step: 269, outputs are 0.028696742\n",
      "epoch: 1, step: 270, outputs are 0.02843383\n",
      "epoch: 1, step: 271, outputs are 0.028325919\n",
      "epoch: 1, step: 272, outputs are 0.028208334\n",
      "epoch: 1, step: 273, outputs are 0.028068164\n",
      "epoch: 1, step: 274, outputs are 0.027895182\n",
      "epoch: 1, step: 275, outputs are 0.027746117\n",
      "epoch: 1, step: 276, outputs are 0.027564209\n",
      "epoch: 1, step: 277, outputs are 0.02753387\n",
      "epoch: 1, step: 278, outputs are 0.02730055\n",
      "epoch: 1, step: 279, outputs are 0.027175833\n",
      "epoch: 1, step: 280, outputs are 0.027028505\n",
      "epoch: 1, step: 281, outputs are 0.026908167\n",
      "epoch: 1, step: 282, outputs are 0.026703782\n",
      "epoch: 1, step: 283, outputs are 0.02652822\n",
      "epoch: 1, step: 284, outputs are 0.026405796\n",
      "epoch: 1, step: 285, outputs are 0.026262295\n",
      "epoch: 1, step: 286, outputs are 0.02612908\n",
      "epoch: 1, step: 287, outputs are 0.025836777\n",
      "epoch: 1, step: 288, outputs are 0.025706246\n",
      "epoch: 1, step: 289, outputs are 0.025648316\n",
      "epoch: 1, step: 290, outputs are 0.025570463\n",
      "epoch: 1, step: 291, outputs are 0.025276793\n",
      "epoch: 1, step: 292, outputs are 0.02518597\n",
      "epoch: 1, step: 293, outputs are 0.02498715\n",
      "epoch: 1, step: 294, outputs are 0.024873355\n",
      "epoch: 1, step: 295, outputs are 0.024728559\n",
      "epoch: 1, step: 296, outputs are 0.024737278\n",
      "epoch: 1, step: 297, outputs are 0.024524773\n",
      "epoch: 1, step: 298, outputs are 0.02432383\n",
      "epoch: 1, step: 299, outputs are 0.024315208\n",
      "epoch: 1, step: 300, outputs are 0.024161287\n",
      "epoch: 1, step: 301, outputs are 0.024007354\n",
      "epoch: 1, step: 302, outputs are 0.023781596\n",
      "epoch: 1, step: 303, outputs are 0.023605991\n",
      "epoch: 1, step: 304, outputs are 0.02352576\n",
      "epoch: 1, step: 305, outputs are 0.02337842\n",
      "epoch: 1, step: 306, outputs are 0.023356348\n",
      "epoch: 1, step: 307, outputs are 0.02323072\n",
      "epoch: 1, step: 308, outputs are 0.02310571\n",
      "epoch: 1, step: 309, outputs are 0.022903599\n",
      "epoch: 1, step: 310, outputs are 0.022814125\n",
      "epoch: 1, step: 311, outputs are 0.022735655\n",
      "epoch: 1, step: 312, outputs are 0.022539683\n",
      "epoch: 1, step: 313, outputs are 0.022472125\n",
      "epoch: 1, step: 314, outputs are 0.022365566\n",
      "epoch: 1, step: 315, outputs are 0.022171877\n",
      "epoch: 1, step: 316, outputs are 0.022076985\n",
      "epoch: 1, step: 317, outputs are 0.02199683\n",
      "epoch: 1, step: 318, outputs are 0.021953285\n",
      "epoch: 1, step: 319, outputs are 0.02178261\n",
      "epoch: 1, step: 320, outputs are 0.021715082\n",
      "epoch: 1, step: 321, outputs are 0.021557065\n",
      "epoch: 1, step: 322, outputs are 0.021536982\n",
      "epoch: 1, step: 323, outputs are 0.021383125\n",
      "epoch: 1, step: 324, outputs are 0.02121928\n",
      "epoch: 1, step: 325, outputs are 0.021106107\n",
      "epoch: 1, step: 326, outputs are 0.021068107\n",
      "epoch: 1, step: 327, outputs are 0.020890297\n",
      "epoch: 1, step: 328, outputs are 0.020782476\n",
      "epoch: 1, step: 329, outputs are 0.020668264\n",
      "epoch: 1, step: 330, outputs are 0.020626424\n",
      "epoch: 1, step: 331, outputs are 0.020488352\n",
      "epoch: 1, step: 332, outputs are 0.020261347\n",
      "epoch: 1, step: 333, outputs are 0.020198923\n",
      "epoch: 1, step: 334, outputs are 0.020029657\n",
      "epoch: 1, step: 335, outputs are 0.019822719\n",
      "epoch: 1, step: 336, outputs are 0.019683097\n",
      "epoch: 1, step: 337, outputs are 0.019662548\n",
      "epoch: 1, step: 338, outputs are 0.019478843\n",
      "epoch: 1, step: 339, outputs are 0.01936908\n",
      "epoch: 1, step: 340, outputs are 0.019171914\n",
      "epoch: 1, step: 341, outputs are 0.019100707\n",
      "epoch: 1, step: 342, outputs are 0.018966924\n",
      "epoch: 1, step: 343, outputs are 0.01884981\n",
      "epoch: 1, step: 344, outputs are 0.018680593\n",
      "epoch: 1, step: 345, outputs are 0.018620372\n",
      "epoch: 1, step: 346, outputs are 0.018435746\n",
      "epoch: 1, step: 347, outputs are 0.01833127\n",
      "epoch: 1, step: 348, outputs are 0.018172197\n",
      "epoch: 1, step: 349, outputs are 0.018114783\n",
      "epoch: 1, step: 350, outputs are 0.018042704\n",
      "epoch: 1, step: 351, outputs are 0.0179101\n",
      "epoch: 1, step: 352, outputs are 0.017737508\n",
      "epoch: 1, step: 353, outputs are 0.017664187\n",
      "epoch: 1, step: 354, outputs are 0.01759026\n",
      "epoch: 1, step: 355, outputs are 0.017486459\n",
      "epoch: 1, step: 356, outputs are 0.017367376\n",
      "epoch: 1, step: 357, outputs are 0.017267019\n",
      "epoch: 1, step: 358, outputs are 0.01715459\n",
      "epoch: 1, step: 359, outputs are 0.017107412\n",
      "epoch: 1, step: 360, outputs are 0.017000524\n",
      "epoch: 1, step: 361, outputs are 0.01687453\n",
      "epoch: 1, step: 362, outputs are 0.016838245\n",
      "epoch: 1, step: 363, outputs are 0.016711742\n",
      "epoch: 1, step: 364, outputs are 0.016574029\n",
      "epoch: 1, step: 365, outputs are 0.016492328\n",
      "epoch: 1, step: 366, outputs are 0.016391313\n",
      "epoch: 1, step: 367, outputs are 0.016384875\n",
      "epoch: 1, step: 368, outputs are 0.016269729\n",
      "epoch: 1, step: 369, outputs are 0.016248386\n",
      "epoch: 1, step: 370, outputs are 0.01615977\n",
      "epoch: 1, step: 371, outputs are 0.016056068\n",
      "epoch: 1, step: 372, outputs are 0.01594258\n",
      "epoch: 1, step: 373, outputs are 0.015874926\n",
      "epoch: 1, step: 374, outputs are 0.015849939\n",
      "epoch: 1, step: 375, outputs are 0.015734158\n",
      "epoch: 1, step: 376, outputs are 0.015697833\n",
      "epoch: 1, step: 377, outputs are 0.015589455\n",
      "epoch: 1, step: 378, outputs are 0.015472375\n",
      "epoch: 1, step: 379, outputs are 0.015420677\n",
      "epoch: 1, step: 380, outputs are 0.01543143\n",
      "epoch: 1, step: 381, outputs are 0.015343195\n",
      "epoch: 1, step: 382, outputs are 0.015243187\n",
      "epoch: 1, step: 383, outputs are 0.015150344\n",
      "epoch: 1, step: 384, outputs are 0.01508161\n",
      "epoch: 1, step: 385, outputs are 0.0149929095\n",
      "epoch: 1, step: 386, outputs are 0.014903331\n",
      "epoch: 1, step: 387, outputs are 0.0148524195\n",
      "epoch: 1, step: 388, outputs are 0.014706075\n",
      "epoch: 1, step: 389, outputs are 0.014621044\n",
      "epoch: 1, step: 390, outputs are 0.014589689\n",
      "epoch: 1, step: 391, outputs are 0.014504826\n",
      "epoch: 1, step: 392, outputs are 0.014436372\n",
      "epoch: 1, step: 393, outputs are 0.01429107\n",
      "epoch: 1, step: 394, outputs are 0.014260798\n",
      "epoch: 1, step: 395, outputs are 0.014137141\n",
      "epoch: 1, step: 396, outputs are 0.014118187\n",
      "epoch: 1, step: 397, outputs are 0.013986366\n",
      "epoch: 1, step: 398, outputs are 0.0139454715\n",
      "epoch: 1, step: 399, outputs are 0.013856024\n",
      "epoch: 1, step: 400, outputs are 0.013771664\n",
      "epoch: 1, step: 401, outputs are 0.01376193\n",
      "epoch: 1, step: 402, outputs are 0.013686212\n",
      "epoch: 1, step: 403, outputs are 0.013601616\n",
      "epoch: 1, step: 404, outputs are 0.013536643\n",
      "epoch: 1, step: 405, outputs are 0.01345766\n",
      "epoch: 1, step: 406, outputs are 0.0134201655\n",
      "epoch: 1, step: 407, outputs are 0.013340923\n",
      "epoch: 1, step: 408, outputs are 0.013298828\n",
      "epoch: 1, step: 409, outputs are 0.013179464\n",
      "epoch: 1, step: 410, outputs are 0.01311276\n",
      "epoch: 1, step: 411, outputs are 0.01307449\n",
      "epoch: 1, step: 412, outputs are 0.012972533\n",
      "epoch: 1, step: 413, outputs are 0.012908906\n",
      "epoch: 1, step: 414, outputs are 0.012889046\n",
      "epoch: 1, step: 415, outputs are 0.012845648\n",
      "epoch: 1, step: 416, outputs are 0.012711544\n",
      "epoch: 1, step: 417, outputs are 0.012683656\n",
      "epoch: 1, step: 418, outputs are 0.012621762\n",
      "epoch: 1, step: 419, outputs are 0.012594941\n",
      "epoch: 1, step: 420, outputs are 0.012525874\n",
      "epoch: 1, step: 421, outputs are 0.012493327\n",
      "epoch: 1, step: 422, outputs are 0.012365118\n",
      "epoch: 1, step: 423, outputs are 0.012341116\n",
      "epoch: 1, step: 424, outputs are 0.012277951\n",
      "epoch: 1, step: 425, outputs are 0.012235308\n",
      "epoch: 1, step: 426, outputs are 0.01217299\n",
      "epoch: 1, step: 427, outputs are 0.012140148\n",
      "epoch: 1, step: 428, outputs are 0.012077398\n",
      "epoch: 1, step: 429, outputs are 0.012070407\n",
      "epoch: 1, step: 430, outputs are 0.011958871\n",
      "epoch: 1, step: 431, outputs are 0.011912702\n",
      "epoch: 1, step: 432, outputs are 0.011895269\n",
      "epoch: 1, step: 433, outputs are 0.011846374\n",
      "epoch: 1, step: 434, outputs are 0.011812498\n",
      "epoch: 1, step: 435, outputs are 0.011711825\n",
      "epoch: 1, step: 436, outputs are 0.011680389\n",
      "epoch: 1, step: 437, outputs are 0.011684759\n",
      "epoch: 1, step: 438, outputs are 0.011610235\n",
      "epoch: 1, step: 439, outputs are 0.011531137\n",
      "epoch: 1, step: 440, outputs are 0.011522456\n",
      "epoch: 1, step: 441, outputs are 0.011446774\n",
      "epoch: 1, step: 442, outputs are 0.011426972\n",
      "epoch: 1, step: 443, outputs are 0.011350841\n",
      "epoch: 1, step: 444, outputs are 0.011333363\n",
      "epoch: 1, step: 445, outputs are 0.011275779\n",
      "epoch: 1, step: 446, outputs are 0.011208326\n",
      "epoch: 1, step: 447, outputs are 0.011166242\n",
      "epoch: 1, step: 448, outputs are 0.011161897\n",
      "epoch: 1, step: 449, outputs are 0.011065999\n",
      "epoch: 1, step: 450, outputs are 0.011074001\n",
      "epoch: 1, step: 451, outputs are 0.011018233\n",
      "epoch: 1, step: 452, outputs are 0.010956652\n",
      "epoch: 1, step: 453, outputs are 0.010914109\n",
      "epoch: 1, step: 454, outputs are 0.010893968\n",
      "epoch: 1, step: 455, outputs are 0.010847998\n",
      "epoch: 1, step: 456, outputs are 0.010803386\n",
      "epoch: 1, step: 457, outputs are 0.010764772\n",
      "epoch: 1, step: 458, outputs are 0.010715118\n",
      "epoch: 1, step: 459, outputs are 0.010679906\n",
      "epoch: 1, step: 460, outputs are 0.010603272\n",
      "epoch: 1, step: 461, outputs are 0.010617742\n",
      "epoch: 1, step: 462, outputs are 0.010564882\n",
      "epoch: 1, step: 463, outputs are 0.010508478\n",
      "epoch: 1, step: 464, outputs are 0.010481919\n",
      "epoch: 1, step: 465, outputs are 0.010434322\n",
      "epoch: 1, step: 466, outputs are 0.010391777\n",
      "epoch: 1, step: 467, outputs are 0.010382684\n",
      "epoch: 1, step: 468, outputs are 0.010331312\n",
      "epoch: 1, step: 469, outputs are 0.0102913845\n",
      "epoch: 1, step: 470, outputs are 0.010244033\n",
      "epoch: 1, step: 471, outputs are 0.010200158\n",
      "epoch: 1, step: 472, outputs are 0.010172719\n",
      "epoch: 1, step: 473, outputs are 0.010139613\n",
      "epoch: 1, step: 474, outputs are 0.010099186\n",
      "epoch: 1, step: 475, outputs are 0.010078247\n",
      "epoch: 1, step: 476, outputs are 0.010022667\n",
      "epoch: 1, step: 477, outputs are 0.010014543\n",
      "epoch: 1, step: 478, outputs are 0.00997135\n",
      "epoch: 1, step: 479, outputs are 0.009930767\n",
      "epoch: 1, step: 480, outputs are 0.009878106\n",
      "epoch: 1, step: 481, outputs are 0.009850444\n",
      "epoch: 1, step: 482, outputs are 0.00981109\n",
      "epoch: 1, step: 483, outputs are 0.009782122\n",
      "epoch: 1, step: 484, outputs are 0.009757022\n",
      "epoch: 1, step: 485, outputs are 0.009723913\n",
      "epoch: 1, step: 486, outputs are 0.009674738\n",
      "epoch: 1, step: 487, outputs are 0.009654516\n",
      "epoch: 1, step: 488, outputs are 0.009643626\n",
      "epoch: 1, step: 489, outputs are 0.0095810015\n",
      "epoch: 1, step: 490, outputs are 0.009521218\n",
      "epoch: 1, step: 491, outputs are 0.0095137805\n",
      "epoch: 1, step: 492, outputs are 0.009481868\n",
      "epoch: 1, step: 493, outputs are 0.009456725\n",
      "epoch: 1, step: 494, outputs are 0.009405473\n",
      "epoch: 1, step: 495, outputs are 0.009360705\n",
      "epoch: 1, step: 496, outputs are 0.009356685\n",
      "epoch: 1, step: 497, outputs are 0.009297371\n",
      "epoch: 1, step: 498, outputs are 0.00926971\n",
      "epoch: 1, step: 499, outputs are 0.009255122\n",
      "epoch: 1, step: 500, outputs are 0.009241998\n",
      "epoch: 1, step: 501, outputs are 0.00918544\n",
      "epoch: 1, step: 502, outputs are 0.009172226\n",
      "epoch: 1, step: 503, outputs are 0.00911491\n",
      "epoch: 1, step: 504, outputs are 0.009103786\n",
      "epoch: 1, step: 505, outputs are 0.009055507\n",
      "epoch: 1, step: 506, outputs are 0.009024577\n",
      "epoch: 1, step: 507, outputs are 0.009024134\n",
      "epoch: 1, step: 508, outputs are 0.008959422\n",
      "epoch: 1, step: 509, outputs are 0.008939683\n",
      "epoch: 1, step: 510, outputs are 0.008910706\n",
      "epoch: 1, step: 511, outputs are 0.008867094\n",
      "epoch: 1, step: 512, outputs are 0.00883908\n",
      "epoch: 1, step: 513, outputs are 0.00880374\n",
      "epoch: 1, step: 514, outputs are 0.008777322\n",
      "epoch: 1, step: 515, outputs are 0.008764336\n",
      "epoch: 1, step: 516, outputs are 0.00871085\n",
      "epoch: 1, step: 517, outputs are 0.00869425\n",
      "epoch: 1, step: 518, outputs are 0.00866377\n",
      "epoch: 1, step: 519, outputs are 0.0086139245\n",
      "epoch: 1, step: 520, outputs are 0.008596571\n",
      "epoch: 1, step: 521, outputs are 0.008561578\n",
      "epoch: 1, step: 522, outputs are 0.008551594\n",
      "epoch: 1, step: 523, outputs are 0.008511624\n",
      "epoch: 1, step: 524, outputs are 0.0084824\n",
      "epoch: 1, step: 525, outputs are 0.008454705\n",
      "epoch: 1, step: 526, outputs are 0.00843935\n",
      "epoch: 1, step: 527, outputs are 0.008410959\n",
      "epoch: 1, step: 528, outputs are 0.008366853\n",
      "epoch: 1, step: 529, outputs are 0.008370111\n",
      "epoch: 1, step: 530, outputs are 0.008322561\n",
      "epoch: 1, step: 531, outputs are 0.008298574\n",
      "epoch: 1, step: 532, outputs are 0.008293771\n",
      "epoch: 1, step: 533, outputs are 0.008267401\n",
      "epoch: 1, step: 534, outputs are 0.008211162\n",
      "epoch: 1, step: 535, outputs are 0.008203781\n",
      "epoch: 1, step: 536, outputs are 0.008169288\n",
      "epoch: 1, step: 537, outputs are 0.008147041\n",
      "epoch: 1, step: 538, outputs are 0.008132035\n",
      "epoch: 1, step: 539, outputs are 0.008113522\n",
      "epoch: 1, step: 540, outputs are 0.008068777\n",
      "epoch: 1, step: 541, outputs are 0.008037044\n",
      "epoch: 1, step: 542, outputs are 0.008010906\n",
      "epoch: 1, step: 543, outputs are 0.007989535\n",
      "epoch: 1, step: 544, outputs are 0.007959516\n",
      "epoch: 1, step: 545, outputs are 0.007936435\n",
      "epoch: 1, step: 546, outputs are 0.007923016\n",
      "epoch: 1, step: 547, outputs are 0.007880352\n",
      "epoch: 1, step: 548, outputs are 0.007866474\n",
      "epoch: 1, step: 549, outputs are 0.007840446\n",
      "epoch: 1, step: 550, outputs are 0.007828833\n",
      "epoch: 1, step: 551, outputs are 0.00779426\n",
      "epoch: 1, step: 552, outputs are 0.0077643725\n",
      "epoch: 1, step: 553, outputs are 0.0077170967\n",
      "epoch: 1, step: 554, outputs are 0.0076925675\n",
      "epoch: 1, step: 555, outputs are 0.007688647\n",
      "epoch: 1, step: 556, outputs are 0.007667646\n",
      "epoch: 1, step: 557, outputs are 0.0076354137\n",
      "epoch: 1, step: 558, outputs are 0.007621128\n",
      "epoch: 1, step: 559, outputs are 0.007594037\n",
      "epoch: 1, step: 560, outputs are 0.007577721\n",
      "epoch: 1, step: 561, outputs are 0.0075541288\n",
      "epoch: 1, step: 562, outputs are 0.0075299824\n",
      "epoch: 1, step: 563, outputs are 0.0075013516\n",
      "epoch: 1, step: 564, outputs are 0.007463113\n",
      "epoch: 1, step: 565, outputs are 0.007457616\n",
      "epoch: 1, step: 566, outputs are 0.007448002\n",
      "epoch: 1, step: 567, outputs are 0.0074237124\n",
      "epoch: 1, step: 568, outputs are 0.0073928777\n",
      "epoch: 1, step: 569, outputs are 0.0073863943\n",
      "epoch: 1, step: 570, outputs are 0.0073550534\n",
      "epoch: 1, step: 571, outputs are 0.0073362524\n",
      "epoch: 1, step: 572, outputs are 0.0072870413\n",
      "epoch: 1, step: 573, outputs are 0.007282938\n",
      "epoch: 1, step: 574, outputs are 0.007263167\n",
      "epoch: 1, step: 575, outputs are 0.007229227\n",
      "epoch: 1, step: 576, outputs are 0.007230522\n",
      "epoch: 1, step: 577, outputs are 0.00718032\n",
      "epoch: 1, step: 578, outputs are 0.0071716136\n",
      "epoch: 1, step: 579, outputs are 0.0071478994\n",
      "epoch: 1, step: 580, outputs are 0.0071289986\n",
      "epoch: 1, step: 581, outputs are 0.0071034296\n",
      "epoch: 1, step: 582, outputs are 0.0070844656\n",
      "epoch: 1, step: 583, outputs are 0.007070569\n",
      "epoch: 1, step: 584, outputs are 0.007055272\n",
      "epoch: 1, step: 585, outputs are 0.007012608\n",
      "epoch: 1, step: 586, outputs are 0.0070101167\n",
      "epoch: 1, step: 587, outputs are 0.006990612\n",
      "epoch: 1, step: 588, outputs are 0.006960015\n",
      "epoch: 1, step: 589, outputs are 0.0069304756\n",
      "epoch: 1, step: 590, outputs are 0.006947336\n",
      "epoch: 1, step: 591, outputs are 0.0069090766\n",
      "epoch: 1, step: 592, outputs are 0.0068792435\n",
      "epoch: 1, step: 593, outputs are 0.006860963\n",
      "epoch: 1, step: 594, outputs are 0.0068562273\n",
      "epoch: 1, step: 595, outputs are 0.0068338723\n",
      "epoch: 1, step: 596, outputs are 0.006791801\n",
      "epoch: 1, step: 597, outputs are 0.006775389\n",
      "epoch: 1, step: 598, outputs are 0.0067656282\n",
      "epoch: 1, step: 599, outputs are 0.0067402003\n",
      "epoch: 1, step: 600, outputs are 0.0067248596\n",
      "epoch: 1, step: 601, outputs are 0.006709804\n",
      "epoch: 1, step: 602, outputs are 0.0066795507\n",
      "epoch: 1, step: 603, outputs are 0.0066722874\n",
      "epoch: 1, step: 604, outputs are 0.0066433055\n",
      "epoch: 1, step: 605, outputs are 0.0066343956\n",
      "epoch: 1, step: 606, outputs are 0.0066093383\n",
      "epoch: 1, step: 607, outputs are 0.0065718023\n",
      "epoch: 1, step: 608, outputs are 0.006576594\n",
      "epoch: 1, step: 609, outputs are 0.006547495\n",
      "epoch: 1, step: 610, outputs are 0.0065255263\n",
      "epoch: 1, step: 611, outputs are 0.0065167043\n",
      "epoch: 1, step: 612, outputs are 0.0064921747\n",
      "epoch: 1, step: 613, outputs are 0.0064736186\n",
      "epoch: 1, step: 614, outputs are 0.00645199\n",
      "epoch: 1, step: 615, outputs are 0.006433639\n",
      "epoch: 1, step: 616, outputs are 0.006417429\n",
      "epoch: 1, step: 617, outputs are 0.006412328\n",
      "epoch: 1, step: 618, outputs are 0.0063899104\n",
      "epoch: 1, step: 619, outputs are 0.00637154\n",
      "epoch: 1, step: 620, outputs are 0.006359348\n",
      "epoch: 1, step: 621, outputs are 0.0063330615\n",
      "epoch: 1, step: 622, outputs are 0.006321918\n",
      "epoch: 1, step: 623, outputs are 0.006302719\n",
      "epoch: 1, step: 624, outputs are 0.0062856195\n",
      "epoch: 1, step: 625, outputs are 0.0062529594\n",
      "epoch: 1, step: 626, outputs are 0.006224719\n",
      "epoch: 1, step: 627, outputs are 0.0062212325\n",
      "epoch: 1, step: 628, outputs are 0.006196248\n",
      "epoch: 1, step: 629, outputs are 0.0061852904\n",
      "epoch: 1, step: 630, outputs are 0.0061579295\n",
      "epoch: 1, step: 631, outputs are 0.006144163\n",
      "epoch: 1, step: 632, outputs are 0.006127163\n",
      "epoch: 1, step: 633, outputs are 0.0061111874\n",
      "epoch: 1, step: 634, outputs are 0.006102346\n",
      "epoch: 1, step: 635, outputs are 0.006073381\n",
      "epoch: 1, step: 636, outputs are 0.0060595656\n",
      "epoch: 1, step: 637, outputs are 0.006034892\n",
      "epoch: 1, step: 638, outputs are 0.006033065\n",
      "epoch: 1, step: 639, outputs are 0.0059998184\n",
      "epoch: 1, step: 640, outputs are 0.005988133\n",
      "epoch: 1, step: 641, outputs are 0.0059677316\n",
      "epoch: 1, step: 642, outputs are 0.005945726\n",
      "epoch: 1, step: 643, outputs are 0.005943628\n",
      "epoch: 1, step: 644, outputs are 0.0059210802\n",
      "epoch: 1, step: 645, outputs are 0.0059142695\n",
      "epoch: 1, step: 646, outputs are 0.005890754\n",
      "epoch: 1, step: 647, outputs are 0.0058712824\n",
      "epoch: 1, step: 648, outputs are 0.005863826\n",
      "epoch: 1, step: 649, outputs are 0.005825148\n",
      "epoch: 1, step: 650, outputs are 0.0058263508\n",
      "epoch: 1, step: 651, outputs are 0.005814694\n",
      "epoch: 1, step: 652, outputs are 0.0057908953\n",
      "epoch: 1, step: 653, outputs are 0.005769048\n",
      "epoch: 1, step: 654, outputs are 0.0057543367\n",
      "epoch: 1, step: 655, outputs are 0.0057344446\n",
      "epoch: 1, step: 656, outputs are 0.0057316544\n",
      "epoch: 1, step: 657, outputs are 0.0057087746\n",
      "epoch: 1, step: 658, outputs are 0.0057003936\n",
      "epoch: 1, step: 659, outputs are 0.0056626587\n",
      "epoch: 1, step: 660, outputs are 0.0056678196\n",
      "epoch: 1, step: 661, outputs are 0.005645834\n",
      "epoch: 1, step: 662, outputs are 0.005634982\n",
      "epoch: 1, step: 663, outputs are 0.0056103365\n",
      "epoch: 1, step: 664, outputs are 0.0055960333\n",
      "epoch: 1, step: 665, outputs are 0.0055873133\n",
      "epoch: 1, step: 666, outputs are 0.005562641\n",
      "epoch: 1, step: 667, outputs are 0.00556182\n",
      "epoch: 1, step: 668, outputs are 0.005530552\n",
      "epoch: 1, step: 669, outputs are 0.005518156\n",
      "epoch: 1, step: 670, outputs are 0.005511463\n",
      "epoch: 1, step: 671, outputs are 0.005492734\n",
      "epoch: 1, step: 672, outputs are 0.005472364\n",
      "epoch: 1, step: 673, outputs are 0.005460811\n",
      "epoch: 1, step: 674, outputs are 0.0054613217\n",
      "epoch: 1, step: 675, outputs are 0.005419764\n",
      "epoch: 1, step: 676, outputs are 0.0054227896\n",
      "epoch: 1, step: 677, outputs are 0.005410744\n",
      "epoch: 1, step: 678, outputs are 0.005388961\n",
      "epoch: 1, step: 679, outputs are 0.0053825667\n",
      "epoch: 1, step: 680, outputs are 0.005360646\n",
      "epoch: 1, step: 681, outputs are 0.005342651\n",
      "epoch: 1, step: 682, outputs are 0.005340106\n",
      "epoch: 1, step: 683, outputs are 0.0053303903\n",
      "epoch: 1, step: 684, outputs are 0.0053144624\n",
      "epoch: 1, step: 685, outputs are 0.005284873\n",
      "epoch: 1, step: 686, outputs are 0.005286943\n",
      "epoch: 1, step: 687, outputs are 0.0052557113\n",
      "epoch: 1, step: 688, outputs are 0.0052449508\n",
      "epoch: 1, step: 689, outputs are 0.0052449605\n",
      "epoch: 1, step: 690, outputs are 0.005232214\n",
      "epoch: 1, step: 691, outputs are 0.0052029095\n",
      "epoch: 1, step: 692, outputs are 0.00519123\n",
      "epoch: 1, step: 693, outputs are 0.00518383\n",
      "epoch: 1, step: 694, outputs are 0.0051678573\n",
      "epoch: 1, step: 695, outputs are 0.0051526893\n",
      "epoch: 1, step: 696, outputs are 0.005139356\n",
      "epoch: 1, step: 697, outputs are 0.00512229\n",
      "epoch: 1, step: 698, outputs are 0.0051018866\n",
      "epoch: 1, step: 699, outputs are 0.005091247\n",
      "epoch: 1, step: 700, outputs are 0.0050792946\n",
      "epoch: 1, step: 701, outputs are 0.005070488\n",
      "epoch: 1, step: 702, outputs are 0.0050533945\n",
      "epoch: 1, step: 703, outputs are 0.005041416\n",
      "epoch: 1, step: 704, outputs are 0.0050223386\n",
      "epoch: 1, step: 705, outputs are 0.005007711\n",
      "epoch: 1, step: 706, outputs are 0.004998315\n",
      "epoch: 1, step: 707, outputs are 0.004982328\n",
      "epoch: 1, step: 708, outputs are 0.0049715517\n",
      "epoch: 1, step: 709, outputs are 0.004956997\n",
      "epoch: 1, step: 710, outputs are 0.004943604\n",
      "epoch: 1, step: 711, outputs are 0.0049359994\n",
      "epoch: 1, step: 712, outputs are 0.004919537\n",
      "epoch: 1, step: 713, outputs are 0.004896735\n",
      "epoch: 1, step: 714, outputs are 0.004886915\n",
      "epoch: 1, step: 715, outputs are 0.0048779277\n",
      "epoch: 1, step: 716, outputs are 0.0048613464\n",
      "epoch: 1, step: 717, outputs are 0.0048465417\n",
      "epoch: 1, step: 718, outputs are 0.0048281476\n",
      "epoch: 1, step: 719, outputs are 0.0048253858\n",
      "epoch: 1, step: 720, outputs are 0.004811773\n",
      "epoch: 1, step: 721, outputs are 0.0047952635\n",
      "epoch: 1, step: 722, outputs are 0.004782986\n",
      "epoch: 1, step: 723, outputs are 0.0047634733\n",
      "epoch: 1, step: 724, outputs are 0.0047650374\n",
      "epoch: 1, step: 725, outputs are 0.0047335494\n",
      "epoch: 1, step: 726, outputs are 0.004725638\n",
      "epoch: 1, step: 727, outputs are 0.0047152294\n",
      "epoch: 1, step: 728, outputs are 0.004697204\n",
      "epoch: 1, step: 729, outputs are 0.0046898127\n",
      "epoch: 1, step: 730, outputs are 0.004669275\n",
      "epoch: 1, step: 731, outputs are 0.0046664\n",
      "epoch: 1, step: 732, outputs are 0.004651975\n",
      "epoch: 1, step: 733, outputs are 0.004648121\n",
      "epoch: 1, step: 734, outputs are 0.004629184\n",
      "epoch: 1, step: 735, outputs are 0.004609109\n",
      "epoch: 1, step: 736, outputs are 0.004599385\n",
      "epoch: 1, step: 737, outputs are 0.004588461\n",
      "epoch: 1, step: 738, outputs are 0.0045786453\n",
      "epoch: 1, step: 739, outputs are 0.0045560645\n",
      "epoch: 1, step: 740, outputs are 0.004545687\n",
      "epoch: 1, step: 741, outputs are 0.004541311\n",
      "epoch: 1, step: 742, outputs are 0.004533214\n",
      "epoch: 1, step: 743, outputs are 0.004509231\n",
      "epoch: 1, step: 744, outputs are 0.0044998582\n",
      "epoch: 1, step: 745, outputs are 0.004485678\n",
      "epoch: 1, step: 746, outputs are 0.0044712056\n",
      "epoch: 1, step: 747, outputs are 0.0044530043\n",
      "epoch: 1, step: 748, outputs are 0.004442016\n",
      "epoch: 1, step: 749, outputs are 0.0044338666\n",
      "epoch: 1, step: 750, outputs are 0.004425669\n",
      "epoch: 1, step: 751, outputs are 0.0044126417\n",
      "epoch: 1, step: 752, outputs are 0.0044035483\n",
      "epoch: 1, step: 753, outputs are 0.004391011\n",
      "epoch: 1, step: 754, outputs are 0.0043731085\n",
      "epoch: 1, step: 755, outputs are 0.0043650502\n",
      "epoch: 1, step: 756, outputs are 0.004343355\n",
      "epoch: 1, step: 757, outputs are 0.0043442775\n",
      "epoch: 1, step: 758, outputs are 0.00432901\n",
      "epoch: 1, step: 759, outputs are 0.0043175407\n",
      "epoch: 1, step: 760, outputs are 0.004305897\n",
      "epoch: 1, step: 761, outputs are 0.004292478\n",
      "epoch: 1, step: 762, outputs are 0.0042830347\n",
      "epoch: 1, step: 763, outputs are 0.004268962\n",
      "epoch: 1, step: 764, outputs are 0.0042626904\n",
      "epoch: 1, step: 765, outputs are 0.0042467453\n",
      "epoch: 1, step: 766, outputs are 0.0042376723\n",
      "epoch: 1, step: 767, outputs are 0.004226269\n",
      "epoch: 1, step: 768, outputs are 0.0042104125\n",
      "epoch: 1, step: 769, outputs are 0.0042052697\n",
      "epoch: 1, step: 770, outputs are 0.004191094\n",
      "epoch: 1, step: 771, outputs are 0.004179096\n",
      "epoch: 1, step: 772, outputs are 0.0041725943\n",
      "epoch: 1, step: 773, outputs are 0.0041588703\n",
      "epoch: 1, step: 774, outputs are 0.004151171\n",
      "epoch: 1, step: 775, outputs are 0.0041463627\n",
      "epoch: 1, step: 776, outputs are 0.0041220747\n",
      "epoch: 1, step: 777, outputs are 0.0041149626\n",
      "epoch: 1, step: 778, outputs are 0.0041078306\n",
      "epoch: 1, step: 779, outputs are 0.004089102\n",
      "epoch: 1, step: 780, outputs are 0.004083825\n",
      "epoch: 1, step: 781, outputs are 0.0040726517\n",
      "epoch: 1, step: 782, outputs are 0.004066823\n",
      "epoch: 1, step: 783, outputs are 0.004051772\n",
      "epoch: 1, step: 784, outputs are 0.0040503177\n",
      "epoch: 1, step: 785, outputs are 0.004045836\n",
      "epoch: 1, step: 786, outputs are 0.004021395\n",
      "epoch: 1, step: 787, outputs are 0.0040223766\n",
      "epoch: 1, step: 788, outputs are 0.004015164\n",
      "epoch: 1, step: 789, outputs are 0.0039913584\n",
      "epoch: 1, step: 790, outputs are 0.003986502\n",
      "epoch: 1, step: 791, outputs are 0.0039742826\n",
      "epoch: 1, step: 792, outputs are 0.003969497\n",
      "epoch: 1, step: 793, outputs are 0.003953629\n",
      "epoch: 1, step: 794, outputs are 0.003948356\n",
      "epoch: 1, step: 795, outputs are 0.0039320677\n",
      "epoch: 1, step: 796, outputs are 0.0039244704\n",
      "epoch: 1, step: 797, outputs are 0.003914731\n",
      "epoch: 1, step: 798, outputs are 0.0039090267\n",
      "epoch: 1, step: 799, outputs are 0.003896142\n",
      "epoch: 1, step: 800, outputs are 0.0038890792\n",
      "epoch: 1, step: 801, outputs are 0.0038802752\n",
      "epoch: 1, step: 802, outputs are 0.0038715308\n",
      "epoch: 1, step: 803, outputs are 0.0038640222\n",
      "epoch: 1, step: 804, outputs are 0.0038477187\n",
      "epoch: 1, step: 805, outputs are 0.0038408614\n",
      "epoch: 1, step: 806, outputs are 0.0038316767\n",
      "epoch: 1, step: 807, outputs are 0.0038211984\n",
      "epoch: 1, step: 808, outputs are 0.0038101983\n",
      "epoch: 1, step: 809, outputs are 0.0038011624\n",
      "epoch: 1, step: 810, outputs are 0.0038028345\n",
      "epoch: 1, step: 811, outputs are 0.0037881476\n",
      "epoch: 1, step: 812, outputs are 0.0037794486\n",
      "epoch: 1, step: 813, outputs are 0.003768507\n",
      "epoch: 1, step: 814, outputs are 0.003755984\n",
      "epoch: 1, step: 815, outputs are 0.0037514842\n",
      "epoch: 1, step: 816, outputs are 0.003746418\n",
      "epoch: 1, step: 817, outputs are 0.0037270922\n",
      "epoch: 1, step: 818, outputs are 0.0037235264\n",
      "epoch: 1, step: 819, outputs are 0.0037049842\n",
      "epoch: 1, step: 820, outputs are 0.0037073263\n",
      "epoch: 1, step: 821, outputs are 0.0036931862\n",
      "epoch: 1, step: 822, outputs are 0.003691752\n",
      "epoch: 1, step: 823, outputs are 0.003679581\n",
      "epoch: 1, step: 824, outputs are 0.0036759519\n",
      "epoch: 1, step: 825, outputs are 0.0036622458\n",
      "epoch: 1, step: 826, outputs are 0.0036562365\n",
      "epoch: 1, step: 827, outputs are 0.003644997\n",
      "epoch: 1, step: 828, outputs are 0.0036326502\n",
      "epoch: 1, step: 829, outputs are 0.0036354265\n",
      "epoch: 1, step: 830, outputs are 0.0036149258\n",
      "epoch: 1, step: 831, outputs are 0.003613635\n",
      "epoch: 1, step: 832, outputs are 0.003598294\n",
      "epoch: 1, step: 833, outputs are 0.003592605\n",
      "epoch: 1, step: 834, outputs are 0.0035912087\n",
      "epoch: 1, step: 835, outputs are 0.0035811216\n",
      "epoch: 1, step: 836, outputs are 0.003573047\n",
      "epoch: 1, step: 837, outputs are 0.0035599354\n",
      "epoch: 1, step: 838, outputs are 0.0035473877\n",
      "epoch: 1, step: 839, outputs are 0.0035433075\n",
      "epoch: 1, step: 840, outputs are 0.0035305684\n",
      "epoch: 1, step: 841, outputs are 0.0035301747\n",
      "epoch: 1, step: 842, outputs are 0.0035143816\n",
      "epoch: 1, step: 843, outputs are 0.0035086912\n",
      "epoch: 1, step: 844, outputs are 0.0034993435\n",
      "epoch: 1, step: 845, outputs are 0.0035035093\n",
      "epoch: 1, step: 846, outputs are 0.0034855232\n",
      "epoch: 1, step: 847, outputs are 0.0034835963\n",
      "epoch: 1, step: 848, outputs are 0.0034704558\n",
      "epoch: 1, step: 849, outputs are 0.0034657707\n",
      "epoch: 1, step: 850, outputs are 0.0034576673\n",
      "epoch: 1, step: 851, outputs are 0.0034449315\n",
      "epoch: 1, step: 852, outputs are 0.0034414472\n",
      "epoch: 1, step: 853, outputs are 0.0034291213\n",
      "epoch: 1, step: 854, outputs are 0.0034241898\n",
      "epoch: 1, step: 855, outputs are 0.0034158058\n",
      "epoch: 1, step: 856, outputs are 0.0034049996\n",
      "epoch: 1, step: 857, outputs are 0.003397109\n",
      "epoch: 1, step: 858, outputs are 0.0033886984\n",
      "epoch: 1, step: 859, outputs are 0.003386619\n",
      "epoch: 1, step: 860, outputs are 0.0033755223\n",
      "epoch: 1, step: 861, outputs are 0.0033598975\n",
      "epoch: 1, step: 862, outputs are 0.003359352\n",
      "epoch: 1, step: 863, outputs are 0.0033565017\n",
      "epoch: 1, step: 864, outputs are 0.0033471808\n",
      "epoch: 1, step: 865, outputs are 0.0033373148\n",
      "epoch: 1, step: 866, outputs are 0.0033307082\n",
      "epoch: 1, step: 867, outputs are 0.0033300323\n",
      "epoch: 1, step: 868, outputs are 0.0033164723\n",
      "epoch: 1, step: 869, outputs are 0.0033130352\n",
      "epoch: 1, step: 870, outputs are 0.0032993024\n",
      "epoch: 1, step: 871, outputs are 0.0032971082\n",
      "epoch: 1, step: 872, outputs are 0.0032828401\n",
      "epoch: 1, step: 873, outputs are 0.0032776478\n",
      "epoch: 1, step: 874, outputs are 0.003280842\n",
      "epoch: 1, step: 875, outputs are 0.0032662558\n",
      "epoch: 1, step: 876, outputs are 0.0032561228\n",
      "epoch: 1, step: 877, outputs are 0.003250217\n",
      "epoch: 1, step: 878, outputs are 0.003245858\n",
      "epoch: 1, step: 879, outputs are 0.0032428065\n",
      "epoch: 1, step: 880, outputs are 0.0032310248\n",
      "epoch: 1, step: 881, outputs are 0.0032190224\n",
      "epoch: 1, step: 882, outputs are 0.0032149365\n",
      "epoch: 1, step: 883, outputs are 0.0032122766\n",
      "epoch: 1, step: 884, outputs are 0.0031971945\n",
      "epoch: 1, step: 885, outputs are 0.0031941906\n",
      "epoch: 1, step: 886, outputs are 0.0031841511\n",
      "epoch: 1, step: 887, outputs are 0.0031813998\n",
      "epoch: 1, step: 888, outputs are 0.0031668488\n",
      "epoch: 1, step: 889, outputs are 0.0031684483\n",
      "epoch: 1, step: 890, outputs are 0.0031554857\n",
      "epoch: 1, step: 891, outputs are 0.0031505045\n",
      "epoch: 1, step: 892, outputs are 0.003149229\n",
      "epoch: 1, step: 893, outputs are 0.0031405746\n",
      "epoch: 1, step: 894, outputs are 0.0031248378\n",
      "epoch: 1, step: 895, outputs are 0.0031259572\n",
      "epoch: 1, step: 896, outputs are 0.0031217844\n",
      "epoch: 1, step: 897, outputs are 0.0031138456\n",
      "epoch: 1, step: 898, outputs are 0.0031062108\n",
      "epoch: 1, step: 899, outputs are 0.003100662\n",
      "epoch: 1, step: 900, outputs are 0.003092154\n",
      "epoch: 1, step: 901, outputs are 0.0030788104\n",
      "epoch: 1, step: 902, outputs are 0.0030766674\n",
      "epoch: 1, step: 903, outputs are 0.0030642427\n",
      "epoch: 1, step: 904, outputs are 0.0030621581\n",
      "epoch: 1, step: 905, outputs are 0.0030619253\n",
      "epoch: 1, step: 906, outputs are 0.0030522812\n",
      "epoch: 1, step: 907, outputs are 0.0030470574\n",
      "epoch: 1, step: 908, outputs are 0.0030368154\n",
      "epoch: 1, step: 909, outputs are 0.0030288592\n",
      "epoch: 1, step: 910, outputs are 0.0030214055\n",
      "epoch: 1, step: 911, outputs are 0.0030122558\n",
      "epoch: 1, step: 912, outputs are 0.0030063167\n",
      "epoch: 1, step: 913, outputs are 0.0030054203\n",
      "epoch: 1, step: 914, outputs are 0.0029961676\n",
      "epoch: 1, step: 915, outputs are 0.002990991\n",
      "epoch: 1, step: 916, outputs are 0.0029864397\n",
      "epoch: 1, step: 917, outputs are 0.0029772078\n",
      "epoch: 1, step: 918, outputs are 0.0029740292\n",
      "epoch: 1, step: 919, outputs are 0.0029681989\n",
      "epoch: 1, step: 920, outputs are 0.0029573035\n",
      "epoch: 1, step: 921, outputs are 0.0029488856\n",
      "epoch: 1, step: 922, outputs are 0.0029420457\n",
      "epoch: 1, step: 923, outputs are 0.0029358543\n",
      "epoch: 1, step: 924, outputs are 0.002933612\n",
      "epoch: 1, step: 925, outputs are 0.0029190802\n",
      "epoch: 1, step: 926, outputs are 0.0029175493\n",
      "epoch: 1, step: 927, outputs are 0.002910174\n",
      "epoch: 1, step: 928, outputs are 0.0029087076\n",
      "epoch: 1, step: 929, outputs are 0.0029052696\n",
      "epoch: 1, step: 930, outputs are 0.0028948763\n",
      "epoch: 1, step: 931, outputs are 0.0028921603\n",
      "epoch: 1, step: 932, outputs are 0.0028840702\n",
      "epoch: 1, step: 933, outputs are 0.00287315\n",
      "epoch: 1, step: 934, outputs are 0.0028737306\n",
      "epoch: 1, step: 935, outputs are 0.002865365\n",
      "epoch: 1, step: 936, outputs are 0.0028525349\n",
      "epoch: 1, step: 937, outputs are 0.0028539363\n",
      "epoch: 1, step: 938, outputs are 0.0028488257\n",
      "epoch: 1, step: 939, outputs are 0.0028436705\n",
      "epoch: 1, step: 940, outputs are 0.0028343257\n",
      "epoch: 1, step: 941, outputs are 0.0028254827\n",
      "epoch: 1, step: 942, outputs are 0.002822729\n",
      "epoch: 1, step: 943, outputs are 0.0028131944\n",
      "epoch: 1, step: 944, outputs are 0.0028109036\n",
      "epoch: 1, step: 945, outputs are 0.0028059483\n",
      "epoch: 1, step: 946, outputs are 0.002793081\n",
      "epoch: 1, step: 947, outputs are 0.0027898685\n",
      "epoch: 1, step: 948, outputs are 0.0027846545\n",
      "epoch: 1, step: 949, outputs are 0.002781149\n",
      "epoch: 1, step: 950, outputs are 0.002774708\n",
      "epoch: 1, step: 951, outputs are 0.0027658215\n",
      "epoch: 1, step: 952, outputs are 0.002762356\n",
      "epoch: 1, step: 953, outputs are 0.0027554673\n",
      "epoch: 1, step: 954, outputs are 0.002753638\n",
      "epoch: 1, step: 955, outputs are 0.0027462188\n",
      "epoch: 1, step: 956, outputs are 0.002746376\n",
      "epoch: 1, step: 957, outputs are 0.002734534\n",
      "epoch: 1, step: 958, outputs are 0.002730335\n",
      "epoch: 1, step: 959, outputs are 0.0027202368\n",
      "epoch: 1, step: 960, outputs are 0.002720212\n",
      "epoch: 1, step: 961, outputs are 0.0027141136\n",
      "epoch: 1, step: 962, outputs are 0.0027058888\n",
      "epoch: 1, step: 963, outputs are 0.0027093724\n",
      "epoch: 1, step: 964, outputs are 0.0026955877\n",
      "epoch: 1, step: 965, outputs are 0.0026868768\n",
      "epoch: 1, step: 966, outputs are 0.0026851867\n",
      "epoch: 1, step: 967, outputs are 0.0026787193\n",
      "epoch: 1, step: 968, outputs are 0.0026784341\n",
      "epoch: 1, step: 969, outputs are 0.0026670422\n",
      "epoch: 1, step: 970, outputs are 0.0026612144\n",
      "epoch: 1, step: 971, outputs are 0.0026609174\n",
      "epoch: 1, step: 972, outputs are 0.002652514\n",
      "epoch: 1, step: 973, outputs are 0.0026464574\n",
      "epoch: 1, step: 974, outputs are 0.0026451205\n",
      "epoch: 1, step: 975, outputs are 0.0026322263\n",
      "epoch: 1, step: 976, outputs are 0.0026315334\n",
      "epoch: 1, step: 977, outputs are 0.0026245238\n",
      "epoch: 1, step: 978, outputs are 0.0026185317\n",
      "epoch: 1, step: 979, outputs are 0.00261686\n",
      "epoch: 1, step: 980, outputs are 0.0026062685\n",
      "epoch: 1, step: 981, outputs are 0.0026036114\n",
      "epoch: 1, step: 982, outputs are 0.0025947\n",
      "epoch: 1, step: 983, outputs are 0.002595013\n",
      "epoch: 1, step: 984, outputs are 0.0025892733\n",
      "epoch: 1, step: 985, outputs are 0.0025821866\n",
      "epoch: 1, step: 986, outputs are 0.0025754552\n",
      "epoch: 1, step: 987, outputs are 0.0025694051\n",
      "epoch: 1, step: 988, outputs are 0.002563309\n",
      "epoch: 1, step: 989, outputs are 0.0025607566\n",
      "epoch: 1, step: 990, outputs are 0.0025554453\n",
      "epoch: 1, step: 991, outputs are 0.0025519447\n",
      "epoch: 1, step: 992, outputs are 0.0025467318\n",
      "epoch: 1, step: 993, outputs are 0.0025420175\n",
      "epoch: 1, step: 994, outputs are 0.0025396615\n",
      "epoch: 1, step: 995, outputs are 0.0025326516\n",
      "epoch: 1, step: 996, outputs are 0.002524693\n",
      "epoch: 1, step: 997, outputs are 0.0025219435\n",
      "epoch: 1, step: 998, outputs are 0.0025208031\n",
      "epoch: 1, step: 999, outputs are 0.002511681\n",
      "epoch: 1, step: 1000, outputs are 0.002506624\n",
      "epoch: 1, step: 1001, outputs are 0.0024996945\n",
      "epoch: 1, step: 1002, outputs are 0.0024926534\n",
      "epoch: 1, step: 1003, outputs are 0.0024897736\n",
      "epoch: 1, step: 1004, outputs are 0.0024831332\n",
      "epoch: 1, step: 1005, outputs are 0.0024771853\n",
      "epoch: 1, step: 1006, outputs are 0.0024739238\n",
      "epoch: 1, step: 1007, outputs are 0.0024695718\n",
      "epoch: 1, step: 1008, outputs are 0.0024635445\n",
      "epoch: 1, step: 1009, outputs are 0.0024592152\n",
      "epoch: 1, step: 1010, outputs are 0.0024555419\n",
      "epoch: 1, step: 1011, outputs are 0.0024518725\n",
      "epoch: 1, step: 1012, outputs are 0.0024440358\n",
      "epoch: 1, step: 1013, outputs are 0.0024396498\n",
      "epoch: 1, step: 1014, outputs are 0.0024336183\n",
      "epoch: 1, step: 1015, outputs are 0.0024309077\n",
      "epoch: 1, step: 1016, outputs are 0.0024268012\n",
      "epoch: 1, step: 1017, outputs are 0.0024229172\n",
      "epoch: 1, step: 1018, outputs are 0.002413663\n",
      "epoch: 1, step: 1019, outputs are 0.0024100621\n",
      "epoch: 1, step: 1020, outputs are 0.0024064905\n",
      "epoch: 1, step: 1021, outputs are 0.0023979552\n",
      "epoch: 1, step: 1022, outputs are 0.002398441\n",
      "epoch: 1, step: 1023, outputs are 0.002392526\n",
      "epoch: 1, step: 1024, outputs are 0.0023882922\n",
      "epoch: 1, step: 1025, outputs are 0.0023850165\n",
      "epoch: 1, step: 1026, outputs are 0.0023765562\n",
      "epoch: 1, step: 1027, outputs are 0.0023724372\n",
      "epoch: 1, step: 1028, outputs are 0.0023665132\n",
      "epoch: 1, step: 1029, outputs are 0.002362968\n",
      "epoch: 1, step: 1030, outputs are 0.0023538815\n",
      "epoch: 1, step: 1031, outputs are 0.0023564054\n",
      "epoch: 1, step: 1032, outputs are 0.0023507383\n",
      "epoch: 1, step: 1033, outputs are 0.0023433403\n",
      "epoch: 1, step: 1034, outputs are 0.0023375484\n",
      "epoch: 1, step: 1035, outputs are 0.0023343847\n",
      "epoch: 1, step: 1036, outputs are 0.0023299553\n",
      "epoch: 1, step: 1037, outputs are 0.0023235246\n",
      "epoch: 1, step: 1038, outputs are 0.0023239134\n",
      "epoch: 1, step: 1039, outputs are 0.0023200675\n",
      "epoch: 1, step: 1040, outputs are 0.0023135652\n",
      "epoch: 1, step: 1041, outputs are 0.0023090295\n",
      "epoch: 1, step: 1042, outputs are 0.0023053647\n",
      "epoch: 1, step: 1043, outputs are 0.002299977\n",
      "epoch: 1, step: 1044, outputs are 0.0022978117\n",
      "epoch: 1, step: 1045, outputs are 0.0022943192\n",
      "epoch: 1, step: 1046, outputs are 0.0022900826\n",
      "epoch: 1, step: 1047, outputs are 0.0022874656\n",
      "epoch: 1, step: 1048, outputs are 0.0022802209\n",
      "epoch: 1, step: 1049, outputs are 0.0022770613\n",
      "epoch: 1, step: 1050, outputs are 0.0022727693\n",
      "epoch: 1, step: 1051, outputs are 0.00227055\n",
      "epoch: 1, step: 1052, outputs are 0.0022644685\n",
      "epoch: 1, step: 1053, outputs are 0.002265604\n",
      "epoch: 1, step: 1054, outputs are 0.0022544255\n",
      "epoch: 1, step: 1055, outputs are 0.002255206\n",
      "epoch: 1, step: 1056, outputs are 0.0022489782\n",
      "epoch: 1, step: 1057, outputs are 0.002248895\n",
      "epoch: 1, step: 1058, outputs are 0.0022418932\n",
      "epoch: 1, step: 1059, outputs are 0.0022395062\n",
      "epoch: 1, step: 1060, outputs are 0.0022350752\n",
      "epoch: 1, step: 1061, outputs are 0.0022305795\n",
      "epoch: 1, step: 1062, outputs are 0.0022308896\n",
      "epoch: 1, step: 1063, outputs are 0.0022228938\n",
      "epoch: 1, step: 1064, outputs are 0.002218857\n",
      "epoch: 1, step: 1065, outputs are 0.0022140015\n",
      "epoch: 1, step: 1066, outputs are 0.00221172\n",
      "epoch: 1, step: 1067, outputs are 0.002208835\n",
      "epoch: 1, step: 1068, outputs are 0.0022016135\n",
      "epoch: 1, step: 1069, outputs are 0.0021988486\n",
      "epoch: 1, step: 1070, outputs are 0.002198049\n",
      "epoch: 1, step: 1071, outputs are 0.002193072\n",
      "epoch: 1, step: 1072, outputs are 0.0021893592\n",
      "epoch: 1, step: 1073, outputs are 0.0021862518\n",
      "epoch: 1, step: 1074, outputs are 0.0021817645\n",
      "epoch: 1, step: 1075, outputs are 0.0021800855\n",
      "epoch: 1, step: 1076, outputs are 0.0021749176\n",
      "epoch: 1, step: 1077, outputs are 0.0021718103\n",
      "epoch: 1, step: 1078, outputs are 0.0021653618\n",
      "epoch: 1, step: 1079, outputs are 0.0021605408\n",
      "epoch: 1, step: 1080, outputs are 0.0021618202\n",
      "epoch: 1, step: 1081, outputs are 0.0021500243\n",
      "epoch: 1, step: 1082, outputs are 0.0021473295\n",
      "epoch: 1, step: 1083, outputs are 0.0021435071\n",
      "epoch: 1, step: 1084, outputs are 0.002135895\n",
      "epoch: 1, step: 1085, outputs are 0.0021336158\n",
      "epoch: 1, step: 1086, outputs are 0.002128359\n",
      "epoch: 1, step: 1087, outputs are 0.0021244187\n",
      "epoch: 1, step: 1088, outputs are 0.002117962\n",
      "epoch: 1, step: 1089, outputs are 0.0021150145\n",
      "epoch: 1, step: 1090, outputs are 0.0021080116\n",
      "epoch: 1, step: 1091, outputs are 0.0021068917\n",
      "epoch: 1, step: 1092, outputs are 0.0020983836\n",
      "epoch: 1, step: 1093, outputs are 0.002093125\n",
      "epoch: 1, step: 1094, outputs are 0.002089887\n",
      "epoch: 1, step: 1095, outputs are 0.0020838482\n",
      "epoch: 1, step: 1096, outputs are 0.0020775013\n",
      "epoch: 1, step: 1097, outputs are 0.002074663\n",
      "epoch: 1, step: 1098, outputs are 0.0020704358\n",
      "epoch: 1, step: 1099, outputs are 0.0020664064\n",
      "epoch: 1, step: 1100, outputs are 0.002064893\n",
      "epoch: 1, step: 1101, outputs are 0.002054387\n",
      "epoch: 1, step: 1102, outputs are 0.0020497218\n",
      "epoch: 1, step: 1103, outputs are 0.0020448775\n",
      "epoch: 1, step: 1104, outputs are 0.002045231\n",
      "epoch: 1, step: 1105, outputs are 0.0020335903\n",
      "epoch: 1, step: 1106, outputs are 0.002031394\n",
      "epoch: 1, step: 1107, outputs are 0.002031141\n",
      "epoch: 1, step: 1108, outputs are 0.0020205043\n",
      "epoch: 1, step: 1109, outputs are 0.002021664\n",
      "epoch: 1, step: 1110, outputs are 0.0020179637\n",
      "epoch: 1, step: 1111, outputs are 0.0020122614\n",
      "epoch: 1, step: 1112, outputs are 0.0020073312\n",
      "epoch: 1, step: 1113, outputs are 0.002004504\n",
      "epoch: 1, step: 1114, outputs are 0.0019996143\n",
      "epoch: 1, step: 1115, outputs are 0.0019980725\n",
      "epoch: 1, step: 1116, outputs are 0.0019894065\n",
      "epoch: 1, step: 1117, outputs are 0.0019885332\n",
      "epoch: 1, step: 1118, outputs are 0.001985412\n",
      "epoch: 1, step: 1119, outputs are 0.0019823492\n",
      "epoch: 1, step: 1120, outputs are 0.00197797\n",
      "epoch: 1, step: 1121, outputs are 0.0019739303\n",
      "epoch: 1, step: 1122, outputs are 0.0019646343\n",
      "epoch: 1, step: 1123, outputs are 0.0019643088\n",
      "epoch: 1, step: 1124, outputs are 0.001964837\n",
      "epoch: 1, step: 1125, outputs are 0.001957036\n",
      "epoch: 1, step: 1126, outputs are 0.0019525927\n",
      "epoch: 1, step: 1127, outputs are 0.0019494877\n",
      "epoch: 1, step: 1128, outputs are 0.0019471922\n",
      "epoch: 1, step: 1129, outputs are 0.0019443703\n",
      "epoch: 1, step: 1130, outputs are 0.0019418804\n",
      "epoch: 1, step: 1131, outputs are 0.0019327942\n",
      "epoch: 1, step: 1132, outputs are 0.0019339833\n",
      "epoch: 1, step: 1133, outputs are 0.0019314097\n",
      "epoch: 1, step: 1134, outputs are 0.0019285248\n",
      "epoch: 1, step: 1135, outputs are 0.0019199345\n",
      "epoch: 1, step: 1136, outputs are 0.0019176678\n",
      "epoch: 1, step: 1137, outputs are 0.0019130997\n",
      "epoch: 1, step: 1138, outputs are 0.0019143494\n",
      "epoch: 1, step: 1139, outputs are 0.0019085882\n",
      "epoch: 1, step: 1140, outputs are 0.0019000693\n",
      "epoch: 1, step: 1141, outputs are 0.0018996734\n",
      "epoch: 1, step: 1142, outputs are 0.0018975585\n",
      "epoch: 1, step: 1143, outputs are 0.0018992946\n",
      "epoch: 1, step: 1144, outputs are 0.0018920808\n",
      "epoch: 1, step: 1145, outputs are 0.001888074\n",
      "epoch: 1, step: 1146, outputs are 0.0018862844\n",
      "epoch: 1, step: 1147, outputs are 0.0018818178\n",
      "epoch: 1, step: 1148, outputs are 0.0018803772\n",
      "epoch: 1, step: 1149, outputs are 0.0018766029\n",
      "epoch: 1, step: 1150, outputs are 0.0018711232\n",
      "epoch: 1, step: 1151, outputs are 0.0018701064\n",
      "epoch: 1, step: 1152, outputs are 0.0018651274\n",
      "epoch: 1, step: 1153, outputs are 0.0018671995\n",
      "epoch: 1, step: 1154, outputs are 0.0018567254\n",
      "epoch: 1, step: 1155, outputs are 0.0018603026\n",
      "epoch: 1, step: 1156, outputs are 0.0018541892\n",
      "epoch: 1, step: 1157, outputs are 0.0018512243\n",
      "epoch: 1, step: 1158, outputs are 0.0018480653\n",
      "epoch: 1, step: 1159, outputs are 0.0018446503\n",
      "epoch: 1, step: 1160, outputs are 0.001839842\n",
      "epoch: 1, step: 1161, outputs are 0.0018394503\n",
      "epoch: 1, step: 1162, outputs are 0.001832759\n",
      "epoch: 1, step: 1163, outputs are 0.0018304149\n",
      "epoch: 1, step: 1164, outputs are 0.0018317964\n",
      "epoch: 1, step: 1165, outputs are 0.0018241496\n",
      "epoch: 1, step: 1166, outputs are 0.0018207692\n",
      "epoch: 1, step: 1167, outputs are 0.0018187184\n",
      "epoch: 1, step: 1168, outputs are 0.0018162864\n",
      "epoch: 1, step: 1169, outputs are 0.0018097335\n",
      "epoch: 1, step: 1170, outputs are 0.0018100257\n",
      "epoch: 1, step: 1171, outputs are 0.0018051257\n",
      "epoch: 1, step: 1172, outputs are 0.0018018365\n",
      "epoch: 1, step: 1173, outputs are 0.0018009443\n",
      "epoch: 1, step: 1174, outputs are 0.001798818\n",
      "epoch: 1, step: 1175, outputs are 0.0017936825\n",
      "epoch: 1, step: 1176, outputs are 0.0017919358\n",
      "epoch: 1, step: 1177, outputs are 0.0017863389\n",
      "epoch: 1, step: 1178, outputs are 0.0017859915\n",
      "epoch: 1, step: 1179, outputs are 0.0017795876\n",
      "epoch: 1, step: 1180, outputs are 0.0017788609\n",
      "epoch: 1, step: 1181, outputs are 0.0017745113\n",
      "epoch: 1, step: 1182, outputs are 0.0017730601\n",
      "epoch: 1, step: 1183, outputs are 0.0017679961\n",
      "epoch: 1, step: 1184, outputs are 0.0017659859\n",
      "epoch: 1, step: 1185, outputs are 0.0017606561\n",
      "epoch: 1, step: 1186, outputs are 0.0017615401\n",
      "epoch: 1, step: 1187, outputs are 0.0017551142\n",
      "epoch: 1, step: 1188, outputs are 0.0017543903\n",
      "epoch: 1, step: 1189, outputs are 0.0017493293\n",
      "epoch: 1, step: 1190, outputs are 0.0017481047\n",
      "epoch: 1, step: 1191, outputs are 0.0017438718\n",
      "epoch: 1, step: 1192, outputs are 0.0017388132\n",
      "epoch: 1, step: 1193, outputs are 0.0017377319\n",
      "epoch: 1, step: 1194, outputs are 0.0017350542\n",
      "epoch: 1, step: 1195, outputs are 0.0017318132\n",
      "epoch: 1, step: 1196, outputs are 0.0017262107\n",
      "epoch: 1, step: 1197, outputs are 0.0017254106\n",
      "epoch: 1, step: 1198, outputs are 0.0017229307\n",
      "epoch: 1, step: 1199, outputs are 0.0017186517\n",
      "epoch: 1, step: 1200, outputs are 0.0017219017\n",
      "epoch: 1, step: 1201, outputs are 0.0017130531\n",
      "epoch: 1, step: 1202, outputs are 0.0017106305\n",
      "epoch: 1, step: 1203, outputs are 0.0017091063\n",
      "epoch: 1, step: 1204, outputs are 0.0017054873\n",
      "epoch: 1, step: 1205, outputs are 0.0017036959\n",
      "epoch: 1, step: 1206, outputs are 0.0016994281\n",
      "epoch: 1, step: 1207, outputs are 0.0016990177\n",
      "epoch: 1, step: 1208, outputs are 0.0016931392\n",
      "epoch: 1, step: 1209, outputs are 0.0016909572\n",
      "epoch: 1, step: 1210, outputs are 0.0016871023\n",
      "epoch: 1, step: 1211, outputs are 0.001684096\n",
      "epoch: 1, step: 1212, outputs are 0.0016811271\n",
      "epoch: 1, step: 1213, outputs are 0.0016817979\n",
      "epoch: 1, step: 1214, outputs are 0.0016784164\n",
      "epoch: 1, step: 1215, outputs are 0.0016732876\n",
      "epoch: 1, step: 1216, outputs are 0.0016732726\n",
      "epoch: 1, step: 1217, outputs are 0.0016698341\n",
      "epoch: 1, step: 1218, outputs are 0.0016667667\n",
      "epoch: 1, step: 1219, outputs are 0.0016641857\n",
      "epoch: 1, step: 1220, outputs are 0.0016605894\n",
      "epoch: 1, step: 1221, outputs are 0.0016594194\n",
      "epoch: 1, step: 1222, outputs are 0.0016534328\n",
      "epoch: 1, step: 1223, outputs are 0.0016535176\n",
      "epoch: 1, step: 1224, outputs are 0.0016489546\n",
      "epoch: 1, step: 1225, outputs are 0.001649388\n",
      "epoch: 1, step: 1226, outputs are 0.0016443746\n",
      "epoch: 1, step: 1227, outputs are 0.0016451558\n",
      "epoch: 1, step: 1228, outputs are 0.0016392894\n",
      "epoch: 1, step: 1229, outputs are 0.0016376894\n",
      "epoch: 1, step: 1230, outputs are 0.001634548\n",
      "epoch: 1, step: 1231, outputs are 0.0016297135\n",
      "epoch: 1, step: 1232, outputs are 0.0016306468\n",
      "epoch: 1, step: 1233, outputs are 0.0016281481\n",
      "epoch: 1, step: 1234, outputs are 0.0016256875\n",
      "epoch: 1, step: 1235, outputs are 0.0016202957\n",
      "epoch: 1, step: 1236, outputs are 0.0016191676\n",
      "epoch: 1, step: 1237, outputs are 0.0016183552\n",
      "epoch: 1, step: 1238, outputs are 0.0016153366\n",
      "epoch: 1, step: 1239, outputs are 0.0016124957\n",
      "epoch: 1, step: 1240, outputs are 0.0016089104\n",
      "epoch: 1, step: 1241, outputs are 0.0016044516\n",
      "epoch: 1, step: 1242, outputs are 0.0015999109\n",
      "epoch: 1, step: 1243, outputs are 0.0015961553\n",
      "epoch: 1, step: 1244, outputs are 0.0015959807\n",
      "epoch: 1, step: 1245, outputs are 0.0015917704\n",
      "epoch: 1, step: 1246, outputs are 0.0015899979\n",
      "epoch: 1, step: 1247, outputs are 0.0015854421\n",
      "epoch: 1, step: 1248, outputs are 0.0015846016\n",
      "epoch: 1, step: 1249, outputs are 0.001581862\n",
      "epoch: 1, step: 1250, outputs are 0.0015791771\n",
      "epoch: 1, step: 1251, outputs are 0.0015767091\n",
      "epoch: 1, step: 1252, outputs are 0.0015765331\n",
      "epoch: 1, step: 1253, outputs are 0.0015716621\n",
      "epoch: 1, step: 1254, outputs are 0.0015702124\n",
      "epoch: 1, step: 1255, outputs are 0.0015680795\n",
      "epoch: 1, step: 1256, outputs are 0.0015645479\n",
      "epoch: 1, step: 1257, outputs are 0.0015650538\n",
      "epoch: 1, step: 1258, outputs are 0.001562107\n",
      "epoch: 1, step: 1259, outputs are 0.001557492\n",
      "epoch: 1, step: 1260, outputs are 0.0015562943\n",
      "epoch: 1, step: 1261, outputs are 0.0015540461\n",
      "epoch: 1, step: 1262, outputs are 0.0015529003\n",
      "epoch: 1, step: 1263, outputs are 0.0015480779\n",
      "epoch: 1, step: 1264, outputs are 0.0015490924\n",
      "epoch: 1, step: 1265, outputs are 0.0015464604\n",
      "epoch: 1, step: 1266, outputs are 0.0015445369\n",
      "epoch: 1, step: 1267, outputs are 0.0015388231\n",
      "epoch: 1, step: 1268, outputs are 0.001538462\n",
      "epoch: 1, step: 1269, outputs are 0.0015361084\n",
      "epoch: 1, step: 1270, outputs are 0.0015335503\n",
      "epoch: 1, step: 1271, outputs are 0.0015296738\n",
      "epoch: 1, step: 1272, outputs are 0.0015292881\n",
      "epoch: 1, step: 1273, outputs are 0.001526994\n",
      "epoch: 1, step: 1274, outputs are 0.0015237697\n",
      "epoch: 1, step: 1275, outputs are 0.0015226103\n",
      "epoch: 1, step: 1276, outputs are 0.0015224488\n",
      "epoch: 1, step: 1277, outputs are 0.0015189969\n",
      "epoch: 1, step: 1278, outputs are 0.0015166195\n",
      "epoch: 1, step: 1279, outputs are 0.0015137134\n",
      "epoch: 1, step: 1280, outputs are 0.0015122478\n",
      "epoch: 1, step: 1281, outputs are 0.0015116482\n",
      "epoch: 1, step: 1282, outputs are 0.0015087301\n",
      "epoch: 1, step: 1283, outputs are 0.0015051764\n",
      "epoch: 1, step: 1284, outputs are 0.0015055566\n",
      "epoch: 1, step: 1285, outputs are 0.0014998598\n",
      "epoch: 1, step: 1286, outputs are 0.0014990916\n",
      "epoch: 1, step: 1287, outputs are 0.0014985306\n",
      "epoch: 1, step: 1288, outputs are 0.0014941918\n",
      "epoch: 1, step: 1289, outputs are 0.0014916738\n",
      "epoch: 1, step: 1290, outputs are 0.0014902102\n",
      "epoch: 1, step: 1291, outputs are 0.0014894209\n",
      "epoch: 1, step: 1292, outputs are 0.0014866595\n",
      "epoch: 1, step: 1293, outputs are 0.0014846523\n",
      "epoch: 1, step: 1294, outputs are 0.0014814407\n",
      "epoch: 1, step: 1295, outputs are 0.0014803272\n",
      "epoch: 1, step: 1296, outputs are 0.001476259\n",
      "epoch: 1, step: 1297, outputs are 0.0014779647\n",
      "epoch: 1, step: 1298, outputs are 0.0014736387\n",
      "epoch: 1, step: 1299, outputs are 0.001472234\n",
      "epoch: 1, step: 1300, outputs are 0.0014704919\n",
      "epoch: 1, step: 1301, outputs are 0.0014705587\n",
      "epoch: 1, step: 1302, outputs are 0.0014660998\n",
      "epoch: 1, step: 1303, outputs are 0.0014650464\n",
      "epoch: 1, step: 1304, outputs are 0.0014600363\n",
      "epoch: 1, step: 1305, outputs are 0.0014589109\n",
      "epoch: 1, step: 1306, outputs are 0.0014579175\n",
      "epoch: 1, step: 1307, outputs are 0.0014565329\n",
      "epoch: 1, step: 1308, outputs are 0.0014549817\n",
      "epoch: 1, step: 1309, outputs are 0.0014509812\n",
      "epoch: 1, step: 1310, outputs are 0.0014502471\n",
      "epoch: 1, step: 1311, outputs are 0.001448211\n",
      "epoch: 1, step: 1312, outputs are 0.0014486815\n",
      "epoch: 1, step: 1313, outputs are 0.0014439505\n",
      "epoch: 1, step: 1314, outputs are 0.0014421531\n",
      "epoch: 1, step: 1315, outputs are 0.0014408594\n",
      "epoch: 1, step: 1316, outputs are 0.0014386966\n",
      "epoch: 1, step: 1317, outputs are 0.0014365477\n",
      "epoch: 1, step: 1318, outputs are 0.0014359794\n",
      "epoch: 1, step: 1319, outputs are 0.0014317845\n",
      "epoch: 1, step: 1320, outputs are 0.0014331699\n",
      "epoch: 1, step: 1321, outputs are 0.0014297802\n",
      "epoch: 1, step: 1322, outputs are 0.0014258851\n",
      "epoch: 1, step: 1323, outputs are 0.0014249461\n",
      "epoch: 1, step: 1324, outputs are 0.0014229731\n",
      "epoch: 1, step: 1325, outputs are 0.0014219207\n",
      "epoch: 1, step: 1326, outputs are 0.0014181053\n",
      "epoch: 1, step: 1327, outputs are 0.0014164038\n",
      "epoch: 1, step: 1328, outputs are 0.0014148697\n",
      "epoch: 1, step: 1329, outputs are 0.0014156839\n",
      "epoch: 1, step: 1330, outputs are 0.00141074\n",
      "epoch: 1, step: 1331, outputs are 0.0014093621\n",
      "epoch: 1, step: 1332, outputs are 0.0014092571\n",
      "epoch: 1, step: 1333, outputs are 0.0014086446\n",
      "epoch: 1, step: 1334, outputs are 0.0014038648\n",
      "epoch: 1, step: 1335, outputs are 0.0014050463\n",
      "epoch: 1, step: 1336, outputs are 0.0014021876\n",
      "epoch: 1, step: 1337, outputs are 0.0013985932\n",
      "epoch: 1, step: 1338, outputs are 0.0013971833\n",
      "epoch: 1, step: 1339, outputs are 0.0013949759\n",
      "epoch: 1, step: 1340, outputs are 0.0013953687\n",
      "epoch: 1, step: 1341, outputs are 0.0013926574\n",
      "epoch: 1, step: 1342, outputs are 0.0013897457\n",
      "epoch: 1, step: 1343, outputs are 0.0013867398\n",
      "epoch: 1, step: 1344, outputs are 0.0013864753\n",
      "epoch: 1, step: 1345, outputs are 0.001385708\n",
      "epoch: 1, step: 1346, outputs are 0.0013828091\n",
      "epoch: 1, step: 1347, outputs are 0.0013814331\n",
      "epoch: 1, step: 1348, outputs are 0.0013772029\n",
      "epoch: 1, step: 1349, outputs are 0.0013769949\n",
      "epoch: 1, step: 1350, outputs are 0.001374702\n",
      "epoch: 1, step: 1351, outputs are 0.0013754927\n",
      "epoch: 1, step: 1352, outputs are 0.0013726928\n",
      "epoch: 1, step: 1353, outputs are 0.0013686254\n",
      "epoch: 1, step: 1354, outputs are 0.0013689285\n",
      "epoch: 1, step: 1355, outputs are 0.0013666304\n",
      "epoch: 1, step: 1356, outputs are 0.0013633723\n",
      "epoch: 1, step: 1357, outputs are 0.001364293\n",
      "epoch: 1, step: 1358, outputs are 0.0013630819\n",
      "epoch: 1, step: 1359, outputs are 0.0013611589\n",
      "epoch: 1, step: 1360, outputs are 0.0013566113\n",
      "epoch: 1, step: 1361, outputs are 0.0013564627\n",
      "epoch: 1, step: 1362, outputs are 0.0013558611\n",
      "epoch: 1, step: 1363, outputs are 0.0013545218\n",
      "epoch: 1, step: 1364, outputs are 0.0013500729\n",
      "epoch: 1, step: 1365, outputs are 0.0013491076\n",
      "epoch: 1, step: 1366, outputs are 0.0013490701\n",
      "epoch: 1, step: 1367, outputs are 0.0013467972\n",
      "epoch: 1, step: 1368, outputs are 0.0013419823\n",
      "epoch: 1, step: 1369, outputs are 0.0013440481\n",
      "epoch: 1, step: 1370, outputs are 0.001341823\n",
      "epoch: 1, step: 1371, outputs are 0.0013390656\n",
      "epoch: 1, step: 1372, outputs are 0.0013381593\n",
      "epoch: 1, step: 1373, outputs are 0.0013379613\n",
      "epoch: 1, step: 1374, outputs are 0.0013345421\n",
      "epoch: 1, step: 1375, outputs are 0.0013329387\n",
      "epoch: 1, step: 1376, outputs are 0.0013305759\n",
      "epoch: 1, step: 1377, outputs are 0.001328774\n",
      "epoch: 1, step: 1378, outputs are 0.0013277617\n",
      "epoch: 1, step: 1379, outputs are 0.0013287411\n",
      "epoch: 1, step: 1380, outputs are 0.0013237275\n",
      "epoch: 1, step: 1381, outputs are 0.0013236916\n",
      "epoch: 1, step: 1382, outputs are 0.0013215207\n",
      "epoch: 1, step: 1383, outputs are 0.001320351\n",
      "epoch: 1, step: 1384, outputs are 0.0013173328\n",
      "epoch: 1, step: 1385, outputs are 0.001316203\n",
      "epoch: 1, step: 1386, outputs are 0.0013166498\n",
      "epoch: 1, step: 1387, outputs are 0.0013140423\n",
      "epoch: 1, step: 1388, outputs are 0.0013113872\n",
      "epoch: 1, step: 1389, outputs are 0.0013102633\n",
      "epoch: 1, step: 1390, outputs are 0.0013083619\n",
      "epoch: 1, step: 1391, outputs are 0.0013061701\n",
      "epoch: 1, step: 1392, outputs are 0.0013051573\n",
      "epoch: 1, step: 1393, outputs are 0.0013050612\n",
      "epoch: 1, step: 1394, outputs are 0.0013034394\n",
      "epoch: 1, step: 1395, outputs are 0.0012985165\n",
      "epoch: 1, step: 1396, outputs are 0.0013004589\n",
      "epoch: 1, step: 1397, outputs are 0.0012976173\n",
      "epoch: 1, step: 1398, outputs are 0.0012952539\n",
      "epoch: 1, step: 1399, outputs are 0.0012943547\n",
      "epoch: 1, step: 1400, outputs are 0.001293396\n",
      "epoch: 1, step: 1401, outputs are 0.0012906137\n",
      "epoch: 1, step: 1402, outputs are 0.0012902282\n",
      "epoch: 1, step: 1403, outputs are 0.0012882224\n",
      "epoch: 1, step: 1404, outputs are 0.0012864805\n",
      "epoch: 1, step: 1405, outputs are 0.0012863239\n",
      "epoch: 1, step: 1406, outputs are 0.0012848065\n",
      "epoch: 1, step: 1407, outputs are 0.0012804528\n",
      "epoch: 1, step: 1408, outputs are 0.0012814901\n",
      "epoch: 1, step: 1409, outputs are 0.0012775783\n",
      "epoch: 1, step: 1410, outputs are 0.0012766306\n",
      "epoch: 1, step: 1411, outputs are 0.0012749855\n",
      "epoch: 1, step: 1412, outputs are 0.0012753964\n",
      "epoch: 1, step: 1413, outputs are 0.0012743818\n",
      "epoch: 1, step: 1414, outputs are 0.0012717862\n",
      "epoch: 1, step: 1415, outputs are 0.0012685908\n",
      "epoch: 1, step: 1416, outputs are 0.0012692406\n",
      "epoch: 1, step: 1417, outputs are 0.0012668701\n",
      "epoch: 1, step: 1418, outputs are 0.0012639386\n",
      "epoch: 1, step: 1419, outputs are 0.0012643333\n",
      "epoch: 1, step: 1420, outputs are 0.0012618687\n",
      "epoch: 1, step: 1421, outputs are 0.0012601027\n",
      "epoch: 1, step: 1422, outputs are 0.0012579708\n",
      "epoch: 1, step: 1423, outputs are 0.0012586925\n",
      "epoch: 1, step: 1424, outputs are 0.0012547364\n",
      "epoch: 1, step: 1425, outputs are 0.0012538603\n",
      "epoch: 1, step: 1426, outputs are 0.0012511428\n",
      "epoch: 1, step: 1427, outputs are 0.001250199\n",
      "epoch: 1, step: 1428, outputs are 0.0012487352\n",
      "epoch: 1, step: 1429, outputs are 0.001249835\n",
      "epoch: 1, step: 1430, outputs are 0.0012483299\n",
      "epoch: 1, step: 1431, outputs are 0.001246528\n",
      "epoch: 1, step: 1432, outputs are 0.0012446583\n",
      "epoch: 1, step: 1433, outputs are 0.0012434288\n",
      "epoch: 1, step: 1434, outputs are 0.0012407079\n",
      "epoch: 1, step: 1435, outputs are 0.0012387312\n",
      "epoch: 1, step: 1436, outputs are 0.0012392452\n",
      "epoch: 1, step: 1437, outputs are 0.0012355791\n",
      "epoch: 1, step: 1438, outputs are 0.0012356603\n",
      "epoch: 1, step: 1439, outputs are 0.0012341411\n",
      "epoch: 1, step: 1440, outputs are 0.001234536\n",
      "epoch: 1, step: 1441, outputs are 0.0012309136\n",
      "epoch: 1, step: 1442, outputs are 0.0012305959\n",
      "epoch: 1, step: 1443, outputs are 0.0012287812\n",
      "epoch: 1, step: 1444, outputs are 0.0012291531\n",
      "epoch: 1, step: 1445, outputs are 0.001224071\n",
      "epoch: 1, step: 1446, outputs are 0.0012252007\n",
      "epoch: 1, step: 1447, outputs are 0.0012244356\n",
      "epoch: 1, step: 1448, outputs are 0.0012218349\n",
      "epoch: 1, step: 1449, outputs are 0.0012196472\n",
      "epoch: 1, step: 1450, outputs are 0.0012186463\n",
      "epoch: 1, step: 1451, outputs are 0.0012167564\n",
      "epoch: 1, step: 1452, outputs are 0.0012149368\n",
      "epoch: 1, step: 1453, outputs are 0.0012156998\n",
      "epoch: 1, step: 1454, outputs are 0.0012133532\n",
      "epoch: 1, step: 1455, outputs are 0.0012120575\n",
      "epoch: 1, step: 1456, outputs are 0.001207914\n",
      "epoch: 1, step: 1457, outputs are 0.0012093303\n",
      "epoch: 1, step: 1458, outputs are 0.0012088623\n",
      "epoch: 1, step: 1459, outputs are 0.0012056688\n",
      "epoch: 1, step: 1460, outputs are 0.0012036138\n",
      "epoch: 1, step: 1461, outputs are 0.0012039926\n",
      "epoch: 1, step: 1462, outputs are 0.0012016627\n",
      "epoch: 1, step: 1463, outputs are 0.0011991367\n",
      "epoch: 1, step: 1464, outputs are 0.0011998422\n",
      "epoch: 1, step: 1465, outputs are 0.0011985435\n",
      "epoch: 1, step: 1466, outputs are 0.0011950394\n",
      "epoch: 1, step: 1467, outputs are 0.001194589\n",
      "epoch: 1, step: 1468, outputs are 0.0011937472\n",
      "epoch: 1, step: 1469, outputs are 0.0011935055\n",
      "epoch: 1, step: 1470, outputs are 0.0011921007\n",
      "epoch: 1, step: 1471, outputs are 0.001190842\n",
      "epoch: 1, step: 1472, outputs are 0.001188771\n",
      "epoch: 1, step: 1473, outputs are 0.0011864349\n",
      "epoch: 1, step: 1474, outputs are 0.0011853806\n",
      "epoch: 1, step: 1475, outputs are 0.0011854466\n",
      "epoch: 1, step: 1476, outputs are 0.0011828579\n",
      "epoch: 1, step: 1477, outputs are 0.0011819635\n",
      "epoch: 1, step: 1478, outputs are 0.001179983\n",
      "epoch: 1, step: 1479, outputs are 0.0011795948\n",
      "epoch: 1, step: 1480, outputs are 0.001178174\n",
      "epoch: 1, step: 1481, outputs are 0.0011783766\n",
      "epoch: 1, step: 1482, outputs are 0.0011741987\n",
      "epoch: 1, step: 1483, outputs are 0.0011746115\n",
      "epoch: 1, step: 1484, outputs are 0.0011736968\n",
      "epoch: 1, step: 1485, outputs are 0.0011720587\n",
      "epoch: 1, step: 1486, outputs are 0.0011701175\n",
      "epoch: 1, step: 1487, outputs are 0.0011681516\n",
      "epoch: 1, step: 1488, outputs are 0.0011679921\n",
      "epoch: 1, step: 1489, outputs are 0.0011664726\n",
      "epoch: 1, step: 1490, outputs are 0.0011637758\n",
      "epoch: 1, step: 1491, outputs are 0.0011629506\n",
      "epoch: 1, step: 1492, outputs are 0.0011616378\n",
      "epoch: 1, step: 1493, outputs are 0.0011622948\n",
      "epoch: 1, step: 1494, outputs are 0.0011609913\n",
      "epoch: 1, step: 1495, outputs are 0.0011578328\n",
      "epoch: 1, step: 1496, outputs are 0.0011564546\n",
      "epoch: 1, step: 1497, outputs are 0.0011573208\n",
      "epoch: 1, step: 1498, outputs are 0.001154529\n",
      "epoch: 1, step: 1499, outputs are 0.0011539217\n",
      "epoch: 1, step: 1500, outputs are 0.001150937\n",
      "epoch: 1, step: 1501, outputs are 0.0011517857\n",
      "epoch: 1, step: 1502, outputs are 0.001151243\n",
      "epoch: 1, step: 1503, outputs are 0.0011493273\n",
      "epoch: 1, step: 1504, outputs are 0.0011462334\n",
      "epoch: 1, step: 1505, outputs are 0.0011462832\n",
      "epoch: 1, step: 1506, outputs are 0.0011446694\n",
      "epoch: 1, step: 1507, outputs are 0.0011434206\n",
      "epoch: 1, step: 1508, outputs are 0.0011418597\n",
      "epoch: 1, step: 1509, outputs are 0.0011399784\n",
      "epoch: 1, step: 1510, outputs are 0.0011373329\n",
      "epoch: 1, step: 1511, outputs are 0.0011385137\n",
      "epoch: 1, step: 1512, outputs are 0.0011371169\n",
      "epoch: 1, step: 1513, outputs are 0.0011361834\n",
      "epoch: 1, step: 1514, outputs are 0.0011359374\n",
      "epoch: 1, step: 1515, outputs are 0.0011347287\n",
      "epoch: 1, step: 1516, outputs are 0.0011318725\n",
      "epoch: 1, step: 1517, outputs are 0.0011324512\n",
      "epoch: 1, step: 1518, outputs are 0.0011290514\n",
      "epoch: 1, step: 1519, outputs are 0.0011302641\n",
      "epoch: 1, step: 1520, outputs are 0.001127053\n",
      "epoch: 1, step: 1521, outputs are 0.0011267313\n",
      "epoch: 1, step: 1522, outputs are 0.0011247356\n",
      "epoch: 1, step: 1523, outputs are 0.0011234315\n",
      "epoch: 1, step: 1524, outputs are 0.0011223202\n",
      "epoch: 1, step: 1525, outputs are 0.001120619\n",
      "epoch: 1, step: 1526, outputs are 0.0011208612\n",
      "epoch: 1, step: 1527, outputs are 0.0011189694\n",
      "epoch: 1, step: 1528, outputs are 0.0011175892\n",
      "epoch: 1, step: 1529, outputs are 0.0011166028\n",
      "epoch: 1, step: 1530, outputs are 0.0011147368\n",
      "epoch: 1, step: 1531, outputs are 0.001113398\n",
      "epoch: 1, step: 1532, outputs are 0.0011133496\n",
      "epoch: 1, step: 1533, outputs are 0.0011106424\n",
      "epoch: 1, step: 1534, outputs are 0.0011109894\n",
      "epoch: 1, step: 1535, outputs are 0.0011097244\n",
      "epoch: 1, step: 1536, outputs are 0.0011094738\n",
      "epoch: 1, step: 1537, outputs are 0.0011076002\n",
      "epoch: 1, step: 1538, outputs are 0.0011055315\n",
      "epoch: 1, step: 1539, outputs are 0.0011050183\n",
      "epoch: 1, step: 1540, outputs are 0.0011032874\n",
      "epoch: 1, step: 1541, outputs are 0.0011017728\n",
      "epoch: 1, step: 1542, outputs are 0.0011008726\n",
      "epoch: 1, step: 1543, outputs are 0.0011008567\n",
      "epoch: 1, step: 1544, outputs are 0.0010981397\n",
      "epoch: 1, step: 1545, outputs are 0.001097962\n",
      "epoch: 1, step: 1546, outputs are 0.0010976172\n",
      "epoch: 1, step: 1547, outputs are 0.0010955199\n",
      "epoch: 1, step: 1548, outputs are 0.0010950866\n",
      "epoch: 1, step: 1549, outputs are 0.0010927175\n",
      "epoch: 1, step: 1550, outputs are 0.0010913623\n",
      "epoch: 1, step: 1551, outputs are 0.0010897\n",
      "epoch: 1, step: 1552, outputs are 0.0010905903\n",
      "epoch: 1, step: 1553, outputs are 0.0010869736\n",
      "epoch: 1, step: 1554, outputs are 0.001087906\n",
      "epoch: 1, step: 1555, outputs are 0.0010855595\n",
      "epoch: 1, step: 1556, outputs are 0.0010843056\n",
      "epoch: 1, step: 1557, outputs are 0.0010847584\n",
      "epoch: 1, step: 1558, outputs are 0.001083683\n",
      "epoch: 1, step: 1559, outputs are 0.0010828192\n",
      "epoch: 1, step: 1560, outputs are 0.0010807626\n",
      "epoch: 1, step: 1561, outputs are 0.0010808897\n",
      "epoch: 1, step: 1562, outputs are 0.0010800236\n",
      "epoch: 1, step: 1563, outputs are 0.0010773747\n",
      "epoch: 1, step: 1564, outputs are 0.0010745828\n",
      "epoch: 1, step: 1565, outputs are 0.001075286\n",
      "epoch: 1, step: 1566, outputs are 0.0010732119\n",
      "epoch: 1, step: 1567, outputs are 0.0010731446\n",
      "epoch: 1, step: 1568, outputs are 0.0010719136\n",
      "epoch: 1, step: 1569, outputs are 0.0010704884\n",
      "epoch: 1, step: 1570, outputs are 0.0010687679\n",
      "epoch: 1, step: 1571, outputs are 0.0010671788\n",
      "epoch: 1, step: 1572, outputs are 0.001067412\n",
      "epoch: 1, step: 1573, outputs are 0.0010662454\n",
      "epoch: 1, step: 1574, outputs are 0.0010651962\n",
      "epoch: 1, step: 1575, outputs are 0.0010658089\n",
      "epoch: 1, step: 1576, outputs are 0.0010631925\n",
      "epoch: 1, step: 1577, outputs are 0.0010625196\n",
      "epoch: 1, step: 1578, outputs are 0.0010607619\n",
      "epoch: 1, step: 1579, outputs are 0.0010603371\n",
      "epoch: 1, step: 1580, outputs are 0.0010587126\n",
      "epoch: 1, step: 1581, outputs are 0.0010558614\n",
      "epoch: 1, step: 1582, outputs are 0.001056032\n",
      "epoch: 1, step: 1583, outputs are 0.0010551987\n",
      "epoch: 1, step: 1584, outputs are 0.0010538326\n",
      "epoch: 1, step: 1585, outputs are 0.0010553023\n",
      "epoch: 1, step: 1586, outputs are 0.0010513265\n",
      "epoch: 1, step: 1587, outputs are 0.0010518137\n",
      "epoch: 1, step: 1588, outputs are 0.0010503489\n",
      "epoch: 1, step: 1589, outputs are 0.0010470573\n",
      "epoch: 1, step: 1590, outputs are 0.0010461307\n",
      "epoch: 1, step: 1591, outputs are 0.0010458841\n",
      "epoch: 1, step: 1592, outputs are 0.0010457631\n",
      "epoch: 1, step: 1593, outputs are 0.0010441625\n",
      "epoch: 1, step: 1594, outputs are 0.0010433032\n",
      "epoch: 1, step: 1595, outputs are 0.0010425798\n",
      "epoch: 1, step: 1596, outputs are 0.0010415942\n",
      "epoch: 1, step: 1597, outputs are 0.001041146\n",
      "epoch: 1, step: 1598, outputs are 0.0010399127\n",
      "epoch: 1, step: 1599, outputs are 0.0010382815\n",
      "epoch: 1, step: 1600, outputs are 0.0010360564\n",
      "epoch: 1, step: 1601, outputs are 0.0010371249\n",
      "epoch: 1, step: 1602, outputs are 0.001034171\n",
      "epoch: 1, step: 1603, outputs are 0.001035287\n",
      "epoch: 1, step: 1604, outputs are 0.0010324388\n",
      "epoch: 1, step: 1605, outputs are 0.0010328378\n",
      "epoch: 1, step: 1606, outputs are 0.001031314\n",
      "epoch: 1, step: 1607, outputs are 0.0010280216\n",
      "epoch: 1, step: 1608, outputs are 0.0010293085\n",
      "epoch: 1, step: 1609, outputs are 0.0010284637\n",
      "epoch: 1, step: 1610, outputs are 0.0010267701\n",
      "epoch: 1, step: 1611, outputs are 0.0010249257\n",
      "epoch: 1, step: 1612, outputs are 0.0010256651\n",
      "epoch: 1, step: 1613, outputs are 0.0010231617\n",
      "epoch: 1, step: 1614, outputs are 0.0010226023\n",
      "epoch: 1, step: 1615, outputs are 0.0010218341\n",
      "epoch: 1, step: 1616, outputs are 0.0010213018\n",
      "epoch: 1, step: 1617, outputs are 0.00101878\n",
      "epoch: 1, step: 1618, outputs are 0.0010180041\n",
      "epoch: 1, step: 1619, outputs are 0.0010183852\n",
      "epoch: 1, step: 1620, outputs are 0.0010166374\n",
      "epoch: 1, step: 1621, outputs are 0.001015253\n",
      "epoch: 1, step: 1622, outputs are 0.0010138962\n",
      "epoch: 1, step: 1623, outputs are 0.0010145875\n",
      "epoch: 1, step: 1624, outputs are 0.0010128698\n",
      "epoch: 1, step: 1625, outputs are 0.0010116701\n",
      "epoch: 1, step: 1626, outputs are 0.0010093115\n",
      "epoch: 1, step: 1627, outputs are 0.001010092\n",
      "epoch: 1, step: 1628, outputs are 0.0010077778\n",
      "epoch: 1, step: 1629, outputs are 0.0010078591\n",
      "epoch: 1, step: 1630, outputs are 0.0010069986\n",
      "epoch: 1, step: 1631, outputs are 0.0010051753\n",
      "epoch: 1, step: 1632, outputs are 0.0010061435\n",
      "epoch: 1, step: 1633, outputs are 0.0010039982\n",
      "epoch: 1, step: 1634, outputs are 0.001002515\n",
      "epoch: 1, step: 1635, outputs are 0.0010031123\n",
      "epoch: 1, step: 1636, outputs are 0.0010003063\n",
      "epoch: 1, step: 1637, outputs are 0.0009987304\n",
      "epoch: 1, step: 1638, outputs are 0.0009988354\n",
      "epoch: 1, step: 1639, outputs are 0.0009980418\n",
      "epoch: 1, step: 1640, outputs are 0.0009965748\n",
      "epoch: 1, step: 1641, outputs are 0.0009963663\n",
      "epoch: 1, step: 1642, outputs are 0.0009934306\n",
      "epoch: 1, step: 1643, outputs are 0.0009942183\n",
      "epoch: 1, step: 1644, outputs are 0.000991554\n",
      "epoch: 1, step: 1645, outputs are 0.0009916357\n",
      "epoch: 1, step: 1646, outputs are 0.000991302\n",
      "epoch: 1, step: 1647, outputs are 0.0009888331\n",
      "epoch: 1, step: 1648, outputs are 0.0009901086\n",
      "epoch: 1, step: 1649, outputs are 0.0009877903\n",
      "epoch: 1, step: 1650, outputs are 0.0009880831\n",
      "epoch: 1, step: 1651, outputs are 0.0009869903\n",
      "epoch: 1, step: 1652, outputs are 0.0009850629\n",
      "epoch: 1, step: 1653, outputs are 0.000983565\n",
      "epoch: 1, step: 1654, outputs are 0.0009831465\n",
      "epoch: 1, step: 1655, outputs are 0.0009820123\n",
      "epoch: 1, step: 1656, outputs are 0.0009808708\n",
      "epoch: 1, step: 1657, outputs are 0.0009807742\n",
      "epoch: 1, step: 1658, outputs are 0.0009799773\n",
      "epoch: 1, step: 1659, outputs are 0.0009790442\n",
      "epoch: 1, step: 1660, outputs are 0.0009771357\n",
      "epoch: 1, step: 1661, outputs are 0.0009756533\n",
      "epoch: 1, step: 1662, outputs are 0.00097598427\n",
      "epoch: 1, step: 1663, outputs are 0.0009738122\n",
      "epoch: 1, step: 1664, outputs are 0.0009725002\n",
      "epoch: 1, step: 1665, outputs are 0.000973151\n",
      "epoch: 1, step: 1666, outputs are 0.000972346\n",
      "epoch: 1, step: 1667, outputs are 0.00097073603\n",
      "epoch: 1, step: 1668, outputs are 0.00097068737\n",
      "epoch: 1, step: 1669, outputs are 0.00097006373\n",
      "epoch: 1, step: 1670, outputs are 0.0009677367\n",
      "epoch: 1, step: 1671, outputs are 0.00096621795\n",
      "epoch: 1, step: 1672, outputs are 0.0009671503\n",
      "epoch: 1, step: 1673, outputs are 0.0009649868\n",
      "epoch: 1, step: 1674, outputs are 0.00096552935\n",
      "epoch: 1, step: 1675, outputs are 0.00096381805\n",
      "epoch: 1, step: 1676, outputs are 0.0009624463\n",
      "epoch: 1, step: 1677, outputs are 0.00096199196\n",
      "epoch: 1, step: 1678, outputs are 0.0009613696\n",
      "epoch: 1, step: 1679, outputs are 0.0009597084\n",
      "epoch: 1, step: 1680, outputs are 0.0009592344\n",
      "epoch: 1, step: 1681, outputs are 0.0009577115\n",
      "epoch: 1, step: 1682, outputs are 0.00095849886\n",
      "epoch: 1, step: 1683, outputs are 0.0009575013\n",
      "epoch: 1, step: 1684, outputs are 0.00095422874\n",
      "epoch: 1, step: 1685, outputs are 0.0009533061\n",
      "epoch: 1, step: 1686, outputs are 0.00095340423\n",
      "epoch: 1, step: 1687, outputs are 0.0009519976\n",
      "epoch: 1, step: 1688, outputs are 0.0009510445\n",
      "epoch: 1, step: 1689, outputs are 0.0009510829\n",
      "epoch: 1, step: 1690, outputs are 0.00094925775\n",
      "epoch: 1, step: 1691, outputs are 0.00094976614\n",
      "epoch: 1, step: 1692, outputs are 0.00094757514\n",
      "epoch: 1, step: 1693, outputs are 0.0009477636\n",
      "epoch: 1, step: 1694, outputs are 0.00094521476\n",
      "epoch: 1, step: 1695, outputs are 0.00094446685\n",
      "epoch: 1, step: 1696, outputs are 0.0009440575\n",
      "epoch: 1, step: 1697, outputs are 0.0009422952\n",
      "epoch: 1, step: 1698, outputs are 0.00094307517\n",
      "epoch: 1, step: 1699, outputs are 0.00094167446\n",
      "epoch: 1, step: 1700, outputs are 0.00094147475\n",
      "epoch: 1, step: 1701, outputs are 0.0009402451\n",
      "epoch: 1, step: 1702, outputs are 0.0009395812\n",
      "epoch: 1, step: 1703, outputs are 0.00093836605\n",
      "epoch: 1, step: 1704, outputs are 0.00093703886\n",
      "epoch: 1, step: 1705, outputs are 0.0009367812\n",
      "epoch: 1, step: 1706, outputs are 0.0009361518\n",
      "epoch: 1, step: 1707, outputs are 0.0009342034\n",
      "epoch: 1, step: 1708, outputs are 0.0009334225\n",
      "epoch: 1, step: 1709, outputs are 0.0009326767\n",
      "epoch: 1, step: 1710, outputs are 0.0009317695\n",
      "epoch: 1, step: 1711, outputs are 0.0009316549\n",
      "epoch: 1, step: 1712, outputs are 0.00092986296\n",
      "epoch: 1, step: 1713, outputs are 0.000929983\n",
      "epoch: 1, step: 1714, outputs are 0.0009280384\n",
      "epoch: 1, step: 1715, outputs are 0.0009280595\n",
      "epoch: 1, step: 1716, outputs are 0.00092664966\n",
      "epoch: 1, step: 1717, outputs are 0.0009264242\n",
      "epoch: 1, step: 1718, outputs are 0.0009251423\n",
      "epoch: 1, step: 1719, outputs are 0.0009237591\n",
      "epoch: 1, step: 1720, outputs are 0.0009254841\n",
      "epoch: 1, step: 1721, outputs are 0.00092166534\n",
      "epoch: 1, step: 1722, outputs are 0.0009213743\n",
      "epoch: 1, step: 1723, outputs are 0.0009212364\n",
      "epoch: 1, step: 1724, outputs are 0.00092007464\n",
      "epoch: 1, step: 1725, outputs are 0.00091983075\n",
      "epoch: 1, step: 1726, outputs are 0.0009188726\n",
      "epoch: 1, step: 1727, outputs are 0.0009177554\n",
      "epoch: 1, step: 1728, outputs are 0.0009164795\n",
      "epoch: 1, step: 1729, outputs are 0.00091553346\n",
      "epoch: 1, step: 1730, outputs are 0.00091486546\n",
      "epoch: 1, step: 1731, outputs are 0.00091475033\n",
      "epoch: 1, step: 1732, outputs are 0.00091391755\n",
      "epoch: 1, step: 1733, outputs are 0.00091171474\n",
      "epoch: 1, step: 1734, outputs are 0.0009111385\n",
      "epoch: 1, step: 1735, outputs are 0.00090962695\n",
      "epoch: 1, step: 1736, outputs are 0.00090912485\n",
      "epoch: 1, step: 1737, outputs are 0.0009088575\n",
      "epoch: 1, step: 1738, outputs are 0.0009083395\n",
      "epoch: 1, step: 1739, outputs are 0.00090824324\n",
      "epoch: 1, step: 1740, outputs are 0.0009062246\n",
      "epoch: 1, step: 1741, outputs are 0.0009055722\n",
      "epoch: 1, step: 1742, outputs are 0.000903924\n",
      "epoch: 1, step: 1743, outputs are 0.00090416934\n",
      "epoch: 1, step: 1744, outputs are 0.00090345764\n",
      "epoch: 1, step: 1745, outputs are 0.0009029512\n",
      "epoch: 1, step: 1746, outputs are 0.0009026363\n",
      "epoch: 1, step: 1747, outputs are 0.0008999361\n",
      "epoch: 1, step: 1748, outputs are 0.00090007985\n",
      "epoch: 1, step: 1749, outputs are 0.0008984757\n",
      "epoch: 1, step: 1750, outputs are 0.0008994696\n",
      "epoch: 1, step: 1751, outputs are 0.0008977491\n",
      "epoch: 1, step: 1752, outputs are 0.0008960449\n",
      "epoch: 1, step: 1753, outputs are 0.0008959448\n",
      "epoch: 1, step: 1754, outputs are 0.000895573\n",
      "epoch: 1, step: 1755, outputs are 0.00089372013\n",
      "epoch: 1, step: 1756, outputs are 0.0008938534\n",
      "epoch: 1, step: 1757, outputs are 0.000892562\n",
      "epoch: 1, step: 1758, outputs are 0.0008907302\n",
      "epoch: 1, step: 1759, outputs are 0.0008897504\n",
      "epoch: 1, step: 1760, outputs are 0.00088933716\n",
      "epoch: 1, step: 1761, outputs are 0.00088913285\n",
      "epoch: 1, step: 1762, outputs are 0.00088893133\n",
      "epoch: 1, step: 1763, outputs are 0.000886882\n",
      "epoch: 1, step: 1764, outputs are 0.00088606053\n",
      "epoch: 1, step: 1765, outputs are 0.0008868084\n",
      "epoch: 1, step: 1766, outputs are 0.0008850894\n",
      "epoch: 1, step: 1767, outputs are 0.0008855688\n",
      "epoch: 1, step: 1768, outputs are 0.00088326016\n",
      "epoch: 1, step: 1769, outputs are 0.0008827578\n",
      "epoch: 1, step: 1770, outputs are 0.00088232674\n",
      "epoch: 1, step: 1771, outputs are 0.0008811037\n",
      "epoch: 1, step: 1772, outputs are 0.00088122295\n",
      "epoch: 1, step: 1773, outputs are 0.0008798893\n",
      "epoch: 1, step: 1774, outputs are 0.00087839644\n",
      "epoch: 1, step: 1775, outputs are 0.00087793096\n",
      "epoch: 1, step: 1776, outputs are 0.0008777172\n",
      "epoch: 1, step: 1777, outputs are 0.00087584706\n",
      "epoch: 1, step: 1778, outputs are 0.00087611325\n",
      "epoch: 1, step: 1779, outputs are 0.00087442517\n",
      "epoch: 1, step: 1780, outputs are 0.00087374495\n",
      "epoch: 1, step: 1781, outputs are 0.00087373937\n",
      "epoch: 1, step: 1782, outputs are 0.0008735371\n",
      "epoch: 1, step: 1783, outputs are 0.00087125355\n",
      "epoch: 1, step: 1784, outputs are 0.0008724566\n",
      "epoch: 1, step: 1785, outputs are 0.000868982\n",
      "epoch: 1, step: 1786, outputs are 0.0008690864\n",
      "epoch: 1, step: 1787, outputs are 0.00086955517\n",
      "epoch: 1, step: 1788, outputs are 0.0008671968\n",
      "epoch: 1, step: 1789, outputs are 0.00086779357\n",
      "epoch: 1, step: 1790, outputs are 0.000866989\n",
      "epoch: 1, step: 1791, outputs are 0.00086581195\n",
      "epoch: 1, step: 1792, outputs are 0.00086563174\n",
      "epoch: 1, step: 1793, outputs are 0.0008638716\n",
      "epoch: 1, step: 1794, outputs are 0.0008627955\n",
      "epoch: 1, step: 1795, outputs are 0.00086175697\n",
      "epoch: 1, step: 1796, outputs are 0.00086179015\n",
      "epoch: 1, step: 1797, outputs are 0.0008607975\n",
      "epoch: 1, step: 1798, outputs are 0.00086063263\n",
      "epoch: 1, step: 1799, outputs are 0.0008596936\n",
      "epoch: 1, step: 1800, outputs are 0.0008589075\n",
      "epoch: 1, step: 1801, outputs are 0.0008578359\n",
      "epoch: 1, step: 1802, outputs are 0.0008574088\n",
      "epoch: 1, step: 1803, outputs are 0.0008574786\n",
      "epoch: 1, step: 1804, outputs are 0.00085485575\n",
      "epoch: 1, step: 1805, outputs are 0.00085602945\n",
      "epoch: 1, step: 1806, outputs are 0.00085393677\n",
      "epoch: 1, step: 1807, outputs are 0.0008530236\n",
      "epoch: 1, step: 1808, outputs are 0.0008527341\n",
      "epoch: 1, step: 1809, outputs are 0.0008515212\n",
      "epoch: 1, step: 1810, outputs are 0.0008506864\n",
      "epoch: 1, step: 1811, outputs are 0.000849784\n",
      "epoch: 1, step: 1812, outputs are 0.00084920897\n",
      "epoch: 1, step: 1813, outputs are 0.00084881694\n",
      "epoch: 1, step: 1814, outputs are 0.00084786164\n",
      "epoch: 1, step: 1815, outputs are 0.00084817153\n",
      "epoch: 1, step: 1816, outputs are 0.00084734155\n",
      "epoch: 1, step: 1817, outputs are 0.0008451089\n",
      "epoch: 1, step: 1818, outputs are 0.000845808\n",
      "epoch: 1, step: 1819, outputs are 0.00084367685\n",
      "epoch: 1, step: 1820, outputs are 0.0008439409\n",
      "epoch: 1, step: 1821, outputs are 0.0008419673\n",
      "epoch: 1, step: 1822, outputs are 0.0008421029\n",
      "epoch: 1, step: 1823, outputs are 0.00084224273\n",
      "epoch: 1, step: 1824, outputs are 0.00084112305\n",
      "epoch: 1, step: 1825, outputs are 0.0008391989\n",
      "epoch: 1, step: 1826, outputs are 0.0008385068\n",
      "epoch: 1, step: 1827, outputs are 0.0008393873\n",
      "epoch: 1, step: 1828, outputs are 0.0008384525\n",
      "epoch: 1, step: 1829, outputs are 0.0008379638\n",
      "epoch: 1, step: 1830, outputs are 0.0008374354\n",
      "epoch: 1, step: 1831, outputs are 0.00083487073\n",
      "epoch: 1, step: 1832, outputs are 0.0008355919\n",
      "epoch: 1, step: 1833, outputs are 0.0008344437\n",
      "epoch: 1, step: 1834, outputs are 0.0008332839\n",
      "epoch: 1, step: 1835, outputs are 0.00083209295\n",
      "epoch: 1, step: 1836, outputs are 0.00083259976\n",
      "epoch: 1, step: 1837, outputs are 0.00083089794\n",
      "epoch: 1, step: 1838, outputs are 0.00083128107\n",
      "epoch: 1, step: 1839, outputs are 0.0008295716\n",
      "epoch: 1, step: 1840, outputs are 0.00082855654\n",
      "epoch: 1, step: 1841, outputs are 0.00082785403\n",
      "epoch: 1, step: 1842, outputs are 0.00082712306\n",
      "epoch: 1, step: 1843, outputs are 0.0008263694\n",
      "epoch: 1, step: 1844, outputs are 0.000826234\n",
      "epoch: 1, step: 1845, outputs are 0.0008259383\n",
      "epoch: 1, step: 1846, outputs are 0.00082437205\n",
      "epoch: 1, step: 1847, outputs are 0.0008244505\n",
      "epoch: 1, step: 1848, outputs are 0.000823029\n",
      "epoch: 1, step: 1849, outputs are 0.00082318205\n",
      "epoch: 1, step: 1850, outputs are 0.0008217914\n",
      "epoch: 1, step: 1851, outputs are 0.000821523\n",
      "epoch: 1, step: 1852, outputs are 0.0008206627\n",
      "epoch: 1, step: 1853, outputs are 0.0008209336\n",
      "epoch: 1, step: 1854, outputs are 0.0008194881\n",
      "epoch: 1, step: 1855, outputs are 0.00081862864\n",
      "epoch: 1, step: 1856, outputs are 0.00081739645\n",
      "epoch: 1, step: 1857, outputs are 0.00081728835\n",
      "epoch: 1, step: 1858, outputs are 0.00081664394\n",
      "epoch: 1, step: 1859, outputs are 0.0008161641\n",
      "epoch: 1, step: 1860, outputs are 0.00081494835\n",
      "epoch: 1, step: 1861, outputs are 0.00081343285\n",
      "epoch: 1, step: 1862, outputs are 0.0008129599\n",
      "epoch: 1, step: 1863, outputs are 0.0008125097\n",
      "epoch: 1, step: 1864, outputs are 0.00081125443\n",
      "epoch: 1, step: 1865, outputs are 0.0008128262\n",
      "epoch: 1, step: 1866, outputs are 0.000810419\n",
      "epoch: 1, step: 1867, outputs are 0.0008101964\n",
      "epoch: 1, step: 1868, outputs are 0.00080921035\n",
      "epoch: 1, step: 1869, outputs are 0.00080918235\n",
      "epoch: 1, step: 1870, outputs are 0.0008084555\n",
      "epoch: 1, step: 1871, outputs are 0.0008065893\n",
      "epoch: 1, step: 1872, outputs are 0.0008064477\n",
      "epoch: 1, step: 1873, outputs are 0.00080721633\n",
      "epoch: 1, step: 1874, outputs are 0.00080534176\n",
      "epoch: 1, step: 1875, outputs are 0.0008045292\n",
      "epoch: 1, step: 1876, outputs are 0.00080445927\n",
      "epoch: 1, step: 1877, outputs are 0.000803308\n",
      "epoch: 1, step: 1878, outputs are 0.0008019878\n",
      "epoch: 1, step: 1879, outputs are 0.0008015913\n",
      "epoch: 1, step: 1880, outputs are 0.0008011646\n",
      "epoch: 1, step: 1881, outputs are 0.00080020144\n",
      "epoch: 1, step: 1882, outputs are 0.0008002168\n",
      "epoch: 1, step: 1883, outputs are 0.0007997513\n",
      "epoch: 1, step: 1884, outputs are 0.0007981096\n",
      "epoch: 1, step: 1885, outputs are 0.00079786894\n",
      "epoch: 1, step: 1886, outputs are 0.0007966005\n",
      "epoch: 1, step: 1887, outputs are 0.0007958055\n",
      "epoch: 1, step: 1888, outputs are 0.0007964295\n",
      "epoch: 1, step: 1889, outputs are 0.000795287\n",
      "epoch: 1, step: 1890, outputs are 0.0007946831\n",
      "epoch: 1, step: 1891, outputs are 0.00079358264\n",
      "epoch: 1, step: 1892, outputs are 0.0007926072\n",
      "epoch: 1, step: 1893, outputs are 0.0007915476\n",
      "epoch: 1, step: 1894, outputs are 0.0007923164\n",
      "epoch: 1, step: 1895, outputs are 0.0007910699\n",
      "epoch: 1, step: 1896, outputs are 0.0007909368\n",
      "epoch: 1, step: 1897, outputs are 0.00078996876\n",
      "epoch: 1, step: 1898, outputs are 0.0007895406\n",
      "epoch: 1, step: 1899, outputs are 0.0007892003\n",
      "epoch: 1, step: 1900, outputs are 0.00078778766\n",
      "epoch: 1, step: 1901, outputs are 0.00078798423\n",
      "epoch: 1, step: 1902, outputs are 0.0007867892\n",
      "epoch: 1, step: 1903, outputs are 0.0007849013\n",
      "epoch: 1, step: 1904, outputs are 0.00078555394\n",
      "epoch: 1, step: 1905, outputs are 0.0007840781\n",
      "epoch: 1, step: 1906, outputs are 0.0007843216\n",
      "epoch: 1, step: 1907, outputs are 0.0007830963\n",
      "epoch: 1, step: 1908, outputs are 0.0007818596\n",
      "epoch: 1, step: 1909, outputs are 0.00078183535\n",
      "epoch: 1, step: 1910, outputs are 0.00078201166\n",
      "epoch: 1, step: 1911, outputs are 0.0007810469\n",
      "epoch: 1, step: 1912, outputs are 0.0007798419\n",
      "epoch: 1, step: 1913, outputs are 0.0007796436\n",
      "epoch: 1, step: 1914, outputs are 0.00077812385\n",
      "epoch: 1, step: 1915, outputs are 0.00077863317\n",
      "epoch: 1, step: 1916, outputs are 0.000777374\n",
      "epoch: 1, step: 1917, outputs are 0.0007771556\n",
      "epoch: 1, step: 1918, outputs are 0.000776847\n",
      "epoch: 1, step: 1919, outputs are 0.00077573676\n",
      "epoch: 1, step: 1920, outputs are 0.00077649445\n",
      "epoch: 1, step: 1921, outputs are 0.0007749472\n",
      "epoch: 1, step: 1922, outputs are 0.00077372836\n",
      "epoch: 1, step: 1923, outputs are 0.0007730746\n",
      "epoch: 1, step: 1924, outputs are 0.0007725599\n",
      "epoch: 1, step: 1925, outputs are 0.0007718102\n",
      "epoch: 1, step: 1926, outputs are 0.00077080564\n",
      "epoch: 1, step: 1927, outputs are 0.0007705473\n",
      "epoch: 1, step: 1928, outputs are 0.00076957\n",
      "epoch: 1, step: 1929, outputs are 0.0007692524\n",
      "epoch: 1, step: 1930, outputs are 0.0007681082\n",
      "epoch: 1, step: 1931, outputs are 0.00076866185\n",
      "epoch: 1, step: 1932, outputs are 0.00076715124\n",
      "epoch: 1, step: 1933, outputs are 0.00076684856\n",
      "epoch: 1, step: 1934, outputs are 0.00076703227\n",
      "epoch: 1, step: 1935, outputs are 0.0007650589\n",
      "epoch: 1, step: 1936, outputs are 0.00076474505\n",
      "epoch: 1, step: 1937, outputs are 0.0007638026\n",
      "epoch: 1, step: 1938, outputs are 0.0007640654\n",
      "epoch: 1, step: 1939, outputs are 0.00076251186\n",
      "epoch: 1, step: 1940, outputs are 0.0007620421\n",
      "epoch: 1, step: 1941, outputs are 0.0007629746\n",
      "epoch: 1, step: 1942, outputs are 0.0007613999\n",
      "epoch: 1, step: 1943, outputs are 0.00075995026\n",
      "epoch: 1, step: 1944, outputs are 0.0007598204\n",
      "epoch: 1, step: 1945, outputs are 0.000757879\n",
      "epoch: 1, step: 1946, outputs are 0.0007592691\n",
      "epoch: 1, step: 1947, outputs are 0.00075864844\n",
      "epoch: 1, step: 1948, outputs are 0.00075764087\n",
      "epoch: 1, step: 1949, outputs are 0.00075671554\n",
      "epoch: 1, step: 1950, outputs are 0.00075635855\n",
      "epoch: 1, step: 1951, outputs are 0.0007555934\n",
      "epoch: 1, step: 1952, outputs are 0.00075451884\n",
      "epoch: 1, step: 1953, outputs are 0.0007542133\n",
      "epoch: 1, step: 1954, outputs are 0.00075367186\n",
      "epoch: 1, step: 1955, outputs are 0.000753173\n",
      "epoch: 1, step: 1956, outputs are 0.0007525552\n",
      "epoch: 1, step: 1957, outputs are 0.00075200025\n",
      "epoch: 1, step: 1958, outputs are 0.00075135124\n",
      "epoch: 1, step: 1959, outputs are 0.000751171\n",
      "epoch: 1, step: 1960, outputs are 0.0007503083\n",
      "epoch: 1, step: 1961, outputs are 0.00074868253\n",
      "epoch: 1, step: 1962, outputs are 0.00074955483\n",
      "epoch: 1, step: 1963, outputs are 0.00074871705\n",
      "epoch: 1, step: 1964, outputs are 0.0007476043\n",
      "epoch: 1, step: 1965, outputs are 0.0007477483\n",
      "epoch: 1, step: 1966, outputs are 0.00074620475\n",
      "epoch: 1, step: 1967, outputs are 0.0007456142\n",
      "epoch: 1, step: 1968, outputs are 0.0007445883\n",
      "epoch: 1, step: 1969, outputs are 0.0007447265\n",
      "epoch: 1, step: 1970, outputs are 0.00074306346\n",
      "epoch: 1, step: 1971, outputs are 0.00074214453\n",
      "epoch: 1, step: 1972, outputs are 0.00074249215\n",
      "epoch: 1, step: 1973, outputs are 0.00074166694\n",
      "epoch: 1, step: 1974, outputs are 0.0007414144\n",
      "epoch: 1, step: 1975, outputs are 0.00074108725\n",
      "epoch: 1, step: 1976, outputs are 0.00073979964\n",
      "epoch: 1, step: 1977, outputs are 0.00073967595\n",
      "epoch: 1, step: 1978, outputs are 0.0007385145\n",
      "epoch: 1, step: 1979, outputs are 0.00073806837\n",
      "epoch: 1, step: 1980, outputs are 0.0007385218\n",
      "epoch: 1, step: 1981, outputs are 0.000737721\n",
      "epoch: 1, step: 1982, outputs are 0.0007359624\n",
      "epoch: 1, step: 1983, outputs are 0.00073530537\n",
      "epoch: 1, step: 1984, outputs are 0.00073610304\n",
      "epoch: 1, step: 1985, outputs are 0.0007344077\n",
      "epoch: 1, step: 1986, outputs are 0.00073446287\n",
      "epoch: 1, step: 1987, outputs are 0.0007345759\n",
      "epoch: 1, step: 1988, outputs are 0.0007334532\n",
      "epoch: 1, step: 1989, outputs are 0.00073269237\n",
      "epoch: 1, step: 1990, outputs are 0.0007316102\n",
      "epoch: 1, step: 1991, outputs are 0.0007317039\n",
      "epoch: 1, step: 1992, outputs are 0.0007307407\n",
      "epoch: 1, step: 1993, outputs are 0.00073016266\n",
      "epoch: 1, step: 1994, outputs are 0.00072932226\n",
      "epoch: 1, step: 1995, outputs are 0.00072902243\n",
      "epoch: 1, step: 1996, outputs are 0.0007284379\n",
      "epoch: 1, step: 1997, outputs are 0.0007282686\n",
      "epoch: 1, step: 1998, outputs are 0.000727941\n",
      "epoch: 1, step: 1999, outputs are 0.00072651834\n",
      "epoch: 1, step: 2000, outputs are 0.00072627945\n",
      "epoch: 1, step: 2001, outputs are 0.0007268001\n",
      "epoch: 1, step: 2002, outputs are 0.00072564953\n",
      "epoch: 1, step: 2003, outputs are 0.0007234645\n",
      "epoch: 1, step: 2004, outputs are 0.0007237526\n",
      "epoch: 1, step: 2005, outputs are 0.0007238684\n",
      "epoch: 1, step: 2006, outputs are 0.0007222381\n",
      "epoch: 1, step: 2007, outputs are 0.00072211126\n",
      "epoch: 1, step: 2008, outputs are 0.0007206303\n",
      "epoch: 1, step: 2009, outputs are 0.0007213225\n",
      "epoch: 1, step: 2010, outputs are 0.00072034623\n",
      "epoch: 1, step: 2011, outputs are 0.00071931013\n",
      "epoch: 1, step: 2012, outputs are 0.0007189344\n",
      "epoch: 1, step: 2013, outputs are 0.00071841304\n",
      "epoch: 1, step: 2014, outputs are 0.00071744726\n",
      "epoch: 1, step: 2015, outputs are 0.0007175193\n",
      "epoch: 1, step: 2016, outputs are 0.00071687385\n",
      "epoch: 1, step: 2017, outputs are 0.00071578217\n",
      "epoch: 1, step: 2018, outputs are 0.0007149609\n",
      "epoch: 1, step: 2019, outputs are 0.00071555143\n",
      "epoch: 1, step: 2020, outputs are 0.0007138661\n",
      "epoch: 1, step: 2021, outputs are 0.0007131867\n",
      "epoch: 1, step: 2022, outputs are 0.0007134101\n",
      "epoch: 1, step: 2023, outputs are 0.00071311364\n",
      "epoch: 1, step: 2024, outputs are 0.0007119648\n",
      "epoch: 1, step: 2025, outputs are 0.00071159913\n",
      "epoch: 1, step: 2026, outputs are 0.0007113584\n",
      "epoch: 1, step: 2027, outputs are 0.0007105409\n",
      "epoch: 1, step: 2028, outputs are 0.0007103961\n",
      "epoch: 1, step: 2029, outputs are 0.0007100472\n",
      "epoch: 1, step: 2030, outputs are 0.00070830155\n",
      "epoch: 1, step: 2031, outputs are 0.00070862053\n",
      "epoch: 1, step: 2032, outputs are 0.00070833927\n",
      "epoch: 1, step: 2033, outputs are 0.00070751156\n",
      "epoch: 1, step: 2034, outputs are 0.0007073233\n",
      "epoch: 1, step: 2035, outputs are 0.0007054417\n",
      "epoch: 1, step: 2036, outputs are 0.0007061104\n",
      "epoch: 1, step: 2037, outputs are 0.00070527964\n",
      "epoch: 1, step: 2038, outputs are 0.0007039146\n",
      "epoch: 1, step: 2039, outputs are 0.0007049974\n",
      "epoch: 1, step: 2040, outputs are 0.00070443214\n",
      "epoch: 1, step: 2041, outputs are 0.00070280966\n",
      "epoch: 1, step: 2042, outputs are 0.00070289086\n",
      "epoch: 1, step: 2043, outputs are 0.00070122024\n",
      "epoch: 1, step: 2044, outputs are 0.0007007162\n",
      "epoch: 1, step: 2045, outputs are 0.0007002093\n",
      "epoch: 1, step: 2046, outputs are 0.0007009476\n",
      "epoch: 1, step: 2047, outputs are 0.0006998092\n",
      "epoch: 1, step: 2048, outputs are 0.0006991281\n",
      "epoch: 1, step: 2049, outputs are 0.0006996194\n",
      "epoch: 1, step: 2050, outputs are 0.00069792976\n",
      "epoch: 1, step: 2051, outputs are 0.00069735\n",
      "epoch: 1, step: 2052, outputs are 0.0006981698\n",
      "epoch: 1, step: 2053, outputs are 0.0006959628\n",
      "epoch: 1, step: 2054, outputs are 0.0006965735\n",
      "epoch: 1, step: 2055, outputs are 0.00069515157\n",
      "epoch: 1, step: 2056, outputs are 0.000695042\n",
      "epoch: 1, step: 2057, outputs are 0.00069433707\n",
      "epoch: 1, step: 2058, outputs are 0.0006940786\n",
      "epoch: 1, step: 2059, outputs are 0.00069340563\n",
      "epoch: 1, step: 2060, outputs are 0.0006925316\n",
      "epoch: 1, step: 2061, outputs are 0.0006928647\n",
      "epoch: 1, step: 2062, outputs are 0.00069220876\n",
      "epoch: 1, step: 2063, outputs are 0.00069123984\n",
      "epoch: 1, step: 2064, outputs are 0.00069002056\n",
      "epoch: 1, step: 2065, outputs are 0.00069073867\n",
      "epoch: 1, step: 2066, outputs are 0.00068889116\n",
      "epoch: 1, step: 2067, outputs are 0.0006883321\n",
      "epoch: 1, step: 2068, outputs are 0.00068773807\n",
      "epoch: 1, step: 2069, outputs are 0.0006867468\n",
      "epoch: 1, step: 2070, outputs are 0.00068696484\n",
      "epoch: 1, step: 2071, outputs are 0.0006868205\n",
      "epoch: 1, step: 2072, outputs are 0.0006858947\n",
      "epoch: 1, step: 2073, outputs are 0.00068537955\n",
      "epoch: 1, step: 2074, outputs are 0.0006851476\n",
      "epoch: 1, step: 2075, outputs are 0.000685512\n",
      "epoch: 1, step: 2076, outputs are 0.0006840682\n",
      "epoch: 1, step: 2077, outputs are 0.0006822349\n",
      "epoch: 1, step: 2078, outputs are 0.0006827634\n",
      "epoch: 1, step: 2079, outputs are 0.00068240706\n",
      "epoch: 1, step: 2080, outputs are 0.000682315\n",
      "epoch: 1, step: 2081, outputs are 0.0006808896\n",
      "epoch: 1, step: 2082, outputs are 0.000681076\n",
      "epoch: 1, step: 2083, outputs are 0.00068034366\n",
      "epoch: 1, step: 2084, outputs are 0.0006795111\n",
      "epoch: 1, step: 2085, outputs are 0.0006786814\n",
      "epoch: 1, step: 2086, outputs are 0.0006788905\n",
      "epoch: 1, step: 2087, outputs are 0.00067794137\n",
      "epoch: 1, step: 2088, outputs are 0.0006779445\n",
      "epoch: 1, step: 2089, outputs are 0.00067745865\n",
      "epoch: 1, step: 2090, outputs are 0.0006761976\n",
      "epoch: 1, step: 2091, outputs are 0.000676289\n",
      "epoch: 1, step: 2092, outputs are 0.0006765107\n",
      "epoch: 1, step: 2093, outputs are 0.00067496253\n",
      "epoch: 1, step: 2094, outputs are 0.00067510107\n",
      "epoch: 1, step: 2095, outputs are 0.0006747259\n",
      "epoch: 1, step: 2096, outputs are 0.00067284645\n",
      "epoch: 1, step: 2097, outputs are 0.00067435915\n",
      "epoch: 1, step: 2098, outputs are 0.0006721017\n",
      "epoch: 1, step: 2099, outputs are 0.00067148264\n",
      "epoch: 1, step: 2100, outputs are 0.00067149\n",
      "epoch: 1, step: 2101, outputs are 0.0006703909\n",
      "epoch: 1, step: 2102, outputs are 0.00067118544\n",
      "epoch: 1, step: 2103, outputs are 0.00067051983\n",
      "epoch: 1, step: 2104, outputs are 0.0006693611\n",
      "epoch: 1, step: 2105, outputs are 0.000669035\n",
      "epoch: 1, step: 2106, outputs are 0.0006680684\n",
      "epoch: 1, step: 2107, outputs are 0.00066764274\n",
      "epoch: 1, step: 2108, outputs are 0.000667259\n",
      "epoch: 1, step: 2109, outputs are 0.00066685013\n",
      "epoch: 1, step: 2110, outputs are 0.0006663931\n",
      "epoch: 1, step: 2111, outputs are 0.0006655333\n",
      "epoch: 1, step: 2112, outputs are 0.0006653502\n",
      "epoch: 1, step: 2113, outputs are 0.00066581613\n",
      "epoch: 1, step: 2114, outputs are 0.0006645239\n",
      "epoch: 1, step: 2115, outputs are 0.0006642521\n",
      "epoch: 1, step: 2116, outputs are 0.0006635622\n",
      "epoch: 1, step: 2117, outputs are 0.0006633849\n",
      "epoch: 1, step: 2118, outputs are 0.0006626776\n",
      "epoch: 1, step: 2119, outputs are 0.0006630015\n",
      "epoch: 1, step: 2120, outputs are 0.000661363\n",
      "epoch: 1, step: 2121, outputs are 0.00066125975\n",
      "epoch: 1, step: 2122, outputs are 0.0006608424\n",
      "epoch: 1, step: 2123, outputs are 0.0006599564\n",
      "epoch: 1, step: 2124, outputs are 0.00065944623\n",
      "epoch: 1, step: 2125, outputs are 0.0006580071\n",
      "epoch: 1, step: 2126, outputs are 0.00065842294\n",
      "epoch: 1, step: 2127, outputs are 0.00065789267\n",
      "epoch: 1, step: 2128, outputs are 0.00065737695\n",
      "epoch: 1, step: 2129, outputs are 0.0006570554\n",
      "epoch: 1, step: 2130, outputs are 0.00065677497\n",
      "epoch: 1, step: 2131, outputs are 0.0006560722\n",
      "epoch: 1, step: 2132, outputs are 0.00065582397\n",
      "epoch: 1, step: 2133, outputs are 0.000655137\n",
      "epoch: 1, step: 2134, outputs are 0.0006544741\n",
      "epoch: 1, step: 2135, outputs are 0.000654023\n",
      "epoch: 1, step: 2136, outputs are 0.00065349456\n",
      "epoch: 1, step: 2137, outputs are 0.0006534646\n",
      "epoch: 1, step: 2138, outputs are 0.00065211556\n",
      "epoch: 1, step: 2139, outputs are 0.0006518844\n",
      "epoch: 1, step: 2140, outputs are 0.0006522306\n",
      "epoch: 1, step: 2141, outputs are 0.00065087026\n",
      "epoch: 1, step: 2142, outputs are 0.0006502145\n",
      "epoch: 1, step: 2143, outputs are 0.00065014046\n",
      "epoch: 1, step: 2144, outputs are 0.0006500918\n",
      "epoch: 1, step: 2145, outputs are 0.0006489045\n",
      "epoch: 1, step: 2146, outputs are 0.0006488043\n",
      "epoch: 1, step: 2147, outputs are 0.00064800907\n",
      "epoch: 1, step: 2148, outputs are 0.00064761704\n",
      "epoch: 1, step: 2149, outputs are 0.0006468024\n",
      "epoch: 1, step: 2150, outputs are 0.0006462545\n",
      "epoch: 1, step: 2151, outputs are 0.0006465784\n",
      "epoch: 1, step: 2152, outputs are 0.00064544706\n",
      "epoch: 1, step: 2153, outputs are 0.000644712\n",
      "epoch: 1, step: 2154, outputs are 0.0006444495\n",
      "epoch: 1, step: 2155, outputs are 0.00064481003\n",
      "epoch: 1, step: 2156, outputs are 0.00064410106\n",
      "epoch: 1, step: 2157, outputs are 0.0006432995\n",
      "epoch: 1, step: 2158, outputs are 0.0006428774\n",
      "epoch: 1, step: 2159, outputs are 0.00064245216\n",
      "epoch: 1, step: 2160, outputs are 0.0006427318\n",
      "epoch: 1, step: 2161, outputs are 0.0006418067\n",
      "epoch: 1, step: 2162, outputs are 0.0006409306\n",
      "epoch: 1, step: 2163, outputs are 0.0006409759\n",
      "epoch: 1, step: 2164, outputs are 0.00063965353\n",
      "epoch: 1, step: 2165, outputs are 0.0006395102\n",
      "epoch: 1, step: 2166, outputs are 0.0006389789\n",
      "epoch: 1, step: 2167, outputs are 0.0006381322\n",
      "epoch: 1, step: 2168, outputs are 0.0006378151\n",
      "epoch: 1, step: 2169, outputs are 0.0006379251\n",
      "epoch: 1, step: 2170, outputs are 0.0006373699\n",
      "epoch: 1, step: 2171, outputs are 0.00063784997\n",
      "epoch: 1, step: 2172, outputs are 0.00063667377\n",
      "epoch: 1, step: 2173, outputs are 0.00063657673\n",
      "epoch: 1, step: 2174, outputs are 0.0006355565\n",
      "epoch: 1, step: 2175, outputs are 0.0006355604\n",
      "epoch: 1, step: 2176, outputs are 0.00063490676\n",
      "epoch: 1, step: 2177, outputs are 0.0006341722\n",
      "epoch: 1, step: 2178, outputs are 0.000633841\n",
      "epoch: 1, step: 2179, outputs are 0.00063331914\n",
      "epoch: 1, step: 2180, outputs are 0.00063322985\n",
      "epoch: 1, step: 2181, outputs are 0.0006329643\n",
      "epoch: 1, step: 2182, outputs are 0.000630761\n",
      "epoch: 1, step: 2183, outputs are 0.00063128205\n",
      "epoch: 1, step: 2184, outputs are 0.00063159707\n",
      "epoch: 1, step: 2185, outputs are 0.0006300617\n",
      "epoch: 1, step: 2186, outputs are 0.00062958745\n",
      "epoch: 1, step: 2187, outputs are 0.00062958477\n",
      "epoch: 1, step: 2188, outputs are 0.0006285353\n",
      "epoch: 1, step: 2189, outputs are 0.0006288969\n",
      "epoch: 1, step: 2190, outputs are 0.0006283679\n",
      "epoch: 1, step: 2191, outputs are 0.00062702387\n",
      "epoch: 1, step: 2192, outputs are 0.00062718743\n",
      "epoch: 1, step: 2193, outputs are 0.00062640314\n",
      "epoch: 1, step: 2194, outputs are 0.00062611053\n",
      "epoch: 1, step: 2195, outputs are 0.00062549434\n",
      "epoch: 1, step: 2196, outputs are 0.00062457065\n",
      "epoch: 1, step: 2197, outputs are 0.0006249392\n",
      "epoch: 1, step: 2198, outputs are 0.0006241065\n",
      "epoch: 1, step: 2199, outputs are 0.00062351034\n",
      "epoch: 1, step: 2200, outputs are 0.0006235279\n",
      "epoch: 1, step: 2201, outputs are 0.00062281615\n",
      "epoch: 1, step: 2202, outputs are 0.00062181853\n",
      "epoch: 1, step: 2203, outputs are 0.0006216584\n",
      "epoch: 1, step: 2204, outputs are 0.0006216796\n",
      "epoch: 1, step: 2205, outputs are 0.0006214468\n",
      "epoch: 1, step: 2206, outputs are 0.00062030065\n",
      "epoch: 1, step: 2207, outputs are 0.00061996013\n",
      "epoch: 1, step: 2208, outputs are 0.000619708\n",
      "epoch: 1, step: 2209, outputs are 0.0006189947\n",
      "epoch: 1, step: 2210, outputs are 0.0006189911\n",
      "epoch: 1, step: 2211, outputs are 0.00061873073\n",
      "epoch: 1, step: 2212, outputs are 0.00061828736\n",
      "epoch: 1, step: 2213, outputs are 0.0006167857\n",
      "epoch: 1, step: 2214, outputs are 0.00061720074\n",
      "epoch: 1, step: 2215, outputs are 0.00061650656\n",
      "epoch: 1, step: 2216, outputs are 0.0006161852\n",
      "epoch: 1, step: 2217, outputs are 0.0006156968\n",
      "epoch: 1, step: 2218, outputs are 0.00061535253\n",
      "epoch: 1, step: 2219, outputs are 0.0006151232\n",
      "epoch: 1, step: 2220, outputs are 0.0006145892\n",
      "epoch: 1, step: 2221, outputs are 0.000614492\n",
      "epoch: 1, step: 2222, outputs are 0.00061318366\n",
      "epoch: 1, step: 2223, outputs are 0.00061291904\n",
      "epoch: 1, step: 2224, outputs are 0.0006127494\n",
      "epoch: 1, step: 2225, outputs are 0.00061157154\n",
      "epoch: 1, step: 2226, outputs are 0.0006115381\n",
      "epoch: 1, step: 2227, outputs are 0.00061086874\n",
      "epoch: 1, step: 2228, outputs are 0.0006108703\n",
      "epoch: 1, step: 2229, outputs are 0.0006106383\n",
      "epoch: 1, step: 2230, outputs are 0.00061072037\n",
      "epoch: 1, step: 2231, outputs are 0.0006093722\n",
      "epoch: 1, step: 2232, outputs are 0.00060953316\n",
      "epoch: 1, step: 2233, outputs are 0.00060848956\n",
      "epoch: 1, step: 2234, outputs are 0.0006089614\n",
      "epoch: 1, step: 2235, outputs are 0.00060810335\n",
      "epoch: 1, step: 2236, outputs are 0.0006077419\n",
      "epoch: 1, step: 2237, outputs are 0.0006069086\n",
      "epoch: 1, step: 2238, outputs are 0.0006061101\n",
      "epoch: 1, step: 2239, outputs are 0.0006062295\n",
      "epoch: 1, step: 2240, outputs are 0.0006051515\n",
      "epoch: 1, step: 2241, outputs are 0.00060542044\n",
      "epoch: 1, step: 2242, outputs are 0.0006048094\n",
      "epoch: 1, step: 2243, outputs are 0.00060486165\n",
      "epoch: 1, step: 2244, outputs are 0.00060351985\n",
      "epoch: 1, step: 2245, outputs are 0.0006040718\n",
      "epoch: 1, step: 2246, outputs are 0.0006034088\n",
      "epoch: 1, step: 2247, outputs are 0.0006026419\n",
      "epoch: 1, step: 2248, outputs are 0.00060245645\n",
      "epoch: 1, step: 2249, outputs are 0.00060181436\n",
      "epoch: 1, step: 2250, outputs are 0.00060118636\n",
      "epoch: 1, step: 2251, outputs are 0.00060085056\n",
      "epoch: 1, step: 2252, outputs are 0.0006006079\n",
      "epoch: 1, step: 2253, outputs are 0.00060001283\n",
      "epoch: 1, step: 2254, outputs are 0.00059968187\n",
      "epoch: 1, step: 2255, outputs are 0.00059900386\n",
      "epoch: 1, step: 2256, outputs are 0.0005986539\n",
      "epoch: 1, step: 2257, outputs are 0.00059882074\n",
      "epoch: 1, step: 2258, outputs are 0.00059847\n",
      "epoch: 1, step: 2259, outputs are 0.00059772376\n",
      "epoch: 1, step: 2260, outputs are 0.0005966766\n",
      "epoch: 1, step: 2261, outputs are 0.0005967431\n",
      "epoch: 1, step: 2262, outputs are 0.0005966379\n",
      "epoch: 1, step: 2263, outputs are 0.00059587223\n",
      "epoch: 1, step: 2264, outputs are 0.00059592514\n",
      "epoch: 1, step: 2265, outputs are 0.000595159\n",
      "epoch: 1, step: 2266, outputs are 0.0005945148\n",
      "epoch: 1, step: 2267, outputs are 0.0005936504\n",
      "epoch: 1, step: 2268, outputs are 0.0005935627\n",
      "epoch: 1, step: 2269, outputs are 0.00059387233\n",
      "epoch: 1, step: 2270, outputs are 0.0005926905\n",
      "epoch: 1, step: 2271, outputs are 0.0005918577\n",
      "epoch: 1, step: 2272, outputs are 0.0005926443\n",
      "epoch: 1, step: 2273, outputs are 0.0005919461\n",
      "epoch: 1, step: 2274, outputs are 0.0005910717\n",
      "epoch: 1, step: 2275, outputs are 0.00059025956\n",
      "epoch: 1, step: 2276, outputs are 0.0005901604\n",
      "epoch: 1, step: 2277, outputs are 0.0005896273\n",
      "epoch: 1, step: 2278, outputs are 0.00058961473\n",
      "epoch: 1, step: 2279, outputs are 0.0005891691\n",
      "epoch: 1, step: 2280, outputs are 0.00058861397\n",
      "epoch: 1, step: 2281, outputs are 0.00058855955\n",
      "epoch: 1, step: 2282, outputs are 0.0005882344\n",
      "epoch: 1, step: 2283, outputs are 0.00058746024\n",
      "epoch: 1, step: 2284, outputs are 0.0005869904\n",
      "epoch: 1, step: 2285, outputs are 0.0005869672\n",
      "epoch: 1, step: 2286, outputs are 0.00058662385\n",
      "epoch: 1, step: 2287, outputs are 0.0005853667\n",
      "epoch: 1, step: 2288, outputs are 0.000585851\n",
      "epoch: 1, step: 2289, outputs are 0.0005842928\n",
      "epoch: 1, step: 2290, outputs are 0.0005840974\n",
      "epoch: 1, step: 2291, outputs are 0.00058425823\n",
      "epoch: 1, step: 2292, outputs are 0.0005833279\n",
      "epoch: 1, step: 2293, outputs are 0.00058262015\n",
      "epoch: 1, step: 2294, outputs are 0.0005827217\n",
      "epoch: 1, step: 2295, outputs are 0.0005822381\n",
      "epoch: 1, step: 2296, outputs are 0.00058208563\n",
      "epoch: 1, step: 2297, outputs are 0.00058130483\n",
      "epoch: 1, step: 2298, outputs are 0.0005805991\n",
      "epoch: 1, step: 2299, outputs are 0.0005812396\n",
      "epoch: 1, step: 2300, outputs are 0.0005809901\n",
      "epoch: 1, step: 2301, outputs are 0.0005803574\n",
      "epoch: 1, step: 2302, outputs are 0.00057919195\n",
      "epoch: 1, step: 2303, outputs are 0.0005791457\n",
      "epoch: 1, step: 2304, outputs are 0.0005783555\n",
      "epoch: 1, step: 2305, outputs are 0.000578225\n",
      "epoch: 1, step: 2306, outputs are 0.000577495\n",
      "epoch: 1, step: 2307, outputs are 0.00057737867\n",
      "epoch: 1, step: 2308, outputs are 0.00057715696\n",
      "epoch: 1, step: 2309, outputs are 0.0005764519\n",
      "epoch: 1, step: 2310, outputs are 0.00057650753\n",
      "epoch: 1, step: 2311, outputs are 0.00057607325\n",
      "epoch: 1, step: 2312, outputs are 0.00057586463\n",
      "epoch: 1, step: 2313, outputs are 0.00057554094\n",
      "epoch: 1, step: 2314, outputs are 0.0005752852\n",
      "epoch: 1, step: 2315, outputs are 0.00057409116\n",
      "epoch: 1, step: 2316, outputs are 0.00057358545\n",
      "epoch: 1, step: 2317, outputs are 0.00057352794\n",
      "epoch: 1, step: 2318, outputs are 0.00057339884\n",
      "epoch: 1, step: 2319, outputs are 0.0005724588\n",
      "epoch: 1, step: 2320, outputs are 0.0005721324\n",
      "epoch: 1, step: 2321, outputs are 0.0005718684\n",
      "epoch: 1, step: 2322, outputs are 0.000571219\n",
      "epoch: 1, step: 2323, outputs are 0.0005709239\n",
      "epoch: 1, step: 2324, outputs are 0.0005715259\n",
      "epoch: 1, step: 2325, outputs are 0.0005710881\n",
      "epoch: 1, step: 2326, outputs are 0.0005696386\n",
      "epoch: 1, step: 2327, outputs are 0.0005689735\n",
      "epoch: 1, step: 2328, outputs are 0.0005690836\n",
      "epoch: 1, step: 2329, outputs are 0.0005685525\n",
      "epoch: 1, step: 2330, outputs are 0.00056765584\n",
      "epoch: 1, step: 2331, outputs are 0.0005687543\n",
      "epoch: 1, step: 2332, outputs are 0.0005677646\n",
      "epoch: 1, step: 2333, outputs are 0.0005662578\n",
      "epoch: 1, step: 2334, outputs are 0.00056751404\n",
      "epoch: 1, step: 2335, outputs are 0.000566717\n",
      "epoch: 1, step: 2336, outputs are 0.00056499883\n",
      "epoch: 1, step: 2337, outputs are 0.0005655387\n",
      "epoch: 1, step: 2338, outputs are 0.00056485843\n",
      "epoch: 1, step: 2339, outputs are 0.0005649593\n",
      "epoch: 1, step: 2340, outputs are 0.0005645663\n",
      "epoch: 1, step: 2341, outputs are 0.00056353403\n",
      "epoch: 1, step: 2342, outputs are 0.00056373246\n",
      "epoch: 1, step: 2343, outputs are 0.00056306156\n",
      "epoch: 1, step: 2344, outputs are 0.0005623031\n",
      "epoch: 1, step: 2345, outputs are 0.0005627399\n",
      "epoch: 1, step: 2346, outputs are 0.00056235865\n",
      "epoch: 1, step: 2347, outputs are 0.0005621\n",
      "epoch: 1, step: 2348, outputs are 0.0005613521\n",
      "epoch: 1, step: 2349, outputs are 0.0005607784\n",
      "epoch: 1, step: 2350, outputs are 0.0005606189\n",
      "epoch: 1, step: 2351, outputs are 0.0005601221\n",
      "epoch: 1, step: 2352, outputs are 0.0005605364\n",
      "epoch: 1, step: 2353, outputs are 0.0005598197\n",
      "epoch: 1, step: 2354, outputs are 0.0005590866\n",
      "epoch: 1, step: 2355, outputs are 0.0005583811\n",
      "epoch: 1, step: 2356, outputs are 0.00055842055\n",
      "epoch: 1, step: 2357, outputs are 0.00055761816\n",
      "epoch: 1, step: 2358, outputs are 0.0005577932\n",
      "epoch: 1, step: 2359, outputs are 0.00055663317\n",
      "epoch: 1, step: 2360, outputs are 0.00055672857\n",
      "epoch: 1, step: 2361, outputs are 0.00055571506\n",
      "epoch: 1, step: 2362, outputs are 0.00055607024\n",
      "epoch: 1, step: 2363, outputs are 0.00055480807\n",
      "epoch: 1, step: 2364, outputs are 0.0005554469\n",
      "epoch: 1, step: 2365, outputs are 0.0005551668\n",
      "epoch: 1, step: 2366, outputs are 0.0005545743\n",
      "epoch: 1, step: 2367, outputs are 0.00055441435\n",
      "epoch: 1, step: 2368, outputs are 0.0005539154\n",
      "epoch: 1, step: 2369, outputs are 0.00055296975\n",
      "epoch: 1, step: 2370, outputs are 0.0005535112\n",
      "epoch: 1, step: 2371, outputs are 0.00055270083\n",
      "epoch: 1, step: 2372, outputs are 0.00055203843\n",
      "epoch: 1, step: 2373, outputs are 0.00055250415\n",
      "epoch: 1, step: 2374, outputs are 0.000551666\n",
      "epoch: 1, step: 2375, outputs are 0.0005504047\n",
      "epoch: 1, step: 2376, outputs are 0.00055141037\n",
      "epoch: 1, step: 2377, outputs are 0.0005496684\n",
      "epoch: 1, step: 2378, outputs are 0.00054997794\n",
      "epoch: 1, step: 2379, outputs are 0.0005500362\n",
      "epoch: 1, step: 2380, outputs are 0.0005495928\n",
      "epoch: 1, step: 2381, outputs are 0.0005492417\n",
      "epoch: 1, step: 2382, outputs are 0.0005480504\n",
      "epoch: 1, step: 2383, outputs are 0.0005476478\n",
      "epoch: 1, step: 2384, outputs are 0.0005478852\n",
      "epoch: 1, step: 2385, outputs are 0.0005474577\n",
      "epoch: 1, step: 2386, outputs are 0.00054742605\n",
      "epoch: 1, step: 2387, outputs are 0.00054640137\n",
      "epoch: 1, step: 2388, outputs are 0.0005456626\n",
      "epoch: 1, step: 2389, outputs are 0.0005456212\n",
      "epoch: 1, step: 2390, outputs are 0.0005456114\n",
      "epoch: 1, step: 2391, outputs are 0.00054508034\n",
      "epoch: 1, step: 2392, outputs are 0.00054475607\n",
      "epoch: 1, step: 2393, outputs are 0.0005441309\n",
      "epoch: 1, step: 2394, outputs are 0.00054398243\n",
      "epoch: 1, step: 2395, outputs are 0.0005443974\n",
      "epoch: 1, step: 2396, outputs are 0.0005432632\n",
      "epoch: 1, step: 2397, outputs are 0.0005430818\n",
      "epoch: 1, step: 2398, outputs are 0.0005419789\n",
      "epoch: 1, step: 2399, outputs are 0.00054260215\n",
      "epoch: 1, step: 2400, outputs are 0.00054234976\n",
      "epoch: 1, step: 2401, outputs are 0.0005414061\n",
      "epoch: 1, step: 2402, outputs are 0.0005414691\n",
      "epoch: 1, step: 2403, outputs are 0.00054151413\n",
      "epoch: 1, step: 2404, outputs are 0.0005404237\n",
      "epoch: 1, step: 2405, outputs are 0.0005402107\n",
      "epoch: 1, step: 2406, outputs are 0.00053938176\n",
      "epoch: 1, step: 2407, outputs are 0.00053903135\n",
      "epoch: 1, step: 2408, outputs are 0.0005385034\n",
      "epoch: 1, step: 2409, outputs are 0.0005387088\n",
      "epoch: 1, step: 2410, outputs are 0.00053729175\n",
      "epoch: 1, step: 2411, outputs are 0.00053773494\n",
      "epoch: 1, step: 2412, outputs are 0.0005377027\n",
      "epoch: 1, step: 2413, outputs are 0.0005374156\n",
      "epoch: 1, step: 2414, outputs are 0.0005367026\n",
      "epoch: 1, step: 2415, outputs are 0.00053658395\n",
      "epoch: 1, step: 2416, outputs are 0.00053617544\n",
      "epoch: 1, step: 2417, outputs are 0.0005356371\n",
      "epoch: 1, step: 2418, outputs are 0.000535507\n",
      "epoch: 1, step: 2419, outputs are 0.00053503027\n",
      "epoch: 1, step: 2420, outputs are 0.00053457054\n",
      "epoch: 1, step: 2421, outputs are 0.0005341545\n",
      "epoch: 1, step: 2422, outputs are 0.0005335445\n",
      "epoch: 1, step: 2423, outputs are 0.00053357147\n",
      "epoch: 1, step: 2424, outputs are 0.0005333356\n",
      "epoch: 1, step: 2425, outputs are 0.0005333215\n",
      "epoch: 1, step: 2426, outputs are 0.0005323006\n",
      "epoch: 1, step: 2427, outputs are 0.000532338\n",
      "epoch: 1, step: 2428, outputs are 0.00053157617\n",
      "epoch: 1, step: 2429, outputs are 0.00053114846\n",
      "epoch: 1, step: 2430, outputs are 0.0005322669\n",
      "epoch: 1, step: 2431, outputs are 0.000530916\n",
      "epoch: 1, step: 2432, outputs are 0.0005304258\n",
      "epoch: 1, step: 2433, outputs are 0.0005307596\n",
      "epoch: 1, step: 2434, outputs are 0.00052949466\n",
      "epoch: 1, step: 2435, outputs are 0.00052959844\n",
      "epoch: 1, step: 2436, outputs are 0.0005281372\n",
      "epoch: 1, step: 2437, outputs are 0.00052865833\n",
      "epoch: 1, step: 2438, outputs are 0.0005281703\n",
      "epoch: 1, step: 2439, outputs are 0.00052752066\n",
      "epoch: 1, step: 2440, outputs are 0.00052696274\n",
      "epoch: 1, step: 2441, outputs are 0.00052706583\n",
      "epoch: 1, step: 2442, outputs are 0.0005273036\n",
      "epoch: 1, step: 2443, outputs are 0.0005266903\n",
      "epoch: 1, step: 2444, outputs are 0.00052623457\n",
      "epoch: 1, step: 2445, outputs are 0.0005267549\n",
      "epoch: 1, step: 2446, outputs are 0.00052489317\n",
      "epoch: 1, step: 2447, outputs are 0.0005249657\n",
      "epoch: 1, step: 2448, outputs are 0.0005245483\n",
      "epoch: 1, step: 2449, outputs are 0.0005239877\n",
      "epoch: 1, step: 2450, outputs are 0.00052379735\n",
      "epoch: 1, step: 2451, outputs are 0.0005239767\n",
      "epoch: 1, step: 2452, outputs are 0.00052314583\n",
      "epoch: 1, step: 2453, outputs are 0.00052337314\n",
      "epoch: 1, step: 2454, outputs are 0.00052206893\n",
      "epoch: 1, step: 2455, outputs are 0.0005220573\n",
      "epoch: 1, step: 2456, outputs are 0.00052255235\n",
      "epoch: 1, step: 2457, outputs are 0.0005209211\n",
      "epoch: 1, step: 2458, outputs are 0.00052127894\n",
      "epoch: 1, step: 2459, outputs are 0.0005210787\n",
      "epoch: 1, step: 2460, outputs are 0.000520322\n",
      "epoch: 1, step: 2461, outputs are 0.0005206845\n",
      "epoch: 1, step: 2462, outputs are 0.0005201318\n",
      "epoch: 1, step: 2463, outputs are 0.000519125\n",
      "epoch: 1, step: 2464, outputs are 0.0005194263\n",
      "epoch: 1, step: 2465, outputs are 0.0005191718\n",
      "epoch: 1, step: 2466, outputs are 0.0005184298\n",
      "epoch: 1, step: 2467, outputs are 0.00051824533\n",
      "epoch: 1, step: 2468, outputs are 0.00051727245\n",
      "epoch: 1, step: 2469, outputs are 0.00051761203\n",
      "epoch: 1, step: 2470, outputs are 0.0005173566\n",
      "epoch: 1, step: 2471, outputs are 0.00051691907\n",
      "epoch: 1, step: 2472, outputs are 0.0005164485\n",
      "epoch: 1, step: 2473, outputs are 0.0005160633\n",
      "epoch: 1, step: 2474, outputs are 0.0005151145\n",
      "epoch: 1, step: 2475, outputs are 0.0005155948\n",
      "epoch: 1, step: 2476, outputs are 0.00051540043\n",
      "epoch: 1, step: 2477, outputs are 0.00051452173\n",
      "epoch: 1, step: 2478, outputs are 0.00051429623\n",
      "epoch: 1, step: 2479, outputs are 0.00051402964\n",
      "epoch: 1, step: 2480, outputs are 0.00051369774\n",
      "epoch: 1, step: 2481, outputs are 0.00051271485\n",
      "epoch: 1, step: 2482, outputs are 0.00051318004\n",
      "epoch: 1, step: 2483, outputs are 0.0005122088\n",
      "epoch: 1, step: 2484, outputs are 0.0005117633\n",
      "epoch: 1, step: 2485, outputs are 0.0005118048\n",
      "epoch: 1, step: 2486, outputs are 0.00051201874\n",
      "epoch: 1, step: 2487, outputs are 0.0005107475\n",
      "epoch: 1, step: 2488, outputs are 0.00051117525\n",
      "epoch: 1, step: 2489, outputs are 0.00051077746\n",
      "epoch: 1, step: 2490, outputs are 0.0005103244\n",
      "epoch: 1, step: 2491, outputs are 0.0005103596\n",
      "epoch: 1, step: 2492, outputs are 0.0005094515\n",
      "epoch: 1, step: 2493, outputs are 0.00050942495\n",
      "epoch: 1, step: 2494, outputs are 0.0005087856\n",
      "epoch: 1, step: 2495, outputs are 0.00050862454\n",
      "epoch: 1, step: 2496, outputs are 0.00050837605\n",
      "epoch: 1, step: 2497, outputs are 0.00050782715\n",
      "epoch: 1, step: 2498, outputs are 0.0005083984\n",
      "epoch: 1, step: 2499, outputs are 0.00050755753\n",
      "epoch: 1, step: 2500, outputs are 0.0005073905\n",
      "epoch: 1, step: 2501, outputs are 0.0005065807\n",
      "epoch: 1, step: 2502, outputs are 0.0005057423\n",
      "epoch: 1, step: 2503, outputs are 0.0005064602\n",
      "epoch: 1, step: 2504, outputs are 0.0005053204\n",
      "epoch: 1, step: 2505, outputs are 0.0005054184\n",
      "epoch: 1, step: 2506, outputs are 0.0005045265\n",
      "epoch: 1, step: 2507, outputs are 0.0005043107\n",
      "epoch: 1, step: 2508, outputs are 0.00050456455\n",
      "epoch: 1, step: 2509, outputs are 0.0005047622\n",
      "epoch: 1, step: 2510, outputs are 0.0005033971\n",
      "epoch: 1, step: 2511, outputs are 0.00050346635\n",
      "epoch: 1, step: 2512, outputs are 0.0005031106\n",
      "epoch: 1, step: 2513, outputs are 0.00050292443\n",
      "epoch: 1, step: 2514, outputs are 0.00050216645\n",
      "epoch: 1, step: 2515, outputs are 0.0005020634\n",
      "epoch: 1, step: 2516, outputs are 0.00050197786\n",
      "epoch: 1, step: 2517, outputs are 0.00050220627\n",
      "epoch: 1, step: 2518, outputs are 0.00050136005\n",
      "epoch: 1, step: 2519, outputs are 0.0005006918\n",
      "epoch: 1, step: 2520, outputs are 0.0005003301\n",
      "epoch: 1, step: 2521, outputs are 0.00050047034\n",
      "epoch: 1, step: 2522, outputs are 0.0005002614\n",
      "epoch: 1, step: 2523, outputs are 0.00049924344\n",
      "epoch: 1, step: 2524, outputs are 0.00049899466\n",
      "epoch: 1, step: 2525, outputs are 0.0004986521\n",
      "epoch: 1, step: 2526, outputs are 0.0004981521\n",
      "epoch: 1, step: 2527, outputs are 0.00049828686\n",
      "epoch: 1, step: 2528, outputs are 0.0004975961\n",
      "epoch: 1, step: 2529, outputs are 0.00049733603\n",
      "epoch: 1, step: 2530, outputs are 0.00049716537\n",
      "epoch: 1, step: 2531, outputs are 0.0004967601\n",
      "epoch: 1, step: 2532, outputs are 0.0004962214\n",
      "epoch: 1, step: 2533, outputs are 0.000495782\n",
      "epoch: 1, step: 2534, outputs are 0.0004960309\n",
      "epoch: 1, step: 2535, outputs are 0.0004951118\n",
      "epoch: 1, step: 2536, outputs are 0.0004953396\n",
      "epoch: 1, step: 2537, outputs are 0.00049537024\n",
      "epoch: 1, step: 2538, outputs are 0.00049475057\n",
      "epoch: 1, step: 2539, outputs are 0.00049447396\n",
      "epoch: 1, step: 2540, outputs are 0.0004943981\n",
      "epoch: 1, step: 2541, outputs are 0.0004939447\n",
      "epoch: 1, step: 2542, outputs are 0.0004928006\n",
      "epoch: 1, step: 2543, outputs are 0.00049317465\n",
      "epoch: 1, step: 2544, outputs are 0.0004933573\n",
      "epoch: 1, step: 2545, outputs are 0.00049253734\n",
      "epoch: 1, step: 2546, outputs are 0.0004924072\n",
      "epoch: 1, step: 2547, outputs are 0.0004918976\n",
      "epoch: 1, step: 2548, outputs are 0.00049130846\n",
      "epoch: 1, step: 2549, outputs are 0.00049088075\n",
      "epoch: 1, step: 2550, outputs are 0.00049070636\n",
      "epoch: 1, step: 2551, outputs are 0.0004908559\n",
      "epoch: 1, step: 2552, outputs are 0.0004905388\n",
      "epoch: 1, step: 2553, outputs are 0.0004901623\n",
      "epoch: 1, step: 2554, outputs are 0.0004893312\n",
      "epoch: 1, step: 2555, outputs are 0.00048900384\n",
      "epoch: 1, step: 2556, outputs are 0.00048880704\n",
      "epoch: 1, step: 2557, outputs are 0.00048891304\n",
      "epoch: 1, step: 2558, outputs are 0.0004885731\n",
      "epoch: 1, step: 2559, outputs are 0.00048803692\n",
      "epoch: 1, step: 2560, outputs are 0.00048822767\n",
      "epoch: 1, step: 2561, outputs are 0.00048761826\n",
      "epoch: 1, step: 2562, outputs are 0.00048761457\n",
      "epoch: 1, step: 2563, outputs are 0.0004868494\n",
      "epoch: 1, step: 2564, outputs are 0.00048622346\n",
      "epoch: 1, step: 2565, outputs are 0.00048668572\n",
      "epoch: 1, step: 2566, outputs are 0.00048539037\n",
      "epoch: 1, step: 2567, outputs are 0.0004855314\n",
      "epoch: 1, step: 2568, outputs are 0.0004844418\n",
      "epoch: 1, step: 2569, outputs are 0.0004847607\n",
      "epoch: 1, step: 2570, outputs are 0.0004846324\n",
      "epoch: 1, step: 2571, outputs are 0.00048476778\n",
      "epoch: 1, step: 2572, outputs are 0.0004838798\n",
      "epoch: 1, step: 2573, outputs are 0.0004840028\n",
      "epoch: 1, step: 2574, outputs are 0.00048292434\n",
      "epoch: 1, step: 2575, outputs are 0.00048312062\n",
      "epoch: 1, step: 2576, outputs are 0.00048270047\n",
      "epoch: 1, step: 2577, outputs are 0.00048268403\n",
      "epoch: 1, step: 2578, outputs are 0.00048148632\n",
      "epoch: 1, step: 2579, outputs are 0.00048240882\n",
      "epoch: 1, step: 2580, outputs are 0.00048115992\n",
      "epoch: 1, step: 2581, outputs are 0.00048130492\n",
      "epoch: 1, step: 2582, outputs are 0.00048043206\n",
      "epoch: 1, step: 2583, outputs are 0.00048067234\n",
      "epoch: 1, step: 2584, outputs are 0.00048045593\n",
      "epoch: 1, step: 2585, outputs are 0.0004799207\n",
      "epoch: 1, step: 2586, outputs are 0.00047960388\n",
      "epoch: 1, step: 2587, outputs are 0.00047971768\n",
      "epoch: 1, step: 2588, outputs are 0.0004794719\n",
      "epoch: 1, step: 2589, outputs are 0.00047931063\n",
      "epoch: 1, step: 2590, outputs are 0.0004781944\n",
      "epoch: 1, step: 2591, outputs are 0.0004784092\n",
      "epoch: 1, step: 2592, outputs are 0.00047797657\n",
      "epoch: 1, step: 2593, outputs are 0.00047840015\n",
      "epoch: 1, step: 2594, outputs are 0.0004768052\n",
      "epoch: 1, step: 2595, outputs are 0.00047692453\n",
      "epoch: 1, step: 2596, outputs are 0.00047652016\n",
      "epoch: 1, step: 2597, outputs are 0.00047616055\n",
      "epoch: 1, step: 2598, outputs are 0.00047531404\n",
      "epoch: 1, step: 2599, outputs are 0.00047556392\n",
      "epoch: 1, step: 2600, outputs are 0.0004753826\n",
      "epoch: 1, step: 2601, outputs are 0.00047542562\n",
      "epoch: 1, step: 2602, outputs are 0.0004750722\n",
      "epoch: 1, step: 2603, outputs are 0.0004747995\n",
      "epoch: 1, step: 2604, outputs are 0.00047467538\n",
      "epoch: 1, step: 2605, outputs are 0.00047413798\n",
      "epoch: 1, step: 2606, outputs are 0.0004736164\n",
      "epoch: 1, step: 2607, outputs are 0.00047292764\n",
      "epoch: 1, step: 2608, outputs are 0.00047306952\n",
      "epoch: 1, step: 2609, outputs are 0.00047306303\n",
      "epoch: 1, step: 2610, outputs are 0.00047262927\n",
      "epoch: 1, step: 2611, outputs are 0.000472093\n",
      "epoch: 1, step: 2612, outputs are 0.0004721348\n",
      "epoch: 1, step: 2613, outputs are 0.00047188636\n",
      "epoch: 1, step: 2614, outputs are 0.0004713161\n",
      "epoch: 1, step: 2615, outputs are 0.00047095964\n",
      "epoch: 1, step: 2616, outputs are 0.0004706922\n",
      "epoch: 1, step: 2617, outputs are 0.00047008437\n",
      "epoch: 1, step: 2618, outputs are 0.00047023193\n",
      "epoch: 1, step: 2619, outputs are 0.00046996327\n",
      "epoch: 1, step: 2620, outputs are 0.00046932983\n",
      "epoch: 1, step: 2621, outputs are 0.0004693927\n",
      "epoch: 1, step: 2622, outputs are 0.000469179\n",
      "epoch: 1, step: 2623, outputs are 0.00046900436\n",
      "epoch: 1, step: 2624, outputs are 0.00046854996\n",
      "epoch: 1, step: 2625, outputs are 0.00046800933\n",
      "epoch: 1, step: 2626, outputs are 0.00046844519\n",
      "epoch: 1, step: 2627, outputs are 0.0004678602\n",
      "epoch: 1, step: 2628, outputs are 0.00046728886\n",
      "epoch: 1, step: 2629, outputs are 0.00046680207\n",
      "epoch: 1, step: 2630, outputs are 0.00046700222\n",
      "epoch: 1, step: 2631, outputs are 0.0004662063\n",
      "epoch: 1, step: 2632, outputs are 0.00046600378\n",
      "epoch: 1, step: 2633, outputs are 0.00046597078\n",
      "epoch: 1, step: 2634, outputs are 0.00046558428\n",
      "epoch: 1, step: 2635, outputs are 0.00046543468\n",
      "epoch: 1, step: 2636, outputs are 0.00046438357\n",
      "epoch: 1, step: 2637, outputs are 0.0004645869\n",
      "epoch: 1, step: 2638, outputs are 0.00046374655\n",
      "epoch: 1, step: 2639, outputs are 0.00046352026\n",
      "epoch: 1, step: 2640, outputs are 0.00046349308\n",
      "epoch: 1, step: 2641, outputs are 0.00046355496\n",
      "epoch: 1, step: 2642, outputs are 0.00046346404\n",
      "epoch: 1, step: 2643, outputs are 0.0004624006\n",
      "epoch: 1, step: 2644, outputs are 0.0004626591\n",
      "epoch: 1, step: 2645, outputs are 0.0004623516\n",
      "epoch: 1, step: 2646, outputs are 0.000461774\n",
      "epoch: 1, step: 2647, outputs are 0.0004613708\n",
      "epoch: 1, step: 2648, outputs are 0.00046115764\n",
      "epoch: 1, step: 2649, outputs are 0.0004611641\n",
      "epoch: 1, step: 2650, outputs are 0.000460523\n",
      "epoch: 1, step: 2651, outputs are 0.00046092406\n",
      "epoch: 1, step: 2652, outputs are 0.00046040525\n",
      "epoch: 1, step: 2653, outputs are 0.00045957763\n",
      "epoch: 1, step: 2654, outputs are 0.00045974372\n",
      "epoch: 1, step: 2655, outputs are 0.00045920064\n",
      "epoch: 1, step: 2656, outputs are 0.00045949002\n",
      "epoch: 1, step: 2657, outputs are 0.000458444\n",
      "epoch: 1, step: 2658, outputs are 0.0004584145\n",
      "epoch: 1, step: 2659, outputs are 0.00045803143\n",
      "epoch: 1, step: 2660, outputs are 0.00045798998\n",
      "epoch: 1, step: 2661, outputs are 0.00045764554\n",
      "epoch: 1, step: 2662, outputs are 0.00045725165\n",
      "epoch: 1, step: 2663, outputs are 0.0004568291\n",
      "epoch: 1, step: 2664, outputs are 0.0004573873\n",
      "epoch: 1, step: 2665, outputs are 0.00045670068\n",
      "epoch: 1, step: 2666, outputs are 0.0004566062\n",
      "epoch: 1, step: 2667, outputs are 0.00045610548\n",
      "epoch: 1, step: 2668, outputs are 0.00045561302\n",
      "epoch: 1, step: 2669, outputs are 0.00045527838\n",
      "epoch: 1, step: 2670, outputs are 0.00045581398\n",
      "epoch: 1, step: 2671, outputs are 0.00045508274\n",
      "epoch: 1, step: 2672, outputs are 0.00045514363\n",
      "epoch: 1, step: 2673, outputs are 0.0004547272\n",
      "epoch: 1, step: 2674, outputs are 0.0004538164\n",
      "epoch: 1, step: 2675, outputs are 0.0004541849\n",
      "epoch: 1, step: 2676, outputs are 0.0004531533\n",
      "epoch: 1, step: 2677, outputs are 0.00045274658\n",
      "epoch: 1, step: 2678, outputs are 0.0004528475\n",
      "epoch: 1, step: 2679, outputs are 0.00045302638\n",
      "epoch: 1, step: 2680, outputs are 0.00045256322\n",
      "epoch: 1, step: 2681, outputs are 0.00045229174\n",
      "epoch: 1, step: 2682, outputs are 0.00045248138\n",
      "epoch: 1, step: 2683, outputs are 0.00045190053\n",
      "epoch: 1, step: 2684, outputs are 0.0004507899\n",
      "epoch: 1, step: 2685, outputs are 0.0004505778\n",
      "epoch: 1, step: 2686, outputs are 0.0004503966\n",
      "epoch: 1, step: 2687, outputs are 0.00045018672\n",
      "epoch: 1, step: 2688, outputs are 0.00045000121\n",
      "epoch: 1, step: 2689, outputs are 0.0004497789\n",
      "epoch: 1, step: 2690, outputs are 0.0004499365\n",
      "epoch: 1, step: 2691, outputs are 0.0004491168\n",
      "epoch: 1, step: 2692, outputs are 0.00044950587\n",
      "epoch: 1, step: 2693, outputs are 0.00044918322\n",
      "epoch: 1, step: 2694, outputs are 0.0004484877\n",
      "epoch: 1, step: 2695, outputs are 0.00044777998\n",
      "epoch: 1, step: 2696, outputs are 0.00044820635\n",
      "epoch: 1, step: 2697, outputs are 0.00044759706\n",
      "epoch: 1, step: 2698, outputs are 0.00044742672\n",
      "epoch: 1, step: 2699, outputs are 0.00044643262\n",
      "epoch: 1, step: 2700, outputs are 0.00044681475\n",
      "epoch: 1, step: 2701, outputs are 0.00044715224\n",
      "epoch: 1, step: 2702, outputs are 0.00044612767\n",
      "epoch: 1, step: 2703, outputs are 0.00044579897\n",
      "epoch: 1, step: 2704, outputs are 0.00044580054\n",
      "epoch: 1, step: 2705, outputs are 0.0004454372\n",
      "epoch: 1, step: 2706, outputs are 0.0004454903\n",
      "epoch: 1, step: 2707, outputs are 0.00044472553\n",
      "epoch: 1, step: 2708, outputs are 0.0004448429\n",
      "epoch: 1, step: 2709, outputs are 0.00044443939\n",
      "epoch: 1, step: 2710, outputs are 0.00044448237\n",
      "epoch: 1, step: 2711, outputs are 0.0004437568\n",
      "epoch: 1, step: 2712, outputs are 0.00044360958\n",
      "epoch: 1, step: 2713, outputs are 0.0004432601\n",
      "epoch: 1, step: 2714, outputs are 0.0004432248\n",
      "epoch: 1, step: 2715, outputs are 0.00044294415\n",
      "epoch: 1, step: 2716, outputs are 0.00044223331\n",
      "epoch: 1, step: 2717, outputs are 0.00044220628\n",
      "epoch: 1, step: 2718, outputs are 0.0004417181\n",
      "epoch: 1, step: 2719, outputs are 0.00044171678\n",
      "epoch: 1, step: 2720, outputs are 0.0004412036\n",
      "epoch: 1, step: 2721, outputs are 0.00044135656\n",
      "epoch: 1, step: 2722, outputs are 0.0004411094\n",
      "epoch: 1, step: 2723, outputs are 0.0004404061\n",
      "epoch: 1, step: 2724, outputs are 0.00044068784\n",
      "epoch: 1, step: 2725, outputs are 0.00044046395\n",
      "epoch: 1, step: 2726, outputs are 0.0004395423\n",
      "epoch: 1, step: 2727, outputs are 0.00043974505\n",
      "epoch: 1, step: 2728, outputs are 0.00043948673\n",
      "epoch: 1, step: 2729, outputs are 0.00043920596\n",
      "epoch: 1, step: 2730, outputs are 0.0004383797\n",
      "epoch: 1, step: 2731, outputs are 0.0004384374\n",
      "epoch: 1, step: 2732, outputs are 0.00043852042\n",
      "epoch: 1, step: 2733, outputs are 0.00043793442\n",
      "epoch: 1, step: 2734, outputs are 0.00043736605\n",
      "epoch: 1, step: 2735, outputs are 0.00043765013\n",
      "epoch: 1, step: 2736, outputs are 0.00043699527\n",
      "epoch: 1, step: 2737, outputs are 0.00043707472\n",
      "epoch: 1, step: 2738, outputs are 0.0004367498\n",
      "epoch: 1, step: 2739, outputs are 0.00043648254\n",
      "epoch: 1, step: 2740, outputs are 0.00043589046\n",
      "epoch: 1, step: 2741, outputs are 0.00043587145\n",
      "epoch: 1, step: 2742, outputs are 0.00043541953\n",
      "epoch: 1, step: 2743, outputs are 0.00043543748\n",
      "epoch: 1, step: 2744, outputs are 0.0004354035\n",
      "epoch: 1, step: 2745, outputs are 0.00043441978\n",
      "epoch: 1, step: 2746, outputs are 0.00043519418\n",
      "epoch: 1, step: 2747, outputs are 0.00043468483\n",
      "epoch: 1, step: 2748, outputs are 0.0004339023\n",
      "epoch: 1, step: 2749, outputs are 0.00043356535\n",
      "epoch: 1, step: 2750, outputs are 0.0004332557\n",
      "epoch: 1, step: 2751, outputs are 0.00043285382\n",
      "epoch: 1, step: 2752, outputs are 0.00043288106\n",
      "epoch: 1, step: 2753, outputs are 0.00043246217\n",
      "epoch: 1, step: 2754, outputs are 0.00043220702\n",
      "epoch: 1, step: 2755, outputs are 0.0004324808\n",
      "epoch: 1, step: 2756, outputs are 0.0004316321\n",
      "epoch: 1, step: 2757, outputs are 0.00043151825\n",
      "epoch: 1, step: 2758, outputs are 0.0004313158\n",
      "epoch: 1, step: 2759, outputs are 0.00043086516\n",
      "epoch: 1, step: 2760, outputs are 0.00043089793\n",
      "epoch: 1, step: 2761, outputs are 0.0004307278\n",
      "epoch: 1, step: 2762, outputs are 0.00043016166\n",
      "epoch: 1, step: 2763, outputs are 0.00043038512\n",
      "epoch: 1, step: 2764, outputs are 0.0004296761\n",
      "epoch: 1, step: 2765, outputs are 0.0004294865\n",
      "epoch: 1, step: 2766, outputs are 0.00042975135\n",
      "epoch: 1, step: 2767, outputs are 0.00042896194\n",
      "epoch: 1, step: 2768, outputs are 0.00042878054\n",
      "epoch: 1, step: 2769, outputs are 0.00042838193\n",
      "epoch: 1, step: 2770, outputs are 0.00042894518\n",
      "epoch: 1, step: 2771, outputs are 0.00042816158\n",
      "epoch: 1, step: 2772, outputs are 0.00042763684\n",
      "epoch: 1, step: 2773, outputs are 0.0004277185\n",
      "epoch: 1, step: 2774, outputs are 0.00042707784\n",
      "epoch: 1, step: 2775, outputs are 0.0004269183\n",
      "epoch: 1, step: 2776, outputs are 0.0004267956\n",
      "epoch: 1, step: 2777, outputs are 0.00042668747\n",
      "epoch: 1, step: 2778, outputs are 0.00042630458\n",
      "epoch: 1, step: 2779, outputs are 0.00042622932\n",
      "epoch: 1, step: 2780, outputs are 0.00042566305\n",
      "epoch: 1, step: 2781, outputs are 0.0004258782\n",
      "epoch: 1, step: 2782, outputs are 0.00042543688\n",
      "epoch: 1, step: 2783, outputs are 0.00042533365\n",
      "epoch: 1, step: 2784, outputs are 0.0004249639\n",
      "epoch: 1, step: 2785, outputs are 0.0004243073\n",
      "epoch: 1, step: 2786, outputs are 0.00042443414\n",
      "epoch: 1, step: 2787, outputs are 0.00042395337\n",
      "epoch: 1, step: 2788, outputs are 0.00042386254\n",
      "epoch: 1, step: 2789, outputs are 0.00042380005\n",
      "epoch: 1, step: 2790, outputs are 0.00042332546\n",
      "epoch: 1, step: 2791, outputs are 0.00042306626\n",
      "epoch: 1, step: 2792, outputs are 0.00042265723\n",
      "epoch: 1, step: 2793, outputs are 0.0004230162\n",
      "epoch: 1, step: 2794, outputs are 0.00042238948\n",
      "epoch: 1, step: 2795, outputs are 0.0004218344\n",
      "epoch: 1, step: 2796, outputs are 0.0004217244\n",
      "epoch: 1, step: 2797, outputs are 0.00042174617\n",
      "epoch: 1, step: 2798, outputs are 0.00042126936\n",
      "epoch: 1, step: 2799, outputs are 0.00042086816\n",
      "epoch: 1, step: 2800, outputs are 0.00042061866\n",
      "epoch: 1, step: 2801, outputs are 0.0004208986\n",
      "epoch: 1, step: 2802, outputs are 0.00042017055\n",
      "epoch: 1, step: 2803, outputs are 0.00041993934\n",
      "epoch: 1, step: 2804, outputs are 0.00041963742\n",
      "epoch: 1, step: 2805, outputs are 0.00041958794\n",
      "epoch: 1, step: 2806, outputs are 0.00041964708\n",
      "epoch: 1, step: 2807, outputs are 0.0004193444\n",
      "epoch: 1, step: 2808, outputs are 0.0004188294\n",
      "epoch: 1, step: 2809, outputs are 0.0004187549\n",
      "epoch: 1, step: 2810, outputs are 0.00041856756\n",
      "epoch: 1, step: 2811, outputs are 0.00041806017\n",
      "epoch: 1, step: 2812, outputs are 0.00041832798\n",
      "epoch: 1, step: 2813, outputs are 0.00041791087\n",
      "epoch: 1, step: 2814, outputs are 0.0004171262\n",
      "epoch: 1, step: 2815, outputs are 0.00041674293\n",
      "epoch: 1, step: 2816, outputs are 0.00041707215\n",
      "epoch: 1, step: 2817, outputs are 0.00041685742\n",
      "epoch: 1, step: 2818, outputs are 0.00041590765\n",
      "epoch: 1, step: 2819, outputs are 0.00041647739\n",
      "epoch: 1, step: 2820, outputs are 0.00041592045\n",
      "epoch: 1, step: 2821, outputs are 0.0004153509\n",
      "epoch: 1, step: 2822, outputs are 0.00041531477\n",
      "epoch: 1, step: 2823, outputs are 0.00041527205\n",
      "epoch: 1, step: 2824, outputs are 0.00041504146\n",
      "epoch: 1, step: 2825, outputs are 0.00041452918\n",
      "epoch: 1, step: 2826, outputs are 0.00041409826\n",
      "epoch: 1, step: 2827, outputs are 0.00041381296\n",
      "epoch: 1, step: 2828, outputs are 0.00041391182\n",
      "epoch: 1, step: 2829, outputs are 0.0004136251\n",
      "epoch: 1, step: 2830, outputs are 0.00041306787\n",
      "epoch: 1, step: 2831, outputs are 0.00041292555\n",
      "epoch: 1, step: 2832, outputs are 0.0004128946\n",
      "epoch: 1, step: 2833, outputs are 0.00041281132\n",
      "epoch: 1, step: 2834, outputs are 0.00041246382\n",
      "epoch: 1, step: 2835, outputs are 0.0004125318\n",
      "epoch: 1, step: 2836, outputs are 0.00041191425\n",
      "epoch: 1, step: 2837, outputs are 0.00041146428\n",
      "epoch: 1, step: 2838, outputs are 0.0004114849\n",
      "epoch: 1, step: 2839, outputs are 0.0004110108\n",
      "epoch: 1, step: 2840, outputs are 0.00041070092\n",
      "epoch: 1, step: 2841, outputs are 0.00041045662\n",
      "epoch: 1, step: 2842, outputs are 0.00040999398\n",
      "epoch: 1, step: 2843, outputs are 0.0004105012\n",
      "epoch: 1, step: 2844, outputs are 0.0004095909\n",
      "epoch: 1, step: 2845, outputs are 0.00040965015\n",
      "epoch: 1, step: 2846, outputs are 0.00040971712\n",
      "epoch: 1, step: 2847, outputs are 0.00040925617\n",
      "epoch: 1, step: 2848, outputs are 0.00040952716\n",
      "epoch: 1, step: 2849, outputs are 0.00040905864\n",
      "epoch: 1, step: 2850, outputs are 0.00040808\n",
      "epoch: 1, step: 2851, outputs are 0.0004085256\n",
      "epoch: 1, step: 2852, outputs are 0.00040824013\n",
      "epoch: 1, step: 2853, outputs are 0.00040783445\n",
      "epoch: 1, step: 2854, outputs are 0.00040773023\n",
      "epoch: 1, step: 2855, outputs are 0.00040742016\n",
      "epoch: 1, step: 2856, outputs are 0.00040752537\n",
      "epoch: 1, step: 2857, outputs are 0.00040703555\n",
      "epoch: 1, step: 2858, outputs are 0.00040656386\n",
      "epoch: 1, step: 2859, outputs are 0.00040639285\n",
      "epoch: 1, step: 2860, outputs are 0.00040598994\n",
      "epoch: 1, step: 2861, outputs are 0.0004059567\n",
      "epoch: 1, step: 2862, outputs are 0.0004057383\n",
      "epoch: 1, step: 2863, outputs are 0.0004056054\n",
      "epoch: 1, step: 2864, outputs are 0.00040495745\n",
      "epoch: 1, step: 2865, outputs are 0.0004052906\n",
      "epoch: 1, step: 2866, outputs are 0.00040430503\n",
      "epoch: 1, step: 2867, outputs are 0.00040426466\n",
      "epoch: 1, step: 2868, outputs are 0.00040439222\n",
      "epoch: 1, step: 2869, outputs are 0.00040404545\n",
      "epoch: 1, step: 2870, outputs are 0.00040391434\n",
      "epoch: 1, step: 2871, outputs are 0.00040331887\n",
      "epoch: 1, step: 2872, outputs are 0.00040325493\n",
      "epoch: 1, step: 2873, outputs are 0.00040290176\n",
      "epoch: 1, step: 2874, outputs are 0.00040277984\n",
      "epoch: 1, step: 2875, outputs are 0.00040257568\n",
      "epoch: 1, step: 2876, outputs are 0.0004026068\n",
      "epoch: 1, step: 2877, outputs are 0.00040210603\n",
      "epoch: 1, step: 2878, outputs are 0.00040201686\n",
      "epoch: 1, step: 2879, outputs are 0.0004014495\n",
      "epoch: 1, step: 2880, outputs are 0.0004015889\n",
      "epoch: 1, step: 2881, outputs are 0.00040121697\n",
      "epoch: 1, step: 2882, outputs are 0.00040131874\n",
      "epoch: 1, step: 2883, outputs are 0.00040062467\n",
      "epoch: 1, step: 2884, outputs are 0.0004006038\n",
      "epoch: 1, step: 2885, outputs are 0.0004001483\n",
      "epoch: 1, step: 2886, outputs are 0.00040001282\n",
      "epoch: 1, step: 2887, outputs are 0.00039994548\n",
      "epoch: 1, step: 2888, outputs are 0.00040014103\n",
      "epoch: 1, step: 2889, outputs are 0.00039975182\n",
      "epoch: 1, step: 2890, outputs are 0.00039864084\n",
      "epoch: 1, step: 2891, outputs are 0.0003989066\n",
      "epoch: 1, step: 2892, outputs are 0.000398612\n",
      "epoch: 1, step: 2893, outputs are 0.00039891506\n",
      "epoch: 1, step: 2894, outputs are 0.00039815507\n",
      "epoch: 1, step: 2895, outputs are 0.00039829404\n",
      "epoch: 1, step: 2896, outputs are 0.00039780952\n",
      "epoch: 1, step: 2897, outputs are 0.00039714642\n",
      "epoch: 1, step: 2898, outputs are 0.0003973212\n",
      "epoch: 1, step: 2899, outputs are 0.0003972132\n",
      "epoch: 1, step: 2900, outputs are 0.00039632322\n",
      "epoch: 1, step: 2901, outputs are 0.00039695483\n",
      "epoch: 1, step: 2902, outputs are 0.00039646044\n",
      "epoch: 1, step: 2903, outputs are 0.0003957343\n",
      "epoch: 1, step: 2904, outputs are 0.00039599626\n",
      "epoch: 1, step: 2905, outputs are 0.00039571512\n",
      "epoch: 1, step: 2906, outputs are 0.00039564836\n",
      "epoch: 1, step: 2907, outputs are 0.00039498552\n",
      "epoch: 1, step: 2908, outputs are 0.00039497027\n",
      "epoch: 1, step: 2909, outputs are 0.00039494946\n",
      "epoch: 1, step: 2910, outputs are 0.00039460452\n",
      "epoch: 1, step: 2911, outputs are 0.00039359686\n",
      "epoch: 1, step: 2912, outputs are 0.00039450865\n",
      "epoch: 1, step: 2913, outputs are 0.00039446607\n",
      "epoch: 1, step: 2914, outputs are 0.00039387442\n",
      "epoch: 1, step: 2915, outputs are 0.0003931976\n",
      "epoch: 1, step: 2916, outputs are 0.0003932812\n",
      "epoch: 1, step: 2917, outputs are 0.00039269548\n",
      "epoch: 1, step: 2918, outputs are 0.00039251504\n",
      "epoch: 1, step: 2919, outputs are 0.000392397\n",
      "epoch: 1, step: 2920, outputs are 0.00039205005\n",
      "epoch: 1, step: 2921, outputs are 0.0003920719\n",
      "epoch: 1, step: 2922, outputs are 0.00039203826\n",
      "epoch: 1, step: 2923, outputs are 0.00039143115\n",
      "epoch: 1, step: 2924, outputs are 0.00039180383\n",
      "epoch: 1, step: 2925, outputs are 0.0003911432\n",
      "epoch: 1, step: 2926, outputs are 0.0003908918\n",
      "epoch: 1, step: 2927, outputs are 0.00039084832\n",
      "epoch: 1, step: 2928, outputs are 0.00039013912\n",
      "epoch: 1, step: 2929, outputs are 0.00039006837\n",
      "epoch: 1, step: 2930, outputs are 0.00039004954\n",
      "epoch: 1, step: 2931, outputs are 0.0003900337\n",
      "epoch: 1, step: 2932, outputs are 0.00038938306\n",
      "epoch: 1, step: 2933, outputs are 0.00038910215\n",
      "epoch: 1, step: 2934, outputs are 0.0003889429\n",
      "epoch: 1, step: 2935, outputs are 0.00038847863\n",
      "epoch: 1, step: 2936, outputs are 0.00038884455\n",
      "epoch: 1, step: 2937, outputs are 0.00038817577\n",
      "epoch: 1, step: 2938, outputs are 0.000388055\n",
      "epoch: 1, step: 2939, outputs are 0.00038804364\n",
      "epoch: 1, step: 2940, outputs are 0.0003877093\n",
      "epoch: 1, step: 2941, outputs are 0.0003873491\n",
      "epoch: 1, step: 2942, outputs are 0.0003878612\n",
      "epoch: 1, step: 2943, outputs are 0.00038693353\n",
      "epoch: 1, step: 2944, outputs are 0.0003869028\n",
      "epoch: 1, step: 2945, outputs are 0.0003867898\n",
      "epoch: 1, step: 2946, outputs are 0.00038641747\n",
      "epoch: 1, step: 2947, outputs are 0.00038639305\n",
      "epoch: 1, step: 2948, outputs are 0.0003857374\n",
      "epoch: 1, step: 2949, outputs are 0.00038577715\n",
      "epoch: 1, step: 2950, outputs are 0.00038546528\n",
      "epoch: 1, step: 2951, outputs are 0.00038533536\n",
      "epoch: 1, step: 2952, outputs are 0.0003853491\n",
      "epoch: 1, step: 2953, outputs are 0.00038493535\n",
      "epoch: 1, step: 2954, outputs are 0.0003847626\n",
      "epoch: 1, step: 2955, outputs are 0.0003842715\n",
      "epoch: 1, step: 2956, outputs are 0.00038416014\n",
      "epoch: 1, step: 2957, outputs are 0.00038430345\n",
      "epoch: 1, step: 2958, outputs are 0.00038354797\n",
      "epoch: 1, step: 2959, outputs are 0.00038351567\n",
      "epoch: 1, step: 2960, outputs are 0.00038346634\n",
      "epoch: 1, step: 2961, outputs are 0.00038329477\n",
      "epoch: 1, step: 2962, outputs are 0.00038271255\n",
      "epoch: 1, step: 2963, outputs are 0.00038291558\n",
      "epoch: 1, step: 2964, outputs are 0.0003825978\n",
      "epoch: 1, step: 2965, outputs are 0.0003819783\n",
      "epoch: 1, step: 2966, outputs are 0.00038222724\n",
      "epoch: 1, step: 2967, outputs are 0.00038213795\n",
      "epoch: 1, step: 2968, outputs are 0.00038131734\n",
      "epoch: 1, step: 2969, outputs are 0.00038113387\n",
      "epoch: 1, step: 2970, outputs are 0.00038118823\n",
      "epoch: 1, step: 2971, outputs are 0.00038090924\n",
      "epoch: 1, step: 2972, outputs are 0.00038102758\n",
      "epoch: 1, step: 2973, outputs are 0.00038062924\n",
      "epoch: 1, step: 2974, outputs are 0.0003803312\n",
      "epoch: 1, step: 2975, outputs are 0.00037980615\n",
      "epoch: 1, step: 2976, outputs are 0.00038005447\n",
      "epoch: 1, step: 2977, outputs are 0.00038001273\n",
      "epoch: 1, step: 2978, outputs are 0.00037946145\n",
      "epoch: 1, step: 2979, outputs are 0.00037935577\n",
      "epoch: 1, step: 2980, outputs are 0.00037866237\n",
      "epoch: 1, step: 2981, outputs are 0.00037854374\n",
      "epoch: 1, step: 2982, outputs are 0.00037824223\n",
      "epoch: 1, step: 2983, outputs are 0.0003780647\n",
      "epoch: 1, step: 2984, outputs are 0.0003782814\n",
      "epoch: 1, step: 2985, outputs are 0.0003782544\n",
      "epoch: 1, step: 2986, outputs are 0.0003773405\n",
      "epoch: 1, step: 2987, outputs are 0.00037772115\n",
      "epoch: 1, step: 2988, outputs are 0.0003773817\n",
      "epoch: 1, step: 2989, outputs are 0.0003773108\n",
      "epoch: 1, step: 2990, outputs are 0.00037697022\n",
      "epoch: 1, step: 2991, outputs are 0.00037702173\n",
      "epoch: 1, step: 2992, outputs are 0.00037649323\n",
      "epoch: 1, step: 2993, outputs are 0.0003760627\n",
      "epoch: 1, step: 2994, outputs are 0.0003759081\n",
      "epoch: 1, step: 2995, outputs are 0.00037593438\n",
      "epoch: 1, step: 2996, outputs are 0.00037572882\n",
      "epoch: 1, step: 2997, outputs are 0.0003752885\n",
      "epoch: 1, step: 2998, outputs are 0.00037508007\n",
      "epoch: 1, step: 2999, outputs are 0.00037547288\n",
      "epoch: 1, step: 3000, outputs are 0.0003750148\n",
      "epoch: 1, step: 3001, outputs are 0.00037504692\n",
      "epoch: 1, step: 3002, outputs are 0.0003743823\n",
      "epoch: 1, step: 3003, outputs are 0.0003741354\n",
      "epoch: 1, step: 3004, outputs are 0.00037401525\n",
      "epoch: 1, step: 3005, outputs are 0.0003738975\n",
      "epoch: 1, step: 3006, outputs are 0.00037370596\n",
      "epoch: 1, step: 3007, outputs are 0.00037313002\n",
      "epoch: 1, step: 3008, outputs are 0.00037340232\n",
      "epoch: 1, step: 3009, outputs are 0.00037249463\n",
      "epoch: 1, step: 3010, outputs are 0.000372581\n",
      "epoch: 1, step: 3011, outputs are 0.00037246893\n",
      "epoch: 1, step: 3012, outputs are 0.00037186197\n",
      "epoch: 1, step: 3013, outputs are 0.0003719916\n",
      "epoch: 1, step: 3014, outputs are 0.0003718594\n",
      "epoch: 1, step: 3015, outputs are 0.00037176756\n",
      "epoch: 1, step: 3016, outputs are 0.0003712161\n",
      "epoch: 1, step: 3017, outputs are 0.00037068955\n",
      "epoch: 1, step: 3018, outputs are 0.00037097876\n",
      "epoch: 1, step: 3019, outputs are 0.00037019106\n",
      "epoch: 1, step: 3020, outputs are 0.00037017436\n",
      "epoch: 1, step: 3021, outputs are 0.0003705704\n",
      "epoch: 1, step: 3022, outputs are 0.00037041976\n",
      "epoch: 1, step: 3023, outputs are 0.0003697896\n",
      "epoch: 1, step: 3024, outputs are 0.0003696857\n",
      "epoch: 1, step: 3025, outputs are 0.00036965415\n",
      "epoch: 1, step: 3026, outputs are 0.00036893427\n",
      "epoch: 1, step: 3027, outputs are 0.00036870805\n",
      "epoch: 1, step: 3028, outputs are 0.00036841896\n",
      "epoch: 1, step: 3029, outputs are 0.00036812812\n",
      "epoch: 1, step: 3030, outputs are 0.0003682194\n",
      "epoch: 1, step: 3031, outputs are 0.00036810298\n",
      "epoch: 1, step: 3032, outputs are 0.00036795076\n",
      "epoch: 1, step: 3033, outputs are 0.0003680027\n",
      "epoch: 1, step: 3034, outputs are 0.00036756258\n",
      "epoch: 1, step: 3035, outputs are 0.00036757163\n",
      "epoch: 1, step: 3036, outputs are 0.0003675471\n",
      "epoch: 1, step: 3037, outputs are 0.00036677357\n",
      "epoch: 1, step: 3038, outputs are 0.0003667355\n",
      "epoch: 1, step: 3039, outputs are 0.0003663224\n",
      "epoch: 1, step: 3040, outputs are 0.00036664493\n",
      "epoch: 1, step: 3041, outputs are 0.00036607072\n",
      "epoch: 1, step: 3042, outputs are 0.00036626033\n",
      "epoch: 1, step: 3043, outputs are 0.0003660754\n",
      "epoch: 1, step: 3044, outputs are 0.00036592805\n",
      "epoch: 1, step: 3045, outputs are 0.00036542787\n",
      "epoch: 1, step: 3046, outputs are 0.00036527368\n",
      "epoch: 1, step: 3047, outputs are 0.00036494609\n",
      "epoch: 1, step: 3048, outputs are 0.00036430726\n",
      "epoch: 1, step: 3049, outputs are 0.0003648544\n",
      "epoch: 1, step: 3050, outputs are 0.0003641532\n",
      "epoch: 1, step: 3051, outputs are 0.00036472874\n",
      "epoch: 1, step: 3052, outputs are 0.00036410976\n",
      "epoch: 1, step: 3053, outputs are 0.00036380193\n",
      "epoch: 1, step: 3054, outputs are 0.00036345972\n",
      "epoch: 1, step: 3055, outputs are 0.0003634071\n",
      "epoch: 1, step: 3056, outputs are 0.0003629509\n",
      "epoch: 1, step: 3057, outputs are 0.00036295468\n",
      "epoch: 1, step: 3058, outputs are 0.00036280515\n",
      "epoch: 1, step: 3059, outputs are 0.0003623605\n",
      "epoch: 1, step: 3060, outputs are 0.0003623467\n",
      "epoch: 1, step: 3061, outputs are 0.00036232494\n",
      "epoch: 1, step: 3062, outputs are 0.00036194207\n",
      "epoch: 1, step: 3063, outputs are 0.00036176605\n",
      "epoch: 1, step: 3064, outputs are 0.0003617354\n",
      "epoch: 1, step: 3065, outputs are 0.000361167\n",
      "epoch: 1, step: 3066, outputs are 0.00036120124\n",
      "epoch: 1, step: 3067, outputs are 0.00036053092\n",
      "epoch: 1, step: 3068, outputs are 0.0003606399\n",
      "epoch: 1, step: 3069, outputs are 0.00036067606\n",
      "epoch: 1, step: 3070, outputs are 0.00036040097\n",
      "epoch: 1, step: 3071, outputs are 0.00036017608\n",
      "epoch: 1, step: 3072, outputs are 0.0003600166\n",
      "epoch: 1, step: 3073, outputs are 0.0003600269\n",
      "epoch: 1, step: 3074, outputs are 0.0003593705\n",
      "epoch: 1, step: 3075, outputs are 0.00035973467\n",
      "epoch: 1, step: 3076, outputs are 0.00035903841\n",
      "epoch: 1, step: 3077, outputs are 0.0003588482\n",
      "epoch: 1, step: 3078, outputs are 0.00035843888\n",
      "epoch: 1, step: 3079, outputs are 0.00035860855\n",
      "epoch: 1, step: 3080, outputs are 0.0003584345\n",
      "epoch: 1, step: 3081, outputs are 0.00035833125\n",
      "epoch: 1, step: 3082, outputs are 0.0003583054\n",
      "epoch: 1, step: 3083, outputs are 0.0003576692\n",
      "epoch: 1, step: 3084, outputs are 0.00035716486\n",
      "epoch: 1, step: 3085, outputs are 0.0003573709\n",
      "epoch: 1, step: 3086, outputs are 0.00035678822\n",
      "epoch: 1, step: 3087, outputs are 0.0003568653\n",
      "epoch: 1, step: 3088, outputs are 0.0003566199\n",
      "epoch: 1, step: 3089, outputs are 0.00035677\n",
      "epoch: 1, step: 3090, outputs are 0.00035627896\n",
      "epoch: 1, step: 3091, outputs are 0.00035583784\n",
      "epoch: 1, step: 3092, outputs are 0.0003557591\n",
      "epoch: 1, step: 3093, outputs are 0.00035572873\n",
      "epoch: 1, step: 3094, outputs are 0.00035582785\n",
      "epoch: 1, step: 3095, outputs are 0.0003550189\n",
      "epoch: 1, step: 3096, outputs are 0.00035552258\n",
      "epoch: 1, step: 3097, outputs are 0.0003548946\n",
      "epoch: 1, step: 3098, outputs are 0.00035448273\n",
      "epoch: 1, step: 3099, outputs are 0.0003545666\n",
      "epoch: 1, step: 3100, outputs are 0.00035437106\n",
      "epoch: 1, step: 3101, outputs are 0.0003539595\n",
      "epoch: 1, step: 3102, outputs are 0.00035417452\n",
      "epoch: 1, step: 3103, outputs are 0.0003537207\n",
      "epoch: 1, step: 3104, outputs are 0.00035379376\n",
      "epoch: 1, step: 3105, outputs are 0.00035344184\n",
      "epoch: 1, step: 3106, outputs are 0.0003528152\n",
      "epoch: 1, step: 3107, outputs are 0.00035294786\n",
      "epoch: 1, step: 3108, outputs are 0.00035280595\n",
      "epoch: 1, step: 3109, outputs are 0.00035291206\n",
      "epoch: 1, step: 3110, outputs are 0.000351912\n",
      "epoch: 1, step: 3111, outputs are 0.00035216438\n",
      "epoch: 1, step: 3112, outputs are 0.00035188263\n",
      "epoch: 1, step: 3113, outputs are 0.00035188108\n",
      "epoch: 1, step: 3114, outputs are 0.00035156848\n",
      "epoch: 1, step: 3115, outputs are 0.0003515141\n",
      "epoch: 1, step: 3116, outputs are 0.0003511267\n",
      "epoch: 1, step: 3117, outputs are 0.000350982\n",
      "epoch: 1, step: 3118, outputs are 0.00035043876\n",
      "epoch: 1, step: 3119, outputs are 0.00035060337\n",
      "epoch: 1, step: 3120, outputs are 0.00035036416\n",
      "epoch: 1, step: 3121, outputs are 0.00035041588\n",
      "epoch: 1, step: 3122, outputs are 0.0003504082\n",
      "epoch: 1, step: 3123, outputs are 0.00035045863\n",
      "epoch: 1, step: 3124, outputs are 0.00034990773\n",
      "epoch: 1, step: 3125, outputs are 0.0003496\n",
      "epoch: 1, step: 3126, outputs are 0.0003491592\n",
      "epoch: 1, step: 3127, outputs are 0.0003489342\n",
      "epoch: 1, step: 3128, outputs are 0.0003493742\n",
      "epoch: 1, step: 3129, outputs are 0.00034920982\n",
      "epoch: 1, step: 3130, outputs are 0.00034852477\n",
      "epoch: 1, step: 3131, outputs are 0.00034820856\n",
      "epoch: 1, step: 3132, outputs are 0.000348518\n",
      "epoch: 1, step: 3133, outputs are 0.00034807614\n",
      "epoch: 1, step: 3134, outputs are 0.0003478924\n",
      "epoch: 1, step: 3135, outputs are 0.00034759927\n",
      "epoch: 1, step: 3136, outputs are 0.00034734682\n",
      "epoch: 1, step: 3137, outputs are 0.0003467651\n",
      "epoch: 1, step: 3138, outputs are 0.0003473353\n",
      "epoch: 1, step: 3139, outputs are 0.0003471631\n",
      "epoch: 1, step: 3140, outputs are 0.00034642091\n",
      "epoch: 1, step: 3141, outputs are 0.0003462028\n",
      "epoch: 1, step: 3142, outputs are 0.00034645712\n",
      "epoch: 1, step: 3143, outputs are 0.00034602947\n",
      "epoch: 1, step: 3144, outputs are 0.00034595083\n",
      "epoch: 1, step: 3145, outputs are 0.00034598756\n",
      "epoch: 1, step: 3146, outputs are 0.0003456813\n",
      "epoch: 1, step: 3147, outputs are 0.00034557702\n",
      "epoch: 1, step: 3148, outputs are 0.0003453328\n",
      "epoch: 1, step: 3149, outputs are 0.00034486005\n",
      "epoch: 1, step: 3150, outputs are 0.00034496095\n",
      "epoch: 1, step: 3151, outputs are 0.00034457998\n",
      "epoch: 1, step: 3152, outputs are 0.00034414366\n",
      "epoch: 1, step: 3153, outputs are 0.00034424086\n",
      "epoch: 1, step: 3154, outputs are 0.00034444768\n",
      "epoch: 1, step: 3155, outputs are 0.00034394162\n",
      "epoch: 1, step: 3156, outputs are 0.000343621\n",
      "epoch: 1, step: 3157, outputs are 0.0003434821\n",
      "epoch: 1, step: 3158, outputs are 0.00034286428\n",
      "epoch: 1, step: 3159, outputs are 0.00034295407\n",
      "epoch: 1, step: 3160, outputs are 0.00034286533\n",
      "epoch: 1, step: 3161, outputs are 0.00034283253\n",
      "epoch: 1, step: 3162, outputs are 0.0003427408\n",
      "epoch: 1, step: 3163, outputs are 0.00034196873\n",
      "epoch: 1, step: 3164, outputs are 0.00034244533\n",
      "epoch: 1, step: 3165, outputs are 0.00034189323\n",
      "epoch: 1, step: 3166, outputs are 0.0003423809\n",
      "epoch: 1, step: 3167, outputs are 0.00034195714\n",
      "epoch: 1, step: 3168, outputs are 0.00034152158\n",
      "epoch: 1, step: 3169, outputs are 0.00034131325\n",
      "epoch: 1, step: 3170, outputs are 0.00034091357\n",
      "epoch: 1, step: 3171, outputs are 0.00034096325\n",
      "epoch: 1, step: 3172, outputs are 0.00034047652\n",
      "epoch: 1, step: 3173, outputs are 0.00034044162\n",
      "epoch: 1, step: 3174, outputs are 0.0003401154\n",
      "epoch: 1, step: 3175, outputs are 0.0003401743\n",
      "epoch: 1, step: 3176, outputs are 0.00034049113\n",
      "epoch: 1, step: 3177, outputs are 0.00033979217\n",
      "epoch: 1, step: 3178, outputs are 0.00033966286\n",
      "epoch: 1, step: 3179, outputs are 0.0003396418\n",
      "epoch: 1, step: 3180, outputs are 0.00033925567\n",
      "epoch: 1, step: 3181, outputs are 0.0003390744\n",
      "epoch: 1, step: 3182, outputs are 0.0003387976\n",
      "epoch: 1, step: 3183, outputs are 0.00033874947\n",
      "epoch: 1, step: 3184, outputs are 0.00033840688\n",
      "epoch: 1, step: 3185, outputs are 0.0003380956\n",
      "epoch: 1, step: 3186, outputs are 0.00033803526\n",
      "epoch: 1, step: 3187, outputs are 0.00033803843\n",
      "epoch: 1, step: 3188, outputs are 0.00033769972\n",
      "epoch: 1, step: 3189, outputs are 0.00033778074\n",
      "epoch: 1, step: 3190, outputs are 0.0003372433\n",
      "epoch: 1, step: 3191, outputs are 0.00033704448\n",
      "epoch: 1, step: 3192, outputs are 0.00033700716\n",
      "epoch: 1, step: 3193, outputs are 0.0003366173\n",
      "epoch: 1, step: 3194, outputs are 0.00033673382\n",
      "epoch: 1, step: 3195, outputs are 0.00033631857\n",
      "epoch: 1, step: 3196, outputs are 0.00033657238\n",
      "epoch: 1, step: 3197, outputs are 0.000336211\n",
      "epoch: 1, step: 3198, outputs are 0.00033584324\n",
      "epoch: 1, step: 3199, outputs are 0.00033571804\n",
      "epoch: 1, step: 3200, outputs are 0.00033545337\n",
      "epoch: 1, step: 3201, outputs are 0.0003352142\n",
      "epoch: 1, step: 3202, outputs are 0.00033481352\n",
      "epoch: 1, step: 3203, outputs are 0.00033503232\n",
      "epoch: 1, step: 3204, outputs are 0.00033473855\n",
      "epoch: 1, step: 3205, outputs are 0.00033473803\n",
      "epoch: 1, step: 3206, outputs are 0.00033475977\n",
      "epoch: 1, step: 3207, outputs are 0.00033428986\n",
      "epoch: 1, step: 3208, outputs are 0.00033396506\n",
      "epoch: 1, step: 3209, outputs are 0.00033408607\n",
      "epoch: 1, step: 3210, outputs are 0.0003338945\n",
      "epoch: 1, step: 3211, outputs are 0.00033348304\n",
      "epoch: 1, step: 3212, outputs are 0.00033343548\n",
      "epoch: 1, step: 3213, outputs are 0.00033282157\n",
      "epoch: 1, step: 3214, outputs are 0.000333196\n",
      "epoch: 1, step: 3215, outputs are 0.0003329786\n",
      "epoch: 1, step: 3216, outputs are 0.00033277587\n",
      "epoch: 1, step: 3217, outputs are 0.00033266543\n",
      "epoch: 1, step: 3218, outputs are 0.00033242066\n",
      "epoch: 1, step: 3219, outputs are 0.00033216702\n",
      "epoch: 1, step: 3220, outputs are 0.0003321633\n",
      "epoch: 1, step: 3221, outputs are 0.00033161702\n",
      "epoch: 1, step: 3222, outputs are 0.0003318835\n",
      "epoch: 1, step: 3223, outputs are 0.0003314615\n",
      "epoch: 1, step: 3224, outputs are 0.0003316737\n",
      "epoch: 1, step: 3225, outputs are 0.00033093515\n",
      "epoch: 1, step: 3226, outputs are 0.00033113477\n",
      "epoch: 1, step: 3227, outputs are 0.00033058203\n",
      "epoch: 1, step: 3228, outputs are 0.00033047344\n",
      "epoch: 1, step: 3229, outputs are 0.00033035583\n",
      "epoch: 1, step: 3230, outputs are 0.00033023075\n",
      "epoch: 1, step: 3231, outputs are 0.0003300762\n",
      "epoch: 1, step: 3232, outputs are 0.00032962247\n",
      "epoch: 1, step: 3233, outputs are 0.00032964972\n",
      "epoch: 1, step: 3234, outputs are 0.00032968645\n",
      "epoch: 1, step: 3235, outputs are 0.00032952658\n",
      "epoch: 1, step: 3236, outputs are 0.00032922346\n",
      "epoch: 1, step: 3237, outputs are 0.0003287826\n",
      "epoch: 1, step: 3238, outputs are 0.00032851083\n",
      "epoch: 1, step: 3239, outputs are 0.00032864645\n",
      "epoch: 1, step: 3240, outputs are 0.00032831775\n",
      "epoch: 1, step: 3241, outputs are 0.0003285415\n",
      "epoch: 1, step: 3242, outputs are 0.00032767298\n",
      "epoch: 1, step: 3243, outputs are 0.0003281998\n",
      "epoch: 1, step: 3244, outputs are 0.00032770776\n",
      "epoch: 1, step: 3245, outputs are 0.00032749661\n",
      "epoch: 1, step: 3246, outputs are 0.0003272834\n",
      "epoch: 1, step: 3247, outputs are 0.00032703005\n",
      "epoch: 1, step: 3248, outputs are 0.00032691067\n",
      "epoch: 1, step: 3249, outputs are 0.00032698587\n",
      "epoch: 1, step: 3250, outputs are 0.00032668302\n",
      "epoch: 1, step: 3251, outputs are 0.00032646075\n",
      "epoch: 1, step: 3252, outputs are 0.00032615958\n",
      "epoch: 1, step: 3253, outputs are 0.00032626244\n",
      "epoch: 1, step: 3254, outputs are 0.0003257612\n",
      "epoch: 1, step: 3255, outputs are 0.00032535725\n",
      "epoch: 1, step: 3256, outputs are 0.0003255519\n",
      "epoch: 1, step: 3257, outputs are 0.0003255006\n",
      "epoch: 1, step: 3258, outputs are 0.0003251848\n",
      "epoch: 1, step: 3259, outputs are 0.00032503984\n",
      "epoch: 1, step: 3260, outputs are 0.00032475346\n",
      "epoch: 1, step: 3261, outputs are 0.00032484342\n",
      "epoch: 1, step: 3262, outputs are 0.0003248289\n",
      "epoch: 1, step: 3263, outputs are 0.00032464165\n",
      "epoch: 1, step: 3264, outputs are 0.00032427462\n",
      "epoch: 1, step: 3265, outputs are 0.000324262\n",
      "epoch: 1, step: 3266, outputs are 0.00032384356\n",
      "epoch: 1, step: 3267, outputs are 0.000323774\n",
      "epoch: 1, step: 3268, outputs are 0.00032367493\n",
      "epoch: 1, step: 3269, outputs are 0.00032309105\n",
      "epoch: 1, step: 3270, outputs are 0.00032323078\n",
      "epoch: 1, step: 3271, outputs are 0.00032293744\n",
      "epoch: 1, step: 3272, outputs are 0.00032305327\n",
      "epoch: 1, step: 3273, outputs are 0.00032283127\n",
      "epoch: 1, step: 3274, outputs are 0.00032235018\n",
      "epoch: 1, step: 3275, outputs are 0.00032230816\n",
      "epoch: 1, step: 3276, outputs are 0.0003221984\n",
      "epoch: 1, step: 3277, outputs are 0.0003217365\n",
      "epoch: 1, step: 3278, outputs are 0.00032193292\n",
      "epoch: 1, step: 3279, outputs are 0.00032156034\n",
      "epoch: 1, step: 3280, outputs are 0.00032155198\n",
      "epoch: 1, step: 3281, outputs are 0.0003215173\n",
      "epoch: 1, step: 3282, outputs are 0.00032093402\n",
      "epoch: 1, step: 3283, outputs are 0.00032106985\n",
      "epoch: 1, step: 3284, outputs are 0.0003203392\n",
      "epoch: 1, step: 3285, outputs are 0.00032058387\n",
      "epoch: 1, step: 3286, outputs are 0.00032034365\n",
      "epoch: 1, step: 3287, outputs are 0.00032051848\n",
      "epoch: 1, step: 3288, outputs are 0.00032024115\n",
      "epoch: 1, step: 3289, outputs are 0.0003198435\n",
      "epoch: 1, step: 3290, outputs are 0.0003198166\n",
      "epoch: 1, step: 3291, outputs are 0.00031971198\n",
      "epoch: 1, step: 3292, outputs are 0.0003193859\n",
      "epoch: 1, step: 3293, outputs are 0.00031952222\n",
      "epoch: 1, step: 3294, outputs are 0.0003193317\n",
      "epoch: 1, step: 3295, outputs are 0.0003189742\n",
      "epoch: 1, step: 3296, outputs are 0.0003186345\n",
      "epoch: 1, step: 3297, outputs are 0.00031894183\n",
      "epoch: 1, step: 3298, outputs are 0.00031847373\n",
      "epoch: 1, step: 3299, outputs are 0.0003181378\n",
      "epoch: 1, step: 3300, outputs are 0.0003184282\n",
      "epoch: 1, step: 3301, outputs are 0.00031827838\n",
      "epoch: 1, step: 3302, outputs are 0.00031777946\n",
      "epoch: 1, step: 3303, outputs are 0.00031783374\n",
      "epoch: 1, step: 3304, outputs are 0.00031760297\n",
      "epoch: 1, step: 3305, outputs are 0.00031709252\n",
      "epoch: 1, step: 3306, outputs are 0.00031713027\n",
      "epoch: 1, step: 3307, outputs are 0.00031690852\n",
      "epoch: 1, step: 3308, outputs are 0.00031652133\n",
      "epoch: 1, step: 3309, outputs are 0.00031658704\n",
      "epoch: 1, step: 3310, outputs are 0.0003165537\n",
      "epoch: 1, step: 3311, outputs are 0.00031620753\n",
      "epoch: 1, step: 3312, outputs are 0.000316428\n",
      "epoch: 1, step: 3313, outputs are 0.00031562016\n",
      "epoch: 1, step: 3314, outputs are 0.0003158562\n",
      "epoch: 1, step: 3315, outputs are 0.00031558593\n",
      "epoch: 1, step: 3316, outputs are 0.00031571515\n",
      "epoch: 1, step: 3317, outputs are 0.00031524725\n",
      "epoch: 1, step: 3318, outputs are 0.00031474023\n",
      "epoch: 1, step: 3319, outputs are 0.00031483665\n",
      "epoch: 1, step: 3320, outputs are 0.00031478057\n",
      "epoch: 1, step: 3321, outputs are 0.000314411\n",
      "epoch: 1, step: 3322, outputs are 0.0003144756\n",
      "epoch: 1, step: 3323, outputs are 0.0003141591\n",
      "epoch: 1, step: 3324, outputs are 0.0003144156\n",
      "epoch: 1, step: 3325, outputs are 0.00031402023\n",
      "epoch: 1, step: 3326, outputs are 0.0003137918\n",
      "epoch: 1, step: 3327, outputs are 0.00031357107\n",
      "epoch: 1, step: 3328, outputs are 0.00031345384\n",
      "epoch: 1, step: 3329, outputs are 0.000313106\n",
      "epoch: 1, step: 3330, outputs are 0.00031303367\n",
      "epoch: 1, step: 3331, outputs are 0.0003126884\n",
      "epoch: 1, step: 3332, outputs are 0.00031261347\n",
      "epoch: 1, step: 3333, outputs are 0.0003125668\n",
      "epoch: 1, step: 3334, outputs are 0.00031264382\n",
      "epoch: 1, step: 3335, outputs are 0.0003122145\n",
      "epoch: 1, step: 3336, outputs are 0.0003123124\n",
      "epoch: 1, step: 3337, outputs are 0.00031204458\n",
      "epoch: 1, step: 3338, outputs are 0.00031152333\n",
      "epoch: 1, step: 3339, outputs are 0.0003116194\n",
      "epoch: 1, step: 3340, outputs are 0.0003117238\n",
      "epoch: 1, step: 3341, outputs are 0.00031185432\n",
      "epoch: 1, step: 3342, outputs are 0.00031152106\n",
      "epoch: 1, step: 3343, outputs are 0.0003111031\n",
      "epoch: 1, step: 3344, outputs are 0.0003108458\n",
      "epoch: 1, step: 3345, outputs are 0.00031086788\n",
      "epoch: 1, step: 3346, outputs are 0.00031076765\n",
      "epoch: 1, step: 3347, outputs are 0.00031015516\n",
      "epoch: 1, step: 3348, outputs are 0.00031015193\n",
      "epoch: 1, step: 3349, outputs are 0.0003097899\n",
      "epoch: 1, step: 3350, outputs are 0.00030977785\n",
      "epoch: 1, step: 3351, outputs are 0.00031014715\n",
      "epoch: 1, step: 3352, outputs are 0.0003097938\n",
      "epoch: 1, step: 3353, outputs are 0.00030925972\n",
      "epoch: 1, step: 3354, outputs are 0.0003095021\n",
      "epoch: 1, step: 3355, outputs are 0.000309149\n",
      "epoch: 1, step: 3356, outputs are 0.0003088532\n",
      "epoch: 1, step: 3357, outputs are 0.00030888355\n",
      "epoch: 1, step: 3358, outputs are 0.00030845113\n",
      "epoch: 1, step: 3359, outputs are 0.00030843535\n",
      "epoch: 1, step: 3360, outputs are 0.00030830214\n",
      "epoch: 1, step: 3361, outputs are 0.00030822243\n",
      "epoch: 1, step: 3362, outputs are 0.0003075339\n",
      "epoch: 1, step: 3363, outputs are 0.0003077705\n",
      "epoch: 1, step: 3364, outputs are 0.00030764638\n",
      "epoch: 1, step: 3365, outputs are 0.00030758174\n",
      "epoch: 1, step: 3366, outputs are 0.00030713092\n",
      "epoch: 1, step: 3367, outputs are 0.00030711113\n",
      "epoch: 1, step: 3368, outputs are 0.0003070308\n",
      "epoch: 1, step: 3369, outputs are 0.00030711392\n",
      "epoch: 1, step: 3370, outputs are 0.00030687315\n",
      "epoch: 1, step: 3371, outputs are 0.0003065052\n",
      "epoch: 1, step: 3372, outputs are 0.00030655006\n",
      "epoch: 1, step: 3373, outputs are 0.00030659093\n",
      "epoch: 1, step: 3374, outputs are 0.0003061032\n",
      "epoch: 1, step: 3375, outputs are 0.0003059613\n",
      "epoch: 1, step: 3376, outputs are 0.00030570943\n",
      "epoch: 1, step: 3377, outputs are 0.00030579613\n",
      "epoch: 1, step: 3378, outputs are 0.0003054107\n",
      "epoch: 1, step: 3379, outputs are 0.00030533376\n",
      "epoch: 1, step: 3380, outputs are 0.00030521693\n",
      "epoch: 1, step: 3381, outputs are 0.0003051742\n",
      "epoch: 1, step: 3382, outputs are 0.00030457805\n",
      "epoch: 1, step: 3383, outputs are 0.00030459068\n",
      "epoch: 1, step: 3384, outputs are 0.00030455706\n",
      "epoch: 1, step: 3385, outputs are 0.000304365\n",
      "epoch: 1, step: 3386, outputs are 0.00030417927\n",
      "epoch: 1, step: 3387, outputs are 0.0003040699\n",
      "epoch: 1, step: 3388, outputs are 0.00030374562\n",
      "epoch: 1, step: 3389, outputs are 0.00030398334\n",
      "epoch: 1, step: 3390, outputs are 0.0003039955\n",
      "epoch: 1, step: 3391, outputs are 0.00030371067\n",
      "epoch: 1, step: 3392, outputs are 0.00030329113\n",
      "epoch: 1, step: 3393, outputs are 0.00030320464\n",
      "epoch: 1, step: 3394, outputs are 0.0003029922\n",
      "epoch: 1, step: 3395, outputs are 0.0003029884\n",
      "epoch: 1, step: 3396, outputs are 0.00030262434\n",
      "epoch: 1, step: 3397, outputs are 0.00030233277\n",
      "epoch: 1, step: 3398, outputs are 0.00030237896\n",
      "epoch: 1, step: 3399, outputs are 0.0003022463\n",
      "epoch: 1, step: 3400, outputs are 0.0003020145\n",
      "epoch: 1, step: 3401, outputs are 0.0003017662\n",
      "epoch: 1, step: 3402, outputs are 0.00030144537\n",
      "epoch: 1, step: 3403, outputs are 0.00030161333\n",
      "epoch: 1, step: 3404, outputs are 0.0003014375\n",
      "epoch: 1, step: 3405, outputs are 0.00030174857\n",
      "epoch: 1, step: 3406, outputs are 0.00030126853\n",
      "epoch: 1, step: 3407, outputs are 0.00030093762\n",
      "epoch: 1, step: 3408, outputs are 0.00030058646\n",
      "epoch: 1, step: 3409, outputs are 0.00030069886\n",
      "epoch: 1, step: 3410, outputs are 0.00030057944\n",
      "epoch: 1, step: 3411, outputs are 0.00030032813\n",
      "epoch: 1, step: 3412, outputs are 0.00030012947\n",
      "epoch: 1, step: 3413, outputs are 0.0003003508\n",
      "epoch: 1, step: 3414, outputs are 0.00029976887\n",
      "epoch: 1, step: 3415, outputs are 0.00029954338\n",
      "epoch: 1, step: 3416, outputs are 0.0002993117\n",
      "epoch: 1, step: 3417, outputs are 0.0002995364\n",
      "epoch: 1, step: 3418, outputs are 0.00029932798\n",
      "epoch: 1, step: 3419, outputs are 0.00029929428\n",
      "epoch: 1, step: 3420, outputs are 0.00029922457\n",
      "epoch: 1, step: 3421, outputs are 0.00029901488\n",
      "epoch: 1, step: 3422, outputs are 0.00029853324\n",
      "epoch: 1, step: 3423, outputs are 0.00029830844\n",
      "epoch: 1, step: 3424, outputs are 0.00029854826\n",
      "epoch: 1, step: 3425, outputs are 0.00029824863\n",
      "epoch: 1, step: 3426, outputs are 0.00029796743\n",
      "epoch: 1, step: 3427, outputs are 0.0002982021\n",
      "epoch: 1, step: 3428, outputs are 0.00029779034\n",
      "epoch: 1, step: 3429, outputs are 0.000297835\n",
      "epoch: 1, step: 3430, outputs are 0.00029765596\n",
      "epoch: 1, step: 3431, outputs are 0.0002974085\n",
      "epoch: 1, step: 3432, outputs are 0.00029721862\n",
      "epoch: 1, step: 3433, outputs are 0.0002971407\n",
      "epoch: 1, step: 3434, outputs are 0.00029696425\n",
      "epoch: 1, step: 3435, outputs are 0.00029693698\n",
      "epoch: 1, step: 3436, outputs are 0.00029657403\n",
      "epoch: 1, step: 3437, outputs are 0.00029663066\n",
      "epoch: 1, step: 3438, outputs are 0.00029619236\n",
      "epoch: 1, step: 3439, outputs are 0.000295821\n",
      "epoch: 1, step: 3440, outputs are 0.00029563368\n",
      "epoch: 1, step: 3441, outputs are 0.00029574154\n",
      "epoch: 1, step: 3442, outputs are 0.00029580249\n",
      "epoch: 1, step: 3443, outputs are 0.00029524052\n",
      "Train epoch time: 3139839.438 ms, per step time: 911.949 ms\n",
      "The best acc is 0.500217833284926\n",
      "epoch: 2, step: 3444, outputs are 0.00029580249\n",
      "epoch: 2, step: 3445, outputs are 0.00029511336\n",
      "epoch: 2, step: 3446, outputs are 0.00029528746\n",
      "epoch: 2, step: 3447, outputs are 0.00029469177\n",
      "epoch: 2, step: 3448, outputs are 0.000294438\n",
      "epoch: 2, step: 3449, outputs are 0.00029435207\n",
      "epoch: 2, step: 3450, outputs are 0.00029479878\n",
      "epoch: 2, step: 3451, outputs are 0.00029441237\n",
      "epoch: 2, step: 3452, outputs are 0.00029457544\n",
      "epoch: 2, step: 3453, outputs are 0.00029437497\n",
      "epoch: 2, step: 3454, outputs are 0.0002939009\n",
      "epoch: 2, step: 3455, outputs are 0.00029380136\n",
      "epoch: 2, step: 3456, outputs are 0.0002937246\n",
      "epoch: 2, step: 3457, outputs are 0.00029331498\n",
      "epoch: 2, step: 3458, outputs are 0.00029329857\n",
      "epoch: 2, step: 3459, outputs are 0.00029344708\n",
      "epoch: 2, step: 3460, outputs are 0.00029283576\n",
      "epoch: 2, step: 3461, outputs are 0.00029271745\n",
      "epoch: 2, step: 3462, outputs are 0.00029293942\n",
      "epoch: 2, step: 3463, outputs are 0.00029239774\n",
      "epoch: 2, step: 3464, outputs are 0.00029271035\n",
      "epoch: 2, step: 3465, outputs are 0.00029211908\n",
      "epoch: 2, step: 3466, outputs are 0.00029207452\n",
      "epoch: 2, step: 3467, outputs are 0.00029198948\n",
      "epoch: 2, step: 3468, outputs are 0.0002920533\n",
      "epoch: 2, step: 3469, outputs are 0.00029170784\n",
      "epoch: 2, step: 3470, outputs are 0.00029117925\n",
      "epoch: 2, step: 3471, outputs are 0.00029135725\n",
      "epoch: 2, step: 3472, outputs are 0.00029091467\n",
      "epoch: 2, step: 3473, outputs are 0.00029136456\n",
      "epoch: 2, step: 3474, outputs are 0.0002909602\n",
      "epoch: 2, step: 3475, outputs are 0.00029081944\n",
      "epoch: 2, step: 3476, outputs are 0.00029065233\n",
      "epoch: 2, step: 3477, outputs are 0.00029028964\n",
      "epoch: 2, step: 3478, outputs are 0.0002903383\n",
      "epoch: 2, step: 3479, outputs are 0.0002901508\n",
      "epoch: 2, step: 3480, outputs are 0.00029005698\n",
      "epoch: 2, step: 3481, outputs are 0.00028995777\n",
      "epoch: 2, step: 3482, outputs are 0.00028965864\n",
      "epoch: 2, step: 3483, outputs are 0.0002894564\n",
      "epoch: 2, step: 3484, outputs are 0.0002896621\n",
      "epoch: 2, step: 3485, outputs are 0.00028903014\n",
      "epoch: 2, step: 3486, outputs are 0.000289073\n",
      "epoch: 2, step: 3487, outputs are 0.000289057\n",
      "epoch: 2, step: 3488, outputs are 0.00028920002\n",
      "epoch: 2, step: 3489, outputs are 0.00028862557\n",
      "epoch: 2, step: 3490, outputs are 0.00028871378\n",
      "epoch: 2, step: 3491, outputs are 0.00028874847\n",
      "epoch: 2, step: 3492, outputs are 0.00028865808\n",
      "epoch: 2, step: 3493, outputs are 0.00028819102\n",
      "epoch: 2, step: 3494, outputs are 0.00028830027\n",
      "epoch: 2, step: 3495, outputs are 0.00028821122\n",
      "epoch: 2, step: 3496, outputs are 0.00028802978\n",
      "epoch: 2, step: 3497, outputs are 0.00028730466\n",
      "epoch: 2, step: 3498, outputs are 0.00028758345\n",
      "epoch: 2, step: 3499, outputs are 0.00028736988\n",
      "epoch: 2, step: 3500, outputs are 0.0002869775\n",
      "epoch: 2, step: 3501, outputs are 0.00028710323\n",
      "epoch: 2, step: 3502, outputs are 0.00028692646\n",
      "epoch: 2, step: 3503, outputs are 0.0002866487\n",
      "epoch: 2, step: 3504, outputs are 0.00028673594\n",
      "epoch: 2, step: 3505, outputs are 0.00028677357\n",
      "epoch: 2, step: 3506, outputs are 0.0002863719\n",
      "epoch: 2, step: 3507, outputs are 0.00028612133\n",
      "epoch: 2, step: 3508, outputs are 0.00028604743\n",
      "epoch: 2, step: 3509, outputs are 0.00028619225\n",
      "epoch: 2, step: 3510, outputs are 0.00028563302\n",
      "epoch: 2, step: 3511, outputs are 0.00028569537\n",
      "epoch: 2, step: 3512, outputs are 0.00028569685\n",
      "epoch: 2, step: 3513, outputs are 0.00028539926\n",
      "epoch: 2, step: 3514, outputs are 0.00028493843\n",
      "epoch: 2, step: 3515, outputs are 0.0002852393\n",
      "epoch: 2, step: 3516, outputs are 0.0002844949\n",
      "epoch: 2, step: 3517, outputs are 0.00028478433\n",
      "epoch: 2, step: 3518, outputs are 0.00028492132\n",
      "epoch: 2, step: 3519, outputs are 0.0002842152\n",
      "epoch: 2, step: 3520, outputs are 0.0002844995\n",
      "epoch: 2, step: 3521, outputs are 0.00028467094\n",
      "epoch: 2, step: 3522, outputs are 0.00028395804\n",
      "epoch: 2, step: 3523, outputs are 0.0002841716\n",
      "epoch: 2, step: 3524, outputs are 0.0002839115\n",
      "epoch: 2, step: 3525, outputs are 0.00028366887\n",
      "epoch: 2, step: 3526, outputs are 0.00028332812\n",
      "epoch: 2, step: 3527, outputs are 0.00028322235\n",
      "epoch: 2, step: 3528, outputs are 0.00028313848\n",
      "epoch: 2, step: 3529, outputs are 0.0002830937\n",
      "epoch: 2, step: 3530, outputs are 0.00028329223\n",
      "epoch: 2, step: 3531, outputs are 0.0002827009\n",
      "epoch: 2, step: 3532, outputs are 0.00028304948\n",
      "epoch: 2, step: 3533, outputs are 0.0002824602\n",
      "epoch: 2, step: 3534, outputs are 0.00028249648\n",
      "epoch: 2, step: 3535, outputs are 0.00028258358\n",
      "epoch: 2, step: 3536, outputs are 0.0002819309\n",
      "epoch: 2, step: 3537, outputs are 0.0002819157\n",
      "epoch: 2, step: 3538, outputs are 0.00028198055\n",
      "epoch: 2, step: 3539, outputs are 0.00028166594\n",
      "epoch: 2, step: 3540, outputs are 0.00028167746\n",
      "epoch: 2, step: 3541, outputs are 0.00028131786\n",
      "epoch: 2, step: 3542, outputs are 0.00028091168\n",
      "epoch: 2, step: 3543, outputs are 0.00028098992\n",
      "epoch: 2, step: 3544, outputs are 0.00028103957\n",
      "epoch: 2, step: 3545, outputs are 0.00028095025\n",
      "epoch: 2, step: 3546, outputs are 0.00028056846\n",
      "epoch: 2, step: 3547, outputs are 0.0002804263\n",
      "epoch: 2, step: 3548, outputs are 0.00028055185\n",
      "epoch: 2, step: 3549, outputs are 0.00028037943\n",
      "epoch: 2, step: 3550, outputs are 0.00028008636\n",
      "epoch: 2, step: 3551, outputs are 0.0002800539\n",
      "epoch: 2, step: 3552, outputs are 0.00027974113\n",
      "epoch: 2, step: 3553, outputs are 0.0002795778\n",
      "epoch: 2, step: 3554, outputs are 0.00027947995\n",
      "epoch: 2, step: 3555, outputs are 0.00027943822\n",
      "epoch: 2, step: 3556, outputs are 0.00027934142\n",
      "epoch: 2, step: 3557, outputs are 0.00027905614\n",
      "epoch: 2, step: 3558, outputs are 0.00027918027\n",
      "epoch: 2, step: 3559, outputs are 0.00027880885\n",
      "epoch: 2, step: 3560, outputs are 0.00027876766\n",
      "epoch: 2, step: 3561, outputs are 0.00027855637\n",
      "epoch: 2, step: 3562, outputs are 0.00027867756\n",
      "epoch: 2, step: 3563, outputs are 0.00027839665\n",
      "epoch: 2, step: 3564, outputs are 0.00027819618\n",
      "epoch: 2, step: 3565, outputs are 0.0002782817\n",
      "epoch: 2, step: 3566, outputs are 0.00027790343\n",
      "epoch: 2, step: 3567, outputs are 0.00027796265\n",
      "epoch: 2, step: 3568, outputs are 0.00027785922\n",
      "epoch: 2, step: 3569, outputs are 0.00027776885\n",
      "epoch: 2, step: 3570, outputs are 0.00027758806\n",
      "epoch: 2, step: 3571, outputs are 0.0002771439\n",
      "epoch: 2, step: 3572, outputs are 0.00027697167\n",
      "epoch: 2, step: 3573, outputs are 0.00027724775\n",
      "epoch: 2, step: 3574, outputs are 0.0002770461\n",
      "epoch: 2, step: 3575, outputs are 0.0002769113\n",
      "epoch: 2, step: 3576, outputs are 0.00027660886\n",
      "epoch: 2, step: 3577, outputs are 0.00027654303\n",
      "epoch: 2, step: 3578, outputs are 0.00027602795\n",
      "epoch: 2, step: 3579, outputs are 0.00027628455\n",
      "epoch: 2, step: 3580, outputs are 0.00027611176\n",
      "epoch: 2, step: 3581, outputs are 0.0002761779\n",
      "epoch: 2, step: 3582, outputs are 0.000275868\n",
      "epoch: 2, step: 3583, outputs are 0.00027524328\n",
      "epoch: 2, step: 3584, outputs are 0.00027554523\n",
      "epoch: 2, step: 3585, outputs are 0.00027525495\n",
      "epoch: 2, step: 3586, outputs are 0.00027557585\n",
      "epoch: 2, step: 3587, outputs are 0.0002748697\n",
      "epoch: 2, step: 3588, outputs are 0.00027497445\n",
      "epoch: 2, step: 3589, outputs are 0.0002747228\n",
      "epoch: 2, step: 3590, outputs are 0.0002746805\n",
      "epoch: 2, step: 3591, outputs are 0.00027473492\n",
      "epoch: 2, step: 3592, outputs are 0.00027484167\n",
      "epoch: 2, step: 3593, outputs are 0.00027453297\n",
      "epoch: 2, step: 3594, outputs are 0.00027437124\n",
      "epoch: 2, step: 3595, outputs are 0.0002740143\n",
      "epoch: 2, step: 3596, outputs are 0.0002737283\n",
      "epoch: 2, step: 3597, outputs are 0.00027328558\n",
      "epoch: 2, step: 3598, outputs are 0.00027381175\n",
      "epoch: 2, step: 3599, outputs are 0.00027346693\n",
      "epoch: 2, step: 3600, outputs are 0.00027334952\n",
      "epoch: 2, step: 3601, outputs are 0.0002733551\n",
      "epoch: 2, step: 3602, outputs are 0.00027335895\n",
      "epoch: 2, step: 3603, outputs are 0.00027297082\n",
      "epoch: 2, step: 3604, outputs are 0.00027305476\n",
      "epoch: 2, step: 3605, outputs are 0.00027258403\n",
      "epoch: 2, step: 3606, outputs are 0.0002726983\n",
      "epoch: 2, step: 3607, outputs are 0.00027219357\n",
      "epoch: 2, step: 3608, outputs are 0.00027238062\n",
      "epoch: 2, step: 3609, outputs are 0.00027220242\n",
      "epoch: 2, step: 3610, outputs are 0.00027210166\n",
      "epoch: 2, step: 3611, outputs are 0.00027206438\n",
      "epoch: 2, step: 3612, outputs are 0.0002717941\n",
      "epoch: 2, step: 3613, outputs are 0.00027197605\n",
      "epoch: 2, step: 3614, outputs are 0.00027163659\n",
      "epoch: 2, step: 3615, outputs are 0.0002714945\n",
      "epoch: 2, step: 3616, outputs are 0.00027119345\n",
      "epoch: 2, step: 3617, outputs are 0.00027108451\n",
      "epoch: 2, step: 3618, outputs are 0.00027105058\n",
      "epoch: 2, step: 3619, outputs are 0.0002705895\n",
      "epoch: 2, step: 3620, outputs are 0.00027075224\n",
      "epoch: 2, step: 3621, outputs are 0.0002708509\n",
      "epoch: 2, step: 3622, outputs are 0.00027061405\n",
      "epoch: 2, step: 3623, outputs are 0.00027026184\n",
      "epoch: 2, step: 3624, outputs are 0.00027020334\n",
      "epoch: 2, step: 3625, outputs are 0.00027033524\n",
      "epoch: 2, step: 3626, outputs are 0.00026988925\n",
      "epoch: 2, step: 3627, outputs are 0.00026975412\n",
      "epoch: 2, step: 3628, outputs are 0.0002698898\n",
      "epoch: 2, step: 3629, outputs are 0.00026963517\n",
      "epoch: 2, step: 3630, outputs are 0.00026949585\n",
      "epoch: 2, step: 3631, outputs are 0.00026908304\n",
      "epoch: 2, step: 3632, outputs are 0.00026930397\n",
      "epoch: 2, step: 3633, outputs are 0.00026882894\n",
      "epoch: 2, step: 3634, outputs are 0.00026888703\n",
      "epoch: 2, step: 3635, outputs are 0.0002685534\n",
      "epoch: 2, step: 3636, outputs are 0.00026869564\n",
      "epoch: 2, step: 3637, outputs are 0.00026854547\n",
      "epoch: 2, step: 3638, outputs are 0.00026818886\n",
      "epoch: 2, step: 3639, outputs are 0.00026816016\n",
      "epoch: 2, step: 3640, outputs are 0.00026801333\n",
      "epoch: 2, step: 3641, outputs are 0.00026803132\n",
      "epoch: 2, step: 3642, outputs are 0.00026789756\n",
      "epoch: 2, step: 3643, outputs are 0.00026758833\n",
      "epoch: 2, step: 3644, outputs are 0.00026787692\n",
      "epoch: 2, step: 3645, outputs are 0.0002676051\n",
      "epoch: 2, step: 3646, outputs are 0.00026743583\n",
      "epoch: 2, step: 3647, outputs are 0.00026717794\n",
      "epoch: 2, step: 3648, outputs are 0.00026718448\n",
      "epoch: 2, step: 3649, outputs are 0.00026715134\n",
      "epoch: 2, step: 3650, outputs are 0.00026674708\n",
      "epoch: 2, step: 3651, outputs are 0.0002668947\n",
      "epoch: 2, step: 3652, outputs are 0.00026654662\n",
      "epoch: 2, step: 3653, outputs are 0.00026673684\n",
      "epoch: 2, step: 3654, outputs are 0.00026615005\n",
      "epoch: 2, step: 3655, outputs are 0.0002661823\n",
      "epoch: 2, step: 3656, outputs are 0.00026612467\n",
      "epoch: 2, step: 3657, outputs are 0.00026549186\n",
      "epoch: 2, step: 3658, outputs are 0.00026562222\n",
      "epoch: 2, step: 3659, outputs are 0.00026528694\n",
      "epoch: 2, step: 3660, outputs are 0.0002653623\n",
      "epoch: 2, step: 3661, outputs are 0.00026519064\n",
      "epoch: 2, step: 3662, outputs are 0.00026519946\n",
      "epoch: 2, step: 3663, outputs are 0.00026533473\n",
      "epoch: 2, step: 3664, outputs are 0.00026488485\n",
      "epoch: 2, step: 3665, outputs are 0.00026489468\n",
      "epoch: 2, step: 3666, outputs are 0.0002646084\n",
      "epoch: 2, step: 3667, outputs are 0.0002644055\n",
      "epoch: 2, step: 3668, outputs are 0.00026447608\n",
      "epoch: 2, step: 3669, outputs are 0.00026438496\n",
      "epoch: 2, step: 3670, outputs are 0.00026418795\n",
      "epoch: 2, step: 3671, outputs are 0.0002640057\n",
      "epoch: 2, step: 3672, outputs are 0.00026363894\n",
      "epoch: 2, step: 3673, outputs are 0.00026369962\n",
      "epoch: 2, step: 3674, outputs are 0.00026359118\n",
      "epoch: 2, step: 3675, outputs are 0.00026341635\n",
      "epoch: 2, step: 3676, outputs are 0.00026365276\n",
      "epoch: 2, step: 3677, outputs are 0.00026324968\n",
      "epoch: 2, step: 3678, outputs are 0.00026339584\n",
      "epoch: 2, step: 3679, outputs are 0.00026305663\n",
      "epoch: 2, step: 3680, outputs are 0.0002630736\n",
      "epoch: 2, step: 3681, outputs are 0.00026282418\n",
      "epoch: 2, step: 3682, outputs are 0.0002627946\n",
      "epoch: 2, step: 3683, outputs are 0.0002626341\n",
      "epoch: 2, step: 3684, outputs are 0.00026260375\n",
      "epoch: 2, step: 3685, outputs are 0.00026208867\n",
      "epoch: 2, step: 3686, outputs are 0.0002618676\n",
      "epoch: 2, step: 3687, outputs are 0.0002619963\n",
      "epoch: 2, step: 3688, outputs are 0.00026165624\n",
      "epoch: 2, step: 3689, outputs are 0.00026212522\n",
      "epoch: 2, step: 3690, outputs are 0.0002614052\n",
      "epoch: 2, step: 3691, outputs are 0.00026148895\n",
      "epoch: 2, step: 3692, outputs are 0.00026158598\n",
      "epoch: 2, step: 3693, outputs are 0.00026137434\n",
      "epoch: 2, step: 3694, outputs are 0.00026113598\n",
      "epoch: 2, step: 3695, outputs are 0.00026105414\n",
      "epoch: 2, step: 3696, outputs are 0.0002608085\n",
      "epoch: 2, step: 3697, outputs are 0.00026057614\n",
      "epoch: 2, step: 3698, outputs are 0.00026047713\n",
      "epoch: 2, step: 3699, outputs are 0.00026036095\n",
      "epoch: 2, step: 3700, outputs are 0.00026050172\n",
      "epoch: 2, step: 3701, outputs are 0.000259935\n",
      "epoch: 2, step: 3702, outputs are 0.00025993673\n",
      "epoch: 2, step: 3703, outputs are 0.00026003493\n",
      "epoch: 2, step: 3704, outputs are 0.0002600033\n",
      "epoch: 2, step: 3705, outputs are 0.00025982896\n",
      "epoch: 2, step: 3706, outputs are 0.00025959488\n",
      "epoch: 2, step: 3707, outputs are 0.00025955556\n",
      "epoch: 2, step: 3708, outputs are 0.00025938544\n",
      "epoch: 2, step: 3709, outputs are 0.00025917447\n",
      "epoch: 2, step: 3710, outputs are 0.00025902252\n",
      "epoch: 2, step: 3711, outputs are 0.00025919767\n",
      "epoch: 2, step: 3712, outputs are 0.00025903218\n",
      "epoch: 2, step: 3713, outputs are 0.0002587548\n",
      "epoch: 2, step: 3714, outputs are 0.0002587557\n",
      "epoch: 2, step: 3715, outputs are 0.00025830336\n",
      "epoch: 2, step: 3716, outputs are 0.00025844824\n",
      "epoch: 2, step: 3717, outputs are 0.0002582616\n",
      "epoch: 2, step: 3718, outputs are 0.00025824676\n",
      "epoch: 2, step: 3719, outputs are 0.00025796366\n",
      "epoch: 2, step: 3720, outputs are 0.00025826466\n",
      "epoch: 2, step: 3721, outputs are 0.00025773176\n",
      "epoch: 2, step: 3722, outputs are 0.0002577375\n",
      "epoch: 2, step: 3723, outputs are 0.00025755458\n",
      "epoch: 2, step: 3724, outputs are 0.00025756587\n",
      "epoch: 2, step: 3725, outputs are 0.00025757673\n",
      "epoch: 2, step: 3726, outputs are 0.00025740635\n",
      "epoch: 2, step: 3727, outputs are 0.00025683414\n",
      "epoch: 2, step: 3728, outputs are 0.00025694715\n",
      "epoch: 2, step: 3729, outputs are 0.00025688304\n",
      "epoch: 2, step: 3730, outputs are 0.00025662236\n",
      "epoch: 2, step: 3731, outputs are 0.00025669165\n",
      "epoch: 2, step: 3732, outputs are 0.00025630943\n",
      "epoch: 2, step: 3733, outputs are 0.00025641668\n",
      "epoch: 2, step: 3734, outputs are 0.00025609764\n",
      "epoch: 2, step: 3735, outputs are 0.00025625457\n",
      "epoch: 2, step: 3736, outputs are 0.00025583705\n",
      "epoch: 2, step: 3737, outputs are 0.00025564287\n",
      "epoch: 2, step: 3738, outputs are 0.00025538699\n",
      "epoch: 2, step: 3739, outputs are 0.00025552278\n",
      "epoch: 2, step: 3740, outputs are 0.00025554054\n",
      "epoch: 2, step: 3741, outputs are 0.00025524088\n",
      "epoch: 2, step: 3742, outputs are 0.0002551137\n",
      "epoch: 2, step: 3743, outputs are 0.00025510025\n",
      "epoch: 2, step: 3744, outputs are 0.00025493518\n",
      "epoch: 2, step: 3745, outputs are 0.00025483716\n",
      "epoch: 2, step: 3746, outputs are 0.0002546733\n",
      "epoch: 2, step: 3747, outputs are 0.0002550907\n",
      "epoch: 2, step: 3748, outputs are 0.00025471995\n",
      "epoch: 2, step: 3749, outputs are 0.00025429222\n",
      "epoch: 2, step: 3750, outputs are 0.00025404105\n",
      "epoch: 2, step: 3751, outputs are 0.00025427906\n",
      "epoch: 2, step: 3752, outputs are 0.00025408674\n",
      "epoch: 2, step: 3753, outputs are 0.00025402295\n",
      "epoch: 2, step: 3754, outputs are 0.000253562\n",
      "epoch: 2, step: 3755, outputs are 0.0002536087\n",
      "epoch: 2, step: 3756, outputs are 0.0002534681\n",
      "epoch: 2, step: 3757, outputs are 0.00025336645\n",
      "epoch: 2, step: 3758, outputs are 0.0002533038\n",
      "epoch: 2, step: 3759, outputs are 0.00025287853\n",
      "epoch: 2, step: 3760, outputs are 0.00025303452\n",
      "epoch: 2, step: 3761, outputs are 0.0002528423\n",
      "epoch: 2, step: 3762, outputs are 0.0002524751\n",
      "epoch: 2, step: 3763, outputs are 0.00025276103\n",
      "epoch: 2, step: 3764, outputs are 0.0002524123\n",
      "epoch: 2, step: 3765, outputs are 0.00025253333\n",
      "epoch: 2, step: 3766, outputs are 0.00025214002\n",
      "epoch: 2, step: 3767, outputs are 0.0002520847\n",
      "epoch: 2, step: 3768, outputs are 0.00025208804\n",
      "epoch: 2, step: 3769, outputs are 0.0002517608\n",
      "epoch: 2, step: 3770, outputs are 0.00025176513\n",
      "epoch: 2, step: 3771, outputs are 0.00025195343\n",
      "epoch: 2, step: 3772, outputs are 0.00025184057\n",
      "epoch: 2, step: 3773, outputs are 0.00025158047\n",
      "epoch: 2, step: 3774, outputs are 0.0002512521\n",
      "epoch: 2, step: 3775, outputs are 0.0002510472\n",
      "epoch: 2, step: 3776, outputs are 0.00025105436\n",
      "epoch: 2, step: 3777, outputs are 0.0002511747\n",
      "epoch: 2, step: 3778, outputs are 0.00025058092\n",
      "epoch: 2, step: 3779, outputs are 0.00025092255\n",
      "epoch: 2, step: 3780, outputs are 0.00025056963\n",
      "epoch: 2, step: 3781, outputs are 0.00025084708\n",
      "epoch: 2, step: 3782, outputs are 0.00025042807\n",
      "epoch: 2, step: 3783, outputs are 0.0002504517\n",
      "epoch: 2, step: 3784, outputs are 0.00025027245\n",
      "epoch: 2, step: 3785, outputs are 0.00025001707\n",
      "epoch: 2, step: 3786, outputs are 0.0002498316\n",
      "epoch: 2, step: 3787, outputs are 0.0002496376\n",
      "epoch: 2, step: 3788, outputs are 0.0002494731\n",
      "epoch: 2, step: 3789, outputs are 0.00024931686\n",
      "epoch: 2, step: 3790, outputs are 0.0002493581\n",
      "epoch: 2, step: 3791, outputs are 0.00024917343\n",
      "epoch: 2, step: 3792, outputs are 0.00024939945\n",
      "epoch: 2, step: 3793, outputs are 0.000249155\n",
      "epoch: 2, step: 3794, outputs are 0.0002492321\n",
      "epoch: 2, step: 3795, outputs are 0.00024885088\n",
      "epoch: 2, step: 3796, outputs are 0.0002487058\n",
      "epoch: 2, step: 3797, outputs are 0.0002485781\n",
      "epoch: 2, step: 3798, outputs are 0.00024851746\n",
      "epoch: 2, step: 3799, outputs are 0.00024819863\n",
      "epoch: 2, step: 3800, outputs are 0.00024825102\n",
      "epoch: 2, step: 3801, outputs are 0.00024835204\n",
      "epoch: 2, step: 3802, outputs are 0.00024803058\n",
      "epoch: 2, step: 3803, outputs are 0.00024795067\n",
      "epoch: 2, step: 3804, outputs are 0.00024770177\n",
      "epoch: 2, step: 3805, outputs are 0.00024775133\n",
      "epoch: 2, step: 3806, outputs are 0.0002473003\n",
      "epoch: 2, step: 3807, outputs are 0.00024736297\n",
      "epoch: 2, step: 3808, outputs are 0.00024728302\n",
      "epoch: 2, step: 3809, outputs are 0.0002473484\n",
      "epoch: 2, step: 3810, outputs are 0.000247163\n",
      "epoch: 2, step: 3811, outputs are 0.00024718328\n",
      "epoch: 2, step: 3812, outputs are 0.0002468506\n",
      "epoch: 2, step: 3813, outputs are 0.00024650432\n",
      "epoch: 2, step: 3814, outputs are 0.0002464896\n",
      "epoch: 2, step: 3815, outputs are 0.00024654134\n",
      "epoch: 2, step: 3816, outputs are 0.00024626314\n",
      "epoch: 2, step: 3817, outputs are 0.00024630653\n",
      "epoch: 2, step: 3818, outputs are 0.0002461633\n",
      "epoch: 2, step: 3819, outputs are 0.00024608726\n",
      "epoch: 2, step: 3820, outputs are 0.00024584477\n",
      "epoch: 2, step: 3821, outputs are 0.0002455603\n",
      "epoch: 2, step: 3822, outputs are 0.0002455503\n",
      "epoch: 2, step: 3823, outputs are 0.0002457679\n",
      "epoch: 2, step: 3824, outputs are 0.00024538042\n",
      "epoch: 2, step: 3825, outputs are 0.00024505725\n",
      "epoch: 2, step: 3826, outputs are 0.0002450924\n",
      "epoch: 2, step: 3827, outputs are 0.00024514482\n",
      "epoch: 2, step: 3828, outputs are 0.00024509526\n",
      "epoch: 2, step: 3829, outputs are 0.00024505143\n",
      "epoch: 2, step: 3830, outputs are 0.00024432328\n",
      "epoch: 2, step: 3831, outputs are 0.00024458603\n",
      "epoch: 2, step: 3832, outputs are 0.00024455698\n",
      "epoch: 2, step: 3833, outputs are 0.0002441518\n",
      "epoch: 2, step: 3834, outputs are 0.0002443186\n",
      "epoch: 2, step: 3835, outputs are 0.00024420224\n",
      "epoch: 2, step: 3836, outputs are 0.00024410621\n",
      "epoch: 2, step: 3837, outputs are 0.00024420742\n",
      "epoch: 2, step: 3838, outputs are 0.00024392476\n",
      "epoch: 2, step: 3839, outputs are 0.00024376428\n",
      "epoch: 2, step: 3840, outputs are 0.00024341923\n",
      "epoch: 2, step: 3841, outputs are 0.00024350983\n",
      "epoch: 2, step: 3842, outputs are 0.00024347147\n",
      "epoch: 2, step: 3843, outputs are 0.00024363554\n",
      "epoch: 2, step: 3844, outputs are 0.00024311116\n",
      "epoch: 2, step: 3845, outputs are 0.00024285831\n",
      "epoch: 2, step: 3846, outputs are 0.0002428609\n",
      "epoch: 2, step: 3847, outputs are 0.00024290687\n",
      "epoch: 2, step: 3848, outputs are 0.00024265243\n",
      "epoch: 2, step: 3849, outputs are 0.00024268508\n",
      "epoch: 2, step: 3850, outputs are 0.00024232005\n",
      "epoch: 2, step: 3851, outputs are 0.00024219631\n",
      "epoch: 2, step: 3852, outputs are 0.00024221023\n",
      "epoch: 2, step: 3853, outputs are 0.00024229518\n",
      "epoch: 2, step: 3854, outputs are 0.00024197802\n",
      "epoch: 2, step: 3855, outputs are 0.00024163973\n",
      "epoch: 2, step: 3856, outputs are 0.00024157418\n",
      "epoch: 2, step: 3857, outputs are 0.00024141691\n",
      "epoch: 2, step: 3858, outputs are 0.00024170014\n",
      "epoch: 2, step: 3859, outputs are 0.00024159142\n",
      "epoch: 2, step: 3860, outputs are 0.00024171147\n",
      "epoch: 2, step: 3861, outputs are 0.00024131143\n",
      "epoch: 2, step: 3862, outputs are 0.00024101272\n",
      "epoch: 2, step: 3863, outputs are 0.00024082727\n",
      "epoch: 2, step: 3864, outputs are 0.00024119543\n",
      "epoch: 2, step: 3865, outputs are 0.0002405878\n",
      "epoch: 2, step: 3866, outputs are 0.00024065647\n",
      "epoch: 2, step: 3867, outputs are 0.00024044632\n",
      "epoch: 2, step: 3868, outputs are 0.0002402962\n",
      "epoch: 2, step: 3869, outputs are 0.00024009822\n",
      "epoch: 2, step: 3870, outputs are 0.0002401501\n",
      "epoch: 2, step: 3871, outputs are 0.00023996257\n",
      "epoch: 2, step: 3872, outputs are 0.00023985586\n",
      "epoch: 2, step: 3873, outputs are 0.0002397189\n",
      "epoch: 2, step: 3874, outputs are 0.00023960578\n",
      "epoch: 2, step: 3875, outputs are 0.00023983007\n",
      "epoch: 2, step: 3876, outputs are 0.00023956235\n",
      "epoch: 2, step: 3877, outputs are 0.0002393252\n",
      "epoch: 2, step: 3878, outputs are 0.00023928168\n",
      "epoch: 2, step: 3879, outputs are 0.00023917702\n",
      "epoch: 2, step: 3880, outputs are 0.0002390174\n",
      "epoch: 2, step: 3881, outputs are 0.00023894549\n",
      "epoch: 2, step: 3882, outputs are 0.00023860604\n",
      "epoch: 2, step: 3883, outputs are 0.00023903954\n",
      "epoch: 2, step: 3884, outputs are 0.00023888699\n",
      "epoch: 2, step: 3885, outputs are 0.00023835545\n",
      "epoch: 2, step: 3886, outputs are 0.00023851304\n",
      "epoch: 2, step: 3887, outputs are 0.00023824297\n",
      "epoch: 2, step: 3888, outputs are 0.00023835036\n",
      "epoch: 2, step: 3889, outputs are 0.00023794855\n",
      "epoch: 2, step: 3890, outputs are 0.00023785897\n",
      "epoch: 2, step: 3891, outputs are 0.00023780914\n",
      "epoch: 2, step: 3892, outputs are 0.00023788423\n",
      "epoch: 2, step: 3893, outputs are 0.00023760743\n",
      "epoch: 2, step: 3894, outputs are 0.00023746694\n",
      "epoch: 2, step: 3895, outputs are 0.00023732992\n",
      "epoch: 2, step: 3896, outputs are 0.00023734046\n",
      "epoch: 2, step: 3897, outputs are 0.0002372901\n",
      "epoch: 2, step: 3898, outputs are 0.0002371745\n",
      "epoch: 2, step: 3899, outputs are 0.00023692616\n",
      "epoch: 2, step: 3900, outputs are 0.00023703744\n",
      "epoch: 2, step: 3901, outputs are 0.00023677584\n",
      "epoch: 2, step: 3902, outputs are 0.00023644941\n",
      "epoch: 2, step: 3903, outputs are 0.00023644064\n",
      "epoch: 2, step: 3904, outputs are 0.0002365055\n",
      "epoch: 2, step: 3905, outputs are 0.0002360299\n",
      "epoch: 2, step: 3906, outputs are 0.00023627082\n",
      "epoch: 2, step: 3907, outputs are 0.0002362289\n",
      "epoch: 2, step: 3908, outputs are 0.00023615487\n",
      "epoch: 2, step: 3909, outputs are 0.00023571734\n",
      "epoch: 2, step: 3910, outputs are 0.0002357446\n",
      "epoch: 2, step: 3911, outputs are 0.00023556584\n",
      "epoch: 2, step: 3912, outputs are 0.00023525878\n",
      "epoch: 2, step: 3913, outputs are 0.0002355245\n",
      "epoch: 2, step: 3914, outputs are 0.00023531605\n",
      "epoch: 2, step: 3915, outputs are 0.00023509207\n",
      "epoch: 2, step: 3916, outputs are 0.00023483204\n",
      "epoch: 2, step: 3917, outputs are 0.00023494266\n",
      "epoch: 2, step: 3918, outputs are 0.00023479066\n",
      "epoch: 2, step: 3919, outputs are 0.00023459269\n",
      "epoch: 2, step: 3920, outputs are 0.00023467737\n",
      "epoch: 2, step: 3921, outputs are 0.00023445347\n",
      "epoch: 2, step: 3922, outputs are 0.00023419093\n",
      "epoch: 2, step: 3923, outputs are 0.00023430411\n",
      "epoch: 2, step: 3924, outputs are 0.00023422422\n",
      "epoch: 2, step: 3925, outputs are 0.0002342345\n",
      "epoch: 2, step: 3926, outputs are 0.00023405688\n",
      "epoch: 2, step: 3927, outputs are 0.00023414695\n",
      "epoch: 2, step: 3928, outputs are 0.0002341984\n",
      "epoch: 2, step: 3929, outputs are 0.00023372227\n",
      "epoch: 2, step: 3930, outputs are 0.00023341719\n",
      "epoch: 2, step: 3931, outputs are 0.00023339214\n",
      "epoch: 2, step: 3932, outputs are 0.00023340245\n",
      "epoch: 2, step: 3933, outputs are 0.0002329738\n",
      "epoch: 2, step: 3934, outputs are 0.00023291251\n",
      "epoch: 2, step: 3935, outputs are 0.00023299444\n",
      "epoch: 2, step: 3936, outputs are 0.00023285877\n",
      "epoch: 2, step: 3937, outputs are 0.00023274972\n",
      "epoch: 2, step: 3938, outputs are 0.00023265713\n",
      "epoch: 2, step: 3939, outputs are 0.00023243621\n",
      "epoch: 2, step: 3940, outputs are 0.00023257291\n",
      "epoch: 2, step: 3941, outputs are 0.00023236636\n",
      "epoch: 2, step: 3942, outputs are 0.0002320383\n",
      "epoch: 2, step: 3943, outputs are 0.00023220401\n",
      "epoch: 2, step: 3944, outputs are 0.00023217699\n",
      "epoch: 2, step: 3945, outputs are 0.00023196242\n",
      "epoch: 2, step: 3946, outputs are 0.00023162767\n",
      "epoch: 2, step: 3947, outputs are 0.0002315935\n",
      "epoch: 2, step: 3948, outputs are 0.00023164885\n",
      "epoch: 2, step: 3949, outputs are 0.00023138637\n",
      "epoch: 2, step: 3950, outputs are 0.00023132146\n",
      "epoch: 2, step: 3951, outputs are 0.0002316378\n",
      "epoch: 2, step: 3952, outputs are 0.00023092105\n",
      "epoch: 2, step: 3953, outputs are 0.0002313019\n",
      "epoch: 2, step: 3954, outputs are 0.00023107941\n",
      "epoch: 2, step: 3955, outputs are 0.000230957\n",
      "epoch: 2, step: 3956, outputs are 0.00023074784\n",
      "epoch: 2, step: 3957, outputs are 0.00023069978\n",
      "epoch: 2, step: 3958, outputs are 0.00023038528\n",
      "epoch: 2, step: 3959, outputs are 0.00023089536\n",
      "epoch: 2, step: 3960, outputs are 0.00023029317\n",
      "epoch: 2, step: 3961, outputs are 0.00022991907\n",
      "epoch: 2, step: 3962, outputs are 0.00023000056\n",
      "epoch: 2, step: 3963, outputs are 0.00022986528\n",
      "epoch: 2, step: 3964, outputs are 0.0002299317\n",
      "epoch: 2, step: 3965, outputs are 0.00023013476\n",
      "epoch: 2, step: 3966, outputs are 0.0002295548\n",
      "epoch: 2, step: 3967, outputs are 0.000229881\n",
      "epoch: 2, step: 3968, outputs are 0.00022971636\n",
      "epoch: 2, step: 3969, outputs are 0.0002296152\n",
      "epoch: 2, step: 3970, outputs are 0.00022942635\n",
      "epoch: 2, step: 3971, outputs are 0.00022916446\n",
      "epoch: 2, step: 3972, outputs are 0.00022922906\n",
      "epoch: 2, step: 3973, outputs are 0.0002287536\n",
      "epoch: 2, step: 3974, outputs are 0.00022898914\n",
      "epoch: 2, step: 3975, outputs are 0.00022906598\n",
      "epoch: 2, step: 3976, outputs are 0.00022877328\n",
      "epoch: 2, step: 3977, outputs are 0.00022853736\n",
      "epoch: 2, step: 3978, outputs are 0.00022832191\n",
      "epoch: 2, step: 3979, outputs are 0.00022813061\n",
      "epoch: 2, step: 3980, outputs are 0.00022821581\n",
      "epoch: 2, step: 3981, outputs are 0.00022815386\n",
      "epoch: 2, step: 3982, outputs are 0.00022800967\n",
      "epoch: 2, step: 3983, outputs are 0.00022765742\n",
      "epoch: 2, step: 3984, outputs are 0.00022777608\n",
      "epoch: 2, step: 3985, outputs are 0.00022771674\n",
      "epoch: 2, step: 3986, outputs are 0.00022764926\n",
      "epoch: 2, step: 3987, outputs are 0.00022759964\n",
      "epoch: 2, step: 3988, outputs are 0.0002276625\n",
      "epoch: 2, step: 3989, outputs are 0.00022742101\n",
      "epoch: 2, step: 3990, outputs are 0.0002270382\n",
      "epoch: 2, step: 3991, outputs are 0.00022689419\n",
      "epoch: 2, step: 3992, outputs are 0.00022690542\n",
      "epoch: 2, step: 3993, outputs are 0.0002269106\n",
      "epoch: 2, step: 3994, outputs are 0.00022679531\n",
      "epoch: 2, step: 3995, outputs are 0.00022639535\n",
      "epoch: 2, step: 3996, outputs are 0.00022663415\n",
      "epoch: 2, step: 3997, outputs are 0.00022626012\n",
      "epoch: 2, step: 3998, outputs are 0.00022659528\n",
      "epoch: 2, step: 3999, outputs are 0.00022621799\n",
      "epoch: 2, step: 4000, outputs are 0.00022616549\n",
      "epoch: 2, step: 4001, outputs are 0.00022600303\n",
      "epoch: 2, step: 4002, outputs are 0.000225936\n",
      "epoch: 2, step: 4003, outputs are 0.000225799\n",
      "epoch: 2, step: 4004, outputs are 0.00022563475\n",
      "epoch: 2, step: 4005, outputs are 0.00022584305\n",
      "epoch: 2, step: 4006, outputs are 0.00022543881\n",
      "epoch: 2, step: 4007, outputs are 0.00022556927\n",
      "epoch: 2, step: 4008, outputs are 0.00022522235\n",
      "epoch: 2, step: 4009, outputs are 0.00022549908\n",
      "epoch: 2, step: 4010, outputs are 0.00022497216\n",
      "epoch: 2, step: 4011, outputs are 0.00022505345\n",
      "epoch: 2, step: 4012, outputs are 0.00022488809\n",
      "epoch: 2, step: 4013, outputs are 0.00022483818\n",
      "epoch: 2, step: 4014, outputs are 0.00022493627\n",
      "epoch: 2, step: 4015, outputs are 0.00022456422\n",
      "epoch: 2, step: 4016, outputs are 0.00022438214\n",
      "epoch: 2, step: 4017, outputs are 0.00022444817\n",
      "epoch: 2, step: 4018, outputs are 0.00022446271\n",
      "epoch: 2, step: 4019, outputs are 0.00022419676\n",
      "epoch: 2, step: 4020, outputs are 0.00022378367\n",
      "epoch: 2, step: 4021, outputs are 0.00022381553\n",
      "epoch: 2, step: 4022, outputs are 0.00022394411\n",
      "epoch: 2, step: 4023, outputs are 0.00022376336\n",
      "epoch: 2, step: 4024, outputs are 0.00022359916\n",
      "epoch: 2, step: 4025, outputs are 0.00022337299\n",
      "epoch: 2, step: 4026, outputs are 0.00022351071\n",
      "epoch: 2, step: 4027, outputs are 0.00022333919\n",
      "epoch: 2, step: 4028, outputs are 0.00022324304\n",
      "epoch: 2, step: 4029, outputs are 0.00022303339\n",
      "epoch: 2, step: 4030, outputs are 0.00022307016\n",
      "epoch: 2, step: 4031, outputs are 0.00022290839\n",
      "epoch: 2, step: 4032, outputs are 0.00022286118\n",
      "epoch: 2, step: 4033, outputs are 0.0002226798\n",
      "epoch: 2, step: 4034, outputs are 0.00022265657\n",
      "epoch: 2, step: 4035, outputs are 0.00022243962\n",
      "epoch: 2, step: 4036, outputs are 0.00022232815\n",
      "epoch: 2, step: 4037, outputs are 0.00022223016\n",
      "epoch: 2, step: 4038, outputs are 0.00022199671\n",
      "epoch: 2, step: 4039, outputs are 0.00022222954\n",
      "epoch: 2, step: 4040, outputs are 0.00022208196\n",
      "epoch: 2, step: 4041, outputs are 0.0002219\n",
      "epoch: 2, step: 4042, outputs are 0.00022183628\n",
      "epoch: 2, step: 4043, outputs are 0.00022180022\n",
      "epoch: 2, step: 4044, outputs are 0.00022158762\n",
      "epoch: 2, step: 4045, outputs are 0.00022159817\n",
      "epoch: 2, step: 4046, outputs are 0.00022166688\n",
      "epoch: 2, step: 4047, outputs are 0.00022137538\n",
      "epoch: 2, step: 4048, outputs are 0.0002212964\n",
      "epoch: 2, step: 4049, outputs are 0.00022124451\n",
      "epoch: 2, step: 4050, outputs are 0.0002210037\n",
      "epoch: 2, step: 4051, outputs are 0.0002207543\n",
      "epoch: 2, step: 4052, outputs are 0.0002207577\n",
      "epoch: 2, step: 4053, outputs are 0.00022108975\n",
      "epoch: 2, step: 4054, outputs are 0.00022091094\n",
      "epoch: 2, step: 4055, outputs are 0.00022043363\n",
      "epoch: 2, step: 4056, outputs are 0.00022050257\n",
      "epoch: 2, step: 4057, outputs are 0.00022010315\n",
      "epoch: 2, step: 4058, outputs are 0.00022011907\n",
      "epoch: 2, step: 4059, outputs are 0.00022002123\n",
      "epoch: 2, step: 4060, outputs are 0.00022006714\n",
      "epoch: 2, step: 4061, outputs are 0.00021989233\n",
      "epoch: 2, step: 4062, outputs are 0.00022004139\n",
      "epoch: 2, step: 4063, outputs are 0.0002197297\n",
      "epoch: 2, step: 4064, outputs are 0.00021957165\n",
      "epoch: 2, step: 4065, outputs are 0.00021946638\n",
      "epoch: 2, step: 4066, outputs are 0.00021931183\n",
      "epoch: 2, step: 4067, outputs are 0.00021917652\n",
      "epoch: 2, step: 4068, outputs are 0.00021929492\n",
      "epoch: 2, step: 4069, outputs are 0.00021933457\n",
      "epoch: 2, step: 4070, outputs are 0.00021918838\n",
      "epoch: 2, step: 4071, outputs are 0.00021881897\n",
      "epoch: 2, step: 4072, outputs are 0.0002187556\n",
      "epoch: 2, step: 4073, outputs are 0.00021864963\n",
      "epoch: 2, step: 4074, outputs are 0.00021847067\n",
      "epoch: 2, step: 4075, outputs are 0.00021865705\n",
      "epoch: 2, step: 4076, outputs are 0.00021862167\n",
      "epoch: 2, step: 4077, outputs are 0.00021833916\n",
      "epoch: 2, step: 4078, outputs are 0.00021855088\n",
      "epoch: 2, step: 4079, outputs are 0.00021819524\n",
      "epoch: 2, step: 4080, outputs are 0.00021797443\n",
      "epoch: 2, step: 4081, outputs are 0.0002179691\n",
      "epoch: 2, step: 4082, outputs are 0.0002176\n",
      "epoch: 2, step: 4083, outputs are 0.00021777031\n",
      "epoch: 2, step: 4084, outputs are 0.00021769239\n",
      "epoch: 2, step: 4085, outputs are 0.00021765786\n",
      "epoch: 2, step: 4086, outputs are 0.00021748302\n",
      "epoch: 2, step: 4087, outputs are 0.00021724202\n",
      "epoch: 2, step: 4088, outputs are 0.00021701113\n",
      "epoch: 2, step: 4089, outputs are 0.0002171786\n",
      "epoch: 2, step: 4090, outputs are 0.00021701785\n",
      "epoch: 2, step: 4091, outputs are 0.00021689647\n",
      "epoch: 2, step: 4092, outputs are 0.00021671108\n",
      "epoch: 2, step: 4093, outputs are 0.0002167504\n",
      "epoch: 2, step: 4094, outputs are 0.00021656204\n",
      "epoch: 2, step: 4095, outputs are 0.00021672268\n",
      "epoch: 2, step: 4096, outputs are 0.00021648809\n",
      "epoch: 2, step: 4097, outputs are 0.0002165918\n",
      "epoch: 2, step: 4098, outputs are 0.0002163782\n",
      "epoch: 2, step: 4099, outputs are 0.0002162468\n",
      "epoch: 2, step: 4100, outputs are 0.00021619513\n",
      "epoch: 2, step: 4101, outputs are 0.0002159168\n",
      "epoch: 2, step: 4102, outputs are 0.00021609073\n",
      "epoch: 2, step: 4103, outputs are 0.00021589934\n",
      "epoch: 2, step: 4104, outputs are 0.0002158533\n",
      "epoch: 2, step: 4105, outputs are 0.00021545509\n",
      "epoch: 2, step: 4106, outputs are 0.00021547006\n",
      "epoch: 2, step: 4107, outputs are 0.000215406\n",
      "epoch: 2, step: 4108, outputs are 0.00021528394\n",
      "epoch: 2, step: 4109, outputs are 0.00021547223\n",
      "epoch: 2, step: 4110, outputs are 0.00021502608\n",
      "epoch: 2, step: 4111, outputs are 0.00021498851\n",
      "epoch: 2, step: 4112, outputs are 0.00021486849\n",
      "epoch: 2, step: 4113, outputs are 0.0002148436\n",
      "epoch: 2, step: 4114, outputs are 0.00021474191\n",
      "epoch: 2, step: 4115, outputs are 0.00021463401\n",
      "epoch: 2, step: 4116, outputs are 0.00021444896\n",
      "epoch: 2, step: 4117, outputs are 0.00021436313\n",
      "epoch: 2, step: 4118, outputs are 0.00021438678\n",
      "epoch: 2, step: 4119, outputs are 0.00021445056\n",
      "epoch: 2, step: 4120, outputs are 0.0002143225\n",
      "epoch: 2, step: 4121, outputs are 0.00021412695\n",
      "epoch: 2, step: 4122, outputs are 0.00021410795\n",
      "epoch: 2, step: 4123, outputs are 0.0002138569\n",
      "epoch: 2, step: 4124, outputs are 0.00021386276\n",
      "epoch: 2, step: 4125, outputs are 0.00021347716\n",
      "epoch: 2, step: 4126, outputs are 0.00021365413\n",
      "epoch: 2, step: 4127, outputs are 0.0002136014\n",
      "epoch: 2, step: 4128, outputs are 0.00021359882\n",
      "epoch: 2, step: 4129, outputs are 0.00021326172\n",
      "epoch: 2, step: 4130, outputs are 0.00021320787\n",
      "epoch: 2, step: 4131, outputs are 0.00021310657\n",
      "epoch: 2, step: 4132, outputs are 0.0002132164\n",
      "epoch: 2, step: 4133, outputs are 0.00021304055\n",
      "epoch: 2, step: 4134, outputs are 0.00021258777\n",
      "epoch: 2, step: 4135, outputs are 0.00021296286\n",
      "epoch: 2, step: 4136, outputs are 0.00021260636\n",
      "epoch: 2, step: 4137, outputs are 0.00021248897\n",
      "epoch: 2, step: 4138, outputs are 0.00021241335\n",
      "epoch: 2, step: 4139, outputs are 0.00021243325\n",
      "epoch: 2, step: 4140, outputs are 0.00021233031\n",
      "epoch: 2, step: 4141, outputs are 0.0002121125\n",
      "epoch: 2, step: 4142, outputs are 0.00021218095\n",
      "epoch: 2, step: 4143, outputs are 0.00021219741\n",
      "epoch: 2, step: 4144, outputs are 0.00021206487\n",
      "epoch: 2, step: 4145, outputs are 0.0002118275\n",
      "epoch: 2, step: 4146, outputs are 0.00021180953\n",
      "epoch: 2, step: 4147, outputs are 0.00021148677\n",
      "epoch: 2, step: 4148, outputs are 0.00021144048\n",
      "epoch: 2, step: 4149, outputs are 0.00021135787\n",
      "epoch: 2, step: 4150, outputs are 0.00021109523\n",
      "epoch: 2, step: 4151, outputs are 0.00021129032\n",
      "epoch: 2, step: 4152, outputs are 0.00021136826\n",
      "epoch: 2, step: 4153, outputs are 0.0002110909\n",
      "epoch: 2, step: 4154, outputs are 0.00021074696\n",
      "epoch: 2, step: 4155, outputs are 0.00021112079\n",
      "epoch: 2, step: 4156, outputs are 0.00021058277\n",
      "epoch: 2, step: 4157, outputs are 0.00021072662\n",
      "epoch: 2, step: 4158, outputs are 0.00021068892\n",
      "epoch: 2, step: 4159, outputs are 0.00021036214\n",
      "epoch: 2, step: 4160, outputs are 0.00021035885\n",
      "epoch: 2, step: 4161, outputs are 0.00021043974\n",
      "epoch: 2, step: 4162, outputs are 0.00020997404\n",
      "epoch: 2, step: 4163, outputs are 0.00020996077\n",
      "epoch: 2, step: 4164, outputs are 0.00020990541\n",
      "epoch: 2, step: 4165, outputs are 0.00020990736\n",
      "epoch: 2, step: 4166, outputs are 0.00020982377\n",
      "epoch: 2, step: 4167, outputs are 0.00020978606\n",
      "epoch: 2, step: 4168, outputs are 0.00020962613\n",
      "epoch: 2, step: 4169, outputs are 0.0002095396\n",
      "epoch: 2, step: 4170, outputs are 0.00020937379\n",
      "epoch: 2, step: 4171, outputs are 0.00020947398\n",
      "epoch: 2, step: 4172, outputs are 0.00020926763\n",
      "epoch: 2, step: 4173, outputs are 0.00020935267\n",
      "epoch: 2, step: 4174, outputs are 0.0002090259\n",
      "epoch: 2, step: 4175, outputs are 0.00020897313\n",
      "epoch: 2, step: 4176, outputs are 0.00020903279\n",
      "epoch: 2, step: 4177, outputs are 0.00020884536\n",
      "epoch: 2, step: 4178, outputs are 0.00020888551\n",
      "epoch: 2, step: 4179, outputs are 0.00020875139\n",
      "epoch: 2, step: 4180, outputs are 0.00020858005\n",
      "epoch: 2, step: 4181, outputs are 0.00020840259\n",
      "epoch: 2, step: 4182, outputs are 0.00020862796\n",
      "epoch: 2, step: 4183, outputs are 0.00020832654\n",
      "epoch: 2, step: 4184, outputs are 0.00020840538\n",
      "epoch: 2, step: 4185, outputs are 0.00020803104\n",
      "epoch: 2, step: 4186, outputs are 0.00020807664\n",
      "epoch: 2, step: 4187, outputs are 0.00020797143\n",
      "epoch: 2, step: 4188, outputs are 0.00020775595\n",
      "epoch: 2, step: 4189, outputs are 0.00020771107\n",
      "epoch: 2, step: 4190, outputs are 0.00020780848\n",
      "epoch: 2, step: 4191, outputs are 0.00020730075\n",
      "epoch: 2, step: 4192, outputs are 0.0002072449\n",
      "epoch: 2, step: 4193, outputs are 0.00020756767\n",
      "epoch: 2, step: 4194, outputs are 0.00020731999\n",
      "epoch: 2, step: 4195, outputs are 0.00020697305\n",
      "epoch: 2, step: 4196, outputs are 0.00020699122\n",
      "epoch: 2, step: 4197, outputs are 0.00020714253\n",
      "epoch: 2, step: 4198, outputs are 0.00020660914\n",
      "epoch: 2, step: 4199, outputs are 0.00020690076\n",
      "epoch: 2, step: 4200, outputs are 0.00020651094\n",
      "epoch: 2, step: 4201, outputs are 0.00020649483\n",
      "epoch: 2, step: 4202, outputs are 0.00020664696\n",
      "epoch: 2, step: 4203, outputs are 0.00020660643\n",
      "epoch: 2, step: 4204, outputs are 0.00020630375\n",
      "epoch: 2, step: 4205, outputs are 0.00020627906\n",
      "epoch: 2, step: 4206, outputs are 0.00020624282\n",
      "epoch: 2, step: 4207, outputs are 0.00020579394\n",
      "epoch: 2, step: 4208, outputs are 0.00020602466\n",
      "epoch: 2, step: 4209, outputs are 0.00020573752\n",
      "epoch: 2, step: 4210, outputs are 0.0002056835\n",
      "epoch: 2, step: 4211, outputs are 0.00020564655\n",
      "epoch: 2, step: 4212, outputs are 0.00020563666\n",
      "epoch: 2, step: 4213, outputs are 0.00020540241\n",
      "epoch: 2, step: 4214, outputs are 0.00020548937\n",
      "epoch: 2, step: 4215, outputs are 0.00020529756\n",
      "epoch: 2, step: 4216, outputs are 0.00020518052\n",
      "epoch: 2, step: 4217, outputs are 0.00020504468\n",
      "epoch: 2, step: 4218, outputs are 0.00020505118\n",
      "epoch: 2, step: 4219, outputs are 0.00020506838\n",
      "epoch: 2, step: 4220, outputs are 0.00020501867\n",
      "epoch: 2, step: 4221, outputs are 0.00020486358\n",
      "epoch: 2, step: 4222, outputs are 0.00020482038\n",
      "epoch: 2, step: 4223, outputs are 0.000204568\n",
      "epoch: 2, step: 4224, outputs are 0.00020428078\n",
      "epoch: 2, step: 4225, outputs are 0.00020455393\n",
      "epoch: 2, step: 4226, outputs are 0.00020434905\n",
      "epoch: 2, step: 4227, outputs are 0.00020418139\n",
      "epoch: 2, step: 4228, outputs are 0.0002040774\n",
      "epoch: 2, step: 4229, outputs are 0.0002039693\n",
      "epoch: 2, step: 4230, outputs are 0.0002040045\n",
      "epoch: 2, step: 4231, outputs are 0.00020380819\n",
      "epoch: 2, step: 4232, outputs are 0.00020372638\n",
      "epoch: 2, step: 4233, outputs are 0.00020393144\n",
      "epoch: 2, step: 4234, outputs are 0.00020349633\n",
      "epoch: 2, step: 4235, outputs are 0.00020333997\n",
      "epoch: 2, step: 4236, outputs are 0.00020339288\n",
      "epoch: 2, step: 4237, outputs are 0.00020335315\n",
      "epoch: 2, step: 4238, outputs are 0.00020334189\n",
      "epoch: 2, step: 4239, outputs are 0.00020325805\n"
     ]
    }
   ],
   "source": [
    "netwithgrads = BertEvaluationCell(netwithloss, optimizer=optimizer)\n",
    "\n",
    "model = Model(netwithgrads)\n",
    "model.train(repeat_count, train_dataset, callbacks=callback,\n",
    "            dataset_sink_mode=(args_opt.enable_data_sink == 'true'),\n",
    "            sink_size=args_opt.data_sink_steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
