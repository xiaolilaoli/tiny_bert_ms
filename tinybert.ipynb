{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d522811-c8b4-4fca-a6b6-81a339d75192",
   "metadata": {},
   "source": [
    "    # **TinyBERT实现案例**\n",
    "\n",
    "    BERT模型是NLP领域最著名的模型之一，它的出现带动了NLP领域预训练+微调方法的快速发展。BERT模型拥有优秀的自然语言理解能力，但模型参数庞大，训练时间耗费较长。TinyBERT是缩小版的BERT，对BERT的架构进行了简化。从推理角度看，TinyBERT比BERT-base（BERT模型基础版本）体积小了7.5倍、速度快了9.4倍，自然语言理解的性能表现更突出。\n",
    "\n",
    "    **论文** :\n",
    "\n",
    "    Transformer：https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html\n",
    "\n",
    "    bert：https://arxiv.org/abs/1810.04805 \n",
    "\n",
    "    tinyBERT:https://arxiv.org/abs/1909.10351"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2925c10-4c0b-4221-a459-2d314458278c",
   "metadata": {},
   "source": [
    "# BERT介绍\n",
    "TinyBERT的模型架构依然是BERT的架构，因此如果想掌握tinyBERT的结构，那么就必须知\n",
    "道BERT的模型结构。反过来说，如果了解BERT的模型结构，那么自然会对tinyBERT有了清晰的认知。因此这里先对BERT的内部结构进行介绍。\n",
    "\n",
    "  了解BERT模型之前，希望你对Transformer架构已经有了基本的了解。Transformer来源于自注意力机制。简单的介绍一下，Transformer的结构分为编码器和解码器，编码器可以把文字编码成高纬度的特征，而解码器可以用这些特征去完成相应任务，比如生成翻译。Transformer非常强大，NLP届的两大巨头模型都是由他的一部分改变而来，如下图。GPT取用的是Transformer的解码器部分，而BERT取的是Transformer的编码器，因此Bert的作用，大家可以理解为一个编码器,将输入的文字抽取为高维的特征。\n",
    "   ![Transformer](ipyphoto/BERTandGPT.jpg)\n",
    "                             \n",
    "                                 \n",
    "                                \n",
    "  <div align=\"center\"><i>图1：Transformer, BERT 和 GPT </i></div>\n",
    "\n",
    "   在CV领域，在imagenet上训练一个编码器，迁移到其他任务，已经是惯用手段了。 而在BERT以前，NLP领域使用预训练技术的并不多。BERT是在大规模的无标注文本上进行预训练的，它为无标注的文本设计了两个自监督预训练任务，第一个是MLM（Masked Language Model）任务： 遮盖掉句子中一定比例的单词，使用剩余单词来预测被遮盖的词。 第二个是NSP（Next Sentence Prediction）任务：预测输入的两个句子是否是上下文关系。这样，无需标注即可在海量文本数据上进行预训练，得到一个效果十分良好的编码器，然后使用此编码器在其他文本任务上进行微调。BERT所建立的预训练加微调的模式在后续的NLP网络中得到了广泛的应用。\n",
    "   \n",
    "## BERT结构  \n",
    "   BERT模型主要由三部分构成：嵌入层（embedding），BERT layer堆叠层， 和输出层。 一句文本，首先会被分词，然后进入BERT，经过嵌入层，得到嵌入向量。之后嵌入向量会进入BERT layer的自注意力模块中进行提取，最后得到深度特征，最后经过输出层输出。下面详细介绍这个步骤。\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "   <img src=\"ipyphoto/BERT.png\" alt=\"Drawing\" style=\"width: 400px;\" align=\"mid\"/>\n",
    "                      \n",
    "\n",
    "                               \n",
    "   <div align=\"center\"><i>图2： BERT结构 </i></div>\n",
    "   \n",
    "   \n",
    "   \n",
    "   **一，嵌入层（embedding layer）**：\n",
    "   \n",
    "   嵌入，有时候又称为向量化。就是把输入映射为高维的向量。比如输入一个词，一般只有一个编号，是一维的。在嵌入层会被转换成高维的向量。\n",
    "   \n",
    "   <img src=\"ipyphoto/trans_emb.jpg\" alt=\"Drawing\" style=\"width: 500px;\" align=\"mid\"/>\n",
    "   \n",
    "                                               \n",
    "   <div align=\"center\"><i>图3： Transformer嵌入层 </i></div>\n",
    "   BERT源自Transformer，他们的嵌入层也是很相似的。上图是Transformer的嵌入层，我们可以看到，当一个分词变为输入时，首先要经过词嵌入（Embedding），变为长度为hidden_size的向量（图中为6，实际一般很长），字所在的位置也要进行嵌入。两个嵌入向量直接数值相加，我们就得到了嵌入层的输出。\n",
    "   \n",
    "   下图则是BERT的嵌入层，它的输出来源于三个嵌入向量相加。我们可以看到有两个区别。1：增加了句子嵌入（segment Embeddings），因为BERT的输入是两个句子，因此要在这里用句子嵌入值标识出是哪一句。2：出现了字符token，如E[cls],E[sep]。CLStoken一般用来统计全局的信息，最后可以用此token的特征进行下游分类任务。SEPtoken则表示句子的分割和中止。  其实还存在着第三个区别：Transformer的位置嵌入是固定的，也就是公式算出的值，而BERT的位置嵌入则是模型训练得到的。（其他嵌入也都是训练得到）。\n",
    "   \n",
    "   <img src=\"ipyphoto/BERT_emb.jpg\" alt=\"Drawing\" style=\"width: 700px;\" align=\"mid\"/>\n",
    "   \n",
    "                                             \n",
    "           \n",
    "  <div align=\"center\"><i>图4： BERT嵌入层 </i></div>\n",
    "   \n",
    "   \n",
    "   \n",
    "   **二，多层BERT layer（BERT layers）**：\n",
    "   \n",
    "   这部分是BERT的核心部分，一般BERT系列的模型都会堆叠多个BERT层。首先需要知道的是，每个BERT layer ，都是不改变特征的维度的（如下图）。由于输入和输出维度一直相同，因此可以堆叠无数层。根据模型的大小，层数会有变化，BERT的base模型堆叠了12层，large模型堆叠了24层。而基础tinyBERT模型，只有4层。\n",
    "   \n",
    "   <img src=\"ipyphoto/BERT_layer.png\" alt=\"Drawing\" style=\"width: 700px;\" align=\"mid\"/>\n",
    "   \n",
    "<div align=\"center\"><i>图5： 多层BERT layer </i></div>\n",
    "\n",
    "\n",
    "   单个BERT layer是由多头自注意力层和MLP组成的。首先介绍多头自注意力层。自注意力层，有时候写作Transformer blocks，是进行特征交互，提取的关键部分。如下图所示。 图中的a1，a2与上图是对应的，指的就是一个token的特征。每一个token，都会分别经过三个不同的线性映射（也就是三个全连接），得到query,key和value(q,k,v)。然后对于每一个token，它的q会与其他token的k相乘得到一个权重，对应的 v 按照这些权重加起来，就得到了这个token的输出。\n",
    "   \n",
    "   <img src=\"ipyphoto/self_att.png\" alt=\"Drawing\" style=\"width: 700px;\" align=\"mid\"/>\n",
    "   <div align=\"center\"><i>图6： 自注意力机制 </i></div>\n",
    "   \n",
    "                                          \n",
    "   一般在模型中使用的是多头注意力机制。通过上图，我们了解到了注意力机制是如何工作的，那么多头注意力机制其实非常简单，就是将一个长维度的向量，分发到多个head中，多个token的向量在对应的head内计算输出，最后合起来。 举例说明：如下图。 一个长为6的特征 被分到3个注意力头中。每个头中，仅需长度为6/3=2的特征计算自注意力，最后得到3个长度为2的特征，再拼在一起就得到了输出。维度和输入相同，每一个token都这样操作，因此能保证输入输出的维度不变。\n",
    "      <img src=\"ipyphoto/multihead.png\" alt=\"Drawing\" style=\"width: 700px;\" align=\"mid\"/>\n",
    "      <div align=\"center\"><i> 图7： 多头自注意力 </i></div>\n",
    "   \n",
    "                                         \n",
    "   向量经过多头注意力层编码后，会经过MLP层。mlp层是两次线性映射，首先通过一个全连接从长度$L$的向量变为$L*ratio$，然后再通过一个全连接从$L*ratio$ 到$L$。当然在BERT层中也加入了残差连接的结构。\n",
    "   \n",
    "   用一个实例的参数来回顾多头注意力层：对于一个BERT-base模型，他的特征长度L为768，注意力头的个数为12.这样每个头计算的特征长度就为64. 而ratio值为4.也就是先从768映射到到3072，再从3072到768.而他的输入token数量，最大为512。\n",
    "   \n",
    "   \n",
    "   **三，输出层（pooler out）**：\n",
    "   输出前面已经提到了，与输入是一样的。 如输入是$L_{token} *L_{emb}$ 那么输出依然是$L_{token} *L_{emb}$（$L_{token}$:token数量，$L_{emb}$：特征维度）。你可以认为输入和输出是一一对应的，也可以认为他们并不对应。BERT的工作其实到这里就结束了，我们要得到的就是一个编码器而已。 \n",
    "   \n",
    "   输出层又作 池化输出。也就是将$L_{token} *L_{emb}$ 的特征池化为$1*L_{emb}$。一般常用的是将第一个token的特征作为池化输出，当然也可以采取平均池化等方式得到。\n",
    "   \n",
    "   BERT的两个预训练任务，其实也表示了bert完成生成和分类两个下游任务的一般方式。 MLM任务： 可以取遮盖token对应的输出token的特征，通过一个分类，得到输出的词，这样可以做生成任务。SEP任务： 取第一个token的特征，也就是CLStoken 进行二分类。\n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "这就是BERT的整体结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e709016a-70d8-431a-a205-32ec01a18b01",
   "metadata": {},
   "source": [
    "# tinyBERT\n",
    "tinyBERT是缩小版的BERT，与BERT的基础模型BERT-base存在着结构上的差异。此外，与BERT系列其他模型，如BERT-small，BERT-large等，不同的是，tinyBERT并没有采取直接在大规模预训练数据上无监督训练，在下游数据集微调的方法。而是对BERT-base模型进行蒸馏学习，来获取优秀的性能。\n",
    "\n",
    "\n",
    "## tinyBERT结构：\n",
    "为了介绍tinyBERT的结构，我们先来读一个BERT的设置文档BERT config，一个config便可以决定一个BERT的结构。\n",
    "     \n",
    "     tinyBERT:\n",
    "    {\n",
    "      \"hidden_size\": 384,                      #决定token被编码的长度，即特征长度，$L_{emb}$ \n",
    "\n",
    "      \"intermediate_size\": 1536,                # MLP层第一次映射的长度，这里特征长度乘以4\n",
    "\n",
    "      \"max_position_embeddings\": 512,                # 最大的输入长度。\n",
    "\n",
    "      \"model_type\": \"tiny_bert\",                \n",
    "\n",
    "      \"num_attention_heads\": 12,               # 注意力头个数\n",
    "\n",
    "      \"num_hidden_layers\": 4,                 # 堆叠多少层\n",
    "\n",
    "      \"vocab_size\": 30522                          # 训练词典个数，与训练语料有关\n",
    "\n",
    "    }\n",
    "\n",
    "    BERT-base:\n",
    "\n",
    "    {\n",
    "      \"hidden_size\": 768,           \n",
    "\n",
    "      \"intermediate_size\": 3072,\n",
    "\n",
    "      \"max_position_embeddings\": 512,\n",
    "\n",
    "      \"model_type\": \"bert\",\n",
    "\n",
    "      \"num_attention_heads\": 12,\n",
    "\n",
    "      \"num_hidden_layers\": 12,\n",
    "\n",
    "      \"vocab_size\": 30522\n",
    "\n",
    "    }\n",
    "BERT模型的结构主要由上面这些参数决定。其中上方是tinyBERT,下方是BERT-base。我们可以看到他们结构上的不同之处。参数的具体意思，可以参考上面的BERT结构。 \n",
    "    在tinyBERT中，首先是词被编码的特征维度减少一半，变为384。对应的mlp层的映射层维度也减少一半（保持四倍）。 BERTlayer变为4层。注意力头个数不变，这样每个注意力头中的特征自然也会减少至一半。\n",
    "    这样tinyBERT自然会参数减少很多。\n",
    "\n",
    "## 蒸馏学习：\n",
    "\n",
    "  参数量的降低，一般就会带来性能的降低。为了拥有良好的性能，tinyBERT并不像BERT家族那样在无标注上预训练，而是采取了蒸馏学习的方式进行训练。通过对BERT-base的蒸馏，得到了很好的性能。简单介绍一下蒸馏学习。 在一般的蒸馏学习中，有一个teacher模型和一个student模型。通过让student模型的输出去模仿teacher模型的输出，即让他们的输出靠的更近，来对student模型进行训练。\n",
    "  \n",
    "  与一般的蒸馏不同的是，tinyBERT是在模型的前向过程中进行多次蒸馏。而且在整个训练过程也进行多次蒸馏。\n",
    "  \n",
    "### 前向过程中的多次蒸馏：\n",
    "    \n",
    "   tinyBERT，对于n层的teacher bert，设计了一个mapping function ：n = g ( m )， 将student bert的第m层映射为原来的teacher的第n层，即让tinyBERT的第m层的输出去靠近teacher bert第n层的输出。 其实这个映射函数非常简单，就是$n = k*m$。k就是多少层当作tinyBERT的一层。当m=0时，对应的就是embedding layer。我们可以通过下图理解。图中仅为示例，tinyBERT每层的输出都去蒸馏学习Teacher net三层的输出，就是“一层顶三层”。\n",
    "   \n",
    "   <img src=\"ipyphoto/zhengliu.png\" alt=\"Drawing\" style=\"width: 700px;\" align=\"mid\"/>\n",
    "     <div align=\"center\"><i> 图8： 蒸馏对应层 </i></div>\n",
    "   \n",
    "   实际上的BERT-base有12层， 对于4层的tinyBERT，正好是三层对一层。 对于蒸馏学习，我们需要根据两个模型对应层的输出来计算loss，更新模型。从上图中，我们可以看到一共有四种loss，下面分别介绍。\n",
    "   \n",
    "   \n",
    "-   **Embedding-layer distillation**\n",
    "\n",
    "   <img src=\"ipyphoto/l_emb.png\" alt=\"Drawing\" style=\"width: 700px;\" align=\"mid\"/>\n",
    "   \n",
    "  这个是对embedding 矩阵的蒸馏loss，说是矩阵，其实是计算两个模型embedding输出的MSEloss。 而因为student的embedding层的特征维度和Teacher是不一样的，因此要乘上一个转换的映射矩阵$W_e$，此矩阵在模型中是一层全连接，在训练时学习。\n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "-   **Attention based distillation and Hidden states based distillation**\n",
    "\n",
    "\n",
    "\n",
    "  前文我们提到，BERT layer每一层包含两部分，一个是自注意力层，一个是MLP（也就是两层全连接）。attention指的是注意力层注意力分数矩阵，也就是对q和k乘算出来的那个值的蒸馏学习。 下面的hidden states层的蒸馏就是指的对MLP层的输出进行蒸馏学习。\n",
    "\n",
    " <img src=\"ipyphoto/l_hid.png\" alt=\"Drawing\" style=\"width: 700px;\" align=\"mid\"/>\n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "-   **Prediction-layer distillation**\n",
    "\n",
    "\n",
    "<img src=\"ipyphoto/l_pre.png\" alt=\"Drawing\" style=\"width: 700px;\" align=\"mid\"/>\n",
    "\n",
    "  pred蒸馏，是最初的蒸馏方法。是对最终输出层输出结果的softmax进行蒸馏学习。T是蒸馏学习中的温度系数。对输出蒸馏时，采用的是带温度系数的交叉熵loss。\n",
    "   \n",
    " \n",
    " \n",
    " ### 训练过程中的多次蒸馏：\n",
    " \n",
    " <img src=\"ipyphoto/train.png\" alt=\"Drawing\" style=\"width: 700px;\" align=\"mid\"/>\n",
    "    <div align=\"center\"><i> tinyBERT训练过程 </i></div>\n",
    "     \n",
    " tinyBERT 并不是像其他蒸馏那样，直接根据成品的Teacher model在分类时蒸馏，而是去模型BERT的训练过程，在预训练和微调阶段都进行蒸馏。我们知道BERT模型使用时要经过预训练和下游任务微调。所以tinyBERT的蒸馏同样分为两步：General Distillation 与 Task-specific Distillation。前者是对BERT在大规模语料库进行预训练蒸馏学习，后者则是在特定的任务上进行蒸馏学习。值得注意的是，在预训练蒸馏阶段，使用的Teacher模型是仅仅经过预训练未微调的BERT，而在特定任务分类蒸馏训练阶段，使用的Teacher 模型是在特定任务上经过微调的BERT。  \n",
    "\n",
    "\n",
    "\n",
    "## 总结：\n",
    "\n",
    "这一部分介绍了tinyBERT结构和训练的过程。可以从参数设置部分看出tinyBERT是如何缩减参数量的，又可以从蒸馏学习部分看出tinyBERT是如何在参数量大大减少的同时保持优秀的性能。下面我们将通过具体的代码对tinyBERT的结构和训练过程进行更加详细的介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3046ca04-2388-4f7b-8922-0b374ad22c4d",
   "metadata": {},
   "source": [
    "# **训练数据模型准备**\n",
    "\n",
    "BERT是在大规模无标注数据上进行预训练的，这里采用的是在wiki上下载文章进行预训练。 ms提供了处理这些数据文件的接口，但我们要把数据转为tfrecord格式或者是ms格式。 \n",
    "\n",
    "https://mp.csdn.net/mp_blog/creation/editor/127061849\n",
    "\n",
    "\n",
    "## 生成通用蒸馏阶段数据集，即wiki数据。\n",
    "\n",
    "下载[wiki](http://t.zoukankan.com/dhName-p-11859318.html)数据集进行预训练，\n",
    "\n",
    "使用[WikiExtractor](https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fattardi%2Fwikiextractor)提取和整理数据集中的文本，使用步骤如下：\n",
    "\n",
    "pip install wikiextractor\n",
    "python -m wikiextractor.WikiExtractor [Wikipedia dump file] -o [output file path] -b 2G\n",
    "\n",
    "## 得到训练好的BERT模型，即teacher模型。\n",
    "下载[BERT](https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fgoogle-research%2Fbert)代码仓，并下载模型文件[BERT-Base, Uncased](https://gitee.com/link?target=https%3A%2F%2Fstorage.googleapis.com%2Fbert_models%2F2018_10_18%2Funcased_L-12_H-768_A-12.zip)，其中包含了转化需要使用的vocab.txt, bert_config.json和预训练模型\n",
    "\n",
    "使用create_pretraining_data.py文件，将下载得到的文件转化成tfrecord数据集，详细用法请参考bert的readme文件，其中input_file第2步会生成多个文本文件，请转化为bert0.tfrecord-bertx.tfrecord，如果出现AttributeError: module 'tokenization' has no attribute 'FullTokenizer’，请安装bert-tensorflow\n",
    "\n",
    "将下载得到的tensorflow模型转化为mindspore模型，注意这个需要环境中同时存在tensorflow 和 mindspore。PATH为bert模型存放位置。\n",
    "\n",
    "```python3\n",
    "cd bert/ms2tf\n",
    "python ms_and_tf_checkpoint_transfer_tools.py --tf_ckpt_path=PATH/model.ckpt　\\\n",
    "    --new_ckpt_path=PATH/ms_model_ckpt.ckpt　\\\n",
    "    --tarnsfer_option=tf2ms\n",
    "```\n",
    "\n",
    "## 生成下游任务蒸馏阶段数据集\n",
    "\n",
    "下载数据集进行微调和评估，如GLUE，使用download_glue_data.py脚本下载SST2, MNLI, QNLI数据集等。后面的训练是以QNLI为例。\n",
    "\n",
    "将数据集文件从JSON格式转换为TFRecord格式。使用通用蒸馏阶段的第三步BERT代码，在处理QNLI数据时，需要加入QNLI数据集的处理代码。参考readme使用代码仓中的run_classifier.py文件。  run_classifier.py代码中包含了训练，推理和预测的代码，对于转化tfrecord数据集来说，这部分代码是多余的，可以将这部分代码注释掉，只保留转化数据集的代码．其中task_name指定为QNLI，bert_config_file指定为通用蒸馏阶段下载得到的bert_config.json文件，max_seq_length为64. 更详细的下载转换方式见 [数据下载转换指南](https://mp.csdn.net/mp_blog/creation/editor/127061849)\n",
    "\n",
    "~~~python3\n",
    "...\n",
    "class QnliProcessor(DataProcessor):\n",
    "\"\"\"Processor for the QNLI data set (GLUE version).\"\"\"\n",
    "\n",
    "def get_train_examples(self, data_dir):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return self._create_examples(\n",
    "        self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "def get_dev_examples(self, data_dir):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return self._create_examples(\n",
    "        self._read_tsv(os.path.join(data_dir, \"dev.tsv\")),\n",
    "                       \"dev_matched\")\n",
    "\n",
    "def get_labels(self):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return [\"entailment\", \"not_entailment\"]\n",
    "\n",
    "def _create_examples(self, lines, set_type):\n",
    "    \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "    examples = []\n",
    "    for (i, line) in enumerate(lines):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        guid = \"%s-%s\" % (set_type, line[0])\n",
    "        text_a = line[1]\n",
    "        text_b = line[2]\n",
    "        label = line[-1]\n",
    "        examples.append(\n",
    "            InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "    return examples\n",
    "...\n",
    "\"qnli\": QnliProcessor,\n",
    "...\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdd2cc8-34a7-4b4a-b6a8-b13f6218bab2",
   "metadata": {},
   "source": [
    "# 代码部分\n",
    "    \n",
    "## 导入环境\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd779868-d3d2-4502-9614-e277da9b8487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import log as logger\n",
    "import mindspore.nn as nn\n",
    "from mindspore import context\n",
    "from mindspore.ops import functional as F\n",
    "from mindspore.ops import composite as C\n",
    "from mindspore.common.parameter import Parameter\n",
    "from mindspore.communication.management import get_group_size\n",
    "from mindspore.nn.wrap.grad_reducer import DistributedGradReducer\n",
    "from mindspore.context import ParallelMode\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "from mindspore import Tensor\n",
    "from mindspore.train.callback import Callback\n",
    "from mindspore.train.serialization import save_checkpoint\n",
    "from mindspore.ops import operations as P\n",
    "import mindspore.communication.management as D\n",
    "from mindspore.nn.learning_rate_schedule import LearningRateSchedule, PolynomialDecayLR, WarmUpLR\n",
    "import mindspore.common.dtype as mstype\n",
    "from mindspore.common.initializer import TruncatedNormal, initializer\n",
    "import mindspore.dataset as ds\n",
    "from mindspore.dataset import transforms\n",
    "from mindspore.common import set_seed\n",
    "from mindspore.nn.optim import AdamWeightDecay\n",
    "from mindspore.train.model import Model\n",
    "from mindspore.train.callback import TimeMonitor\n",
    "from mindspore.nn.wrap.loss_scale import DynamicLossScaleUpdateCell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40fe0762-b753-4c2e-b9fc-37f3919f5a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import math\n",
    "import copy\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import ast\n",
    "from pprint import pformat\n",
    "import yaml\n",
    "import argparse\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a491b2c-9669-4d84-b032-4bf021620340",
   "metadata": {},
   "source": [
    "# 模型代码部分\n",
    "\n",
    "这一部分，会定义所有使用到的模型代码。看代码时，可以对照上面对bert模型的结构介绍来看。\n",
    "\n",
    "\n",
    "## BERT config\n",
    "BERT的config是模型构建的依据所在。规定了模型的整体结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "277c304f-6dbf-47f5-bec0-6a001fce3c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertConfig:\n",
    "    \"\"\"\n",
    "    Configuration for `BertModel`.\n",
    "\n",
    "    Args:\n",
    "        seq_length (int): Length of input sequence. Default: 128.\n",
    "        vocab_size (int): The shape of each embedding vector. Default: 32000.\n",
    "        hidden_size (int): Size of the bert encoder layers. Default: 768.\n",
    "        num_hidden_layers (int): Number of hidden layers in the BertTransformer encoder\n",
    "                           cell. Default: 12.\n",
    "        num_attention_heads (int): Number of attention heads in the BertTransformer\n",
    "                             encoder cell. Default: 12.\n",
    "        intermediate_size (int): Size of intermediate layer in the BertTransformer\n",
    "                           encoder cell. Default: 3072.\n",
    "        hidden_act (str): Activation function used in the BertTransformer encoder\n",
    "                    cell. Default: \"gelu\".\n",
    "        hidden_dropout_prob (float): The dropout probability for BertOutput. Default: 0.1.\n",
    "        attention_probs_dropout_prob (float): The dropout probability for\n",
    "                                      BertAttention. Default: 0.1.\n",
    "        max_position_embeddings (int): Maximum length of sequences used in this\n",
    "                                 model. Default: 512.\n",
    "        type_vocab_size (int): Size of token type vocab. Default: 16.\n",
    "        initializer_range (float): Initialization value of TruncatedNormal. Default: 0.02.\n",
    "        use_relative_positions (bool): Specifies whether to use relative positions. Default: False.\n",
    "        dtype (:class:`mindspore.dtype`): Data type of the input. Default: mstype.float32.\n",
    "        compute_type (:class:`mindspore.dtype`): Compute type in BertTransformer. Default: mstype.float32.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 seq_length=128,\n",
    "                 vocab_size=32000,\n",
    "                 hidden_size=768,\n",
    "                 num_hidden_layers=12,\n",
    "                 num_attention_heads=12,\n",
    "                 intermediate_size=3072,\n",
    "                 hidden_act=\"gelu\",\n",
    "                 hidden_dropout_prob=0.1,\n",
    "                 attention_probs_dropout_prob=0.1,\n",
    "                 max_position_embeddings=512,\n",
    "                 type_vocab_size=16,\n",
    "                 initializer_range=0.02,\n",
    "                 use_relative_positions=False,\n",
    "                 dtype=mstype.float32,\n",
    "                 compute_type=mstype.float32):\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.hidden_act = hidden_act\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.type_vocab_size = type_vocab_size\n",
    "        self.initializer_range = initializer_range\n",
    "        self.use_relative_positions = use_relative_positions\n",
    "        self.dtype = dtype\n",
    "        self.compute_type = compute_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d5c48c-0ca3-4181-aecd-51144dd47c0c",
   "metadata": {},
   "source": [
    "## BERT模型。\n",
    "bert的整体模型可以对照图片和代码。\n",
    "\n",
    "<img src=\"ipyphoto/BERT.png\" alt=\"Drawing\" style=\"width: 400px;\" align=\"mid\"/>\n",
    "\n",
    "对应代码中是，\n",
    "- EmbeddingPostprocessor \n",
    "- BertTransformer\n",
    "- pooler out\n",
    "\n",
    "但是ms官方的代码中，并没有严格按照这个来行。 输入的embedding和 pooler 都放在主体部分进行，而 位置和句子的embedding放在EmbeddingPostprocessor中进行。bert layers 在BertTransformer中。\n",
    "\n",
    "## 位置和句子embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "839d40d7-866e-4c81-a765-042e8e5c256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingPostprocessor(nn.Cell):\n",
    "    \"\"\"\n",
    "    Postprocessors apply positional and token type embeddings to word embeddings.\n",
    "\n",
    "    Args:\n",
    "        embedding_size (int): The size of each embedding vector.\n",
    "        embedding_shape (list): [batch_size, seq_length, embedding_size], the shape of\n",
    "                         each embedding vector.\n",
    "        use_token_type (bool): Specifies whether to use token type embeddings. Default: False.\n",
    "        token_type_vocab_size (int): Size of token type vocab. Default: 16.\n",
    "       use_one_hot_embeddings (bool): Specifies whether to use one hot encoding form. Default: False.\n",
    "        initializer_range (float): Initialization value of TruncatedNormal. Default: 0.02.\n",
    "        max_position_embeddings (int): Maximum length of sequences used in this\n",
    "                                 model. Default: 512.\n",
    "        dropout_prob (float): The dropout probability. Default: 0.1.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 use_relative_positions,\n",
    "                 embedding_size,\n",
    "                 embedding_shape,\n",
    "                 use_token_type=False,\n",
    "                 token_type_vocab_size=16,\n",
    "                 use_one_hot_embeddings=False,\n",
    "                 initializer_range=0.02,\n",
    "                 max_position_embeddings=512,\n",
    "                 dropout_prob=0.1):\n",
    "        super(EmbeddingPostprocessor, self).__init__()\n",
    "        self.use_token_type = use_token_type\n",
    "        self.token_type_vocab_size = token_type_vocab_size\n",
    "        self.use_one_hot_embeddings = use_one_hot_embeddings\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.token_type_embedding = nn.Embedding(\n",
    "            vocab_size=token_type_vocab_size,\n",
    "            embedding_size=embedding_size,\n",
    "            use_one_hot=use_one_hot_embeddings)\n",
    "        self.shape_flat = (-1,)\n",
    "        self.one_hot = P.OneHot()\n",
    "        self.on_value = Tensor(1.0, mstype.float32)\n",
    "        self.off_value = Tensor(0.1, mstype.float32)\n",
    "        self.array_mul = P.MatMul()\n",
    "        self.reshape = P.Reshape()\n",
    "        self.shape = tuple(embedding_shape)\n",
    "        self.dropout = nn.Dropout(1 - dropout_prob)\n",
    "        self.gather = P.Gather()\n",
    "        self.use_relative_positions = use_relative_positions\n",
    "        self.slice = P.StridedSlice()\n",
    "        _, seq, _ = self.shape\n",
    "        self.full_position_embedding = nn.Embedding(\n",
    "            vocab_size=max_position_embeddings,\n",
    "            embedding_size=embedding_size,\n",
    "            use_one_hot=False)\n",
    "        self.layernorm = nn.LayerNorm((embedding_size,))\n",
    "        self.position_ids = Tensor(np.arange(seq).reshape(-1, seq).astype(np.int32))\n",
    "        self.add = P.Add()\n",
    "\n",
    "    def construct(self, token_type_ids, word_embeddings):\n",
    "        \"\"\"Postprocessors apply positional and token type embeddings to word embeddings.\"\"\"\n",
    "        output = word_embeddings\n",
    "        if self.use_token_type:\n",
    "            token_type_embeddings = self.token_type_embedding(token_type_ids)\n",
    "            output = self.add(output, token_type_embeddings)\n",
    "        if not self.use_relative_positions:\n",
    "            position_embeddings = self.full_position_embedding(self.position_ids)\n",
    "            output = self.add(output, position_embeddings)\n",
    "        output = self.layernorm(output)\n",
    "        output = self.dropout(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1925014-1c3b-4bdf-b911-afa4506d02dc",
   "metadata": {},
   "source": [
    "下面是一些关于相对位置和格式转换的代码。 我们在这里不用相对位置编码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31da4299-2775-4dde-9a52-b3f6b1aac379",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RelaPosMatrixGenerator(nn.Cell):\n",
    "    \"\"\"\n",
    "    Generates matrix of relative positions between inputs.\n",
    "\n",
    "    Args:\n",
    "        length (int): Length of one dim for the matrix to be generated.\n",
    "        max_relative_position (int): Max value of relative position.\n",
    "    \"\"\"\n",
    "    def __init__(self, length, max_relative_position):\n",
    "        super(RelaPosMatrixGenerator, self).__init__()\n",
    "        self._length = length\n",
    "        self._max_relative_position = Tensor(max_relative_position, dtype=mstype.int32)\n",
    "        self._min_relative_position = Tensor(-max_relative_position, dtype=mstype.int32)\n",
    "        self.range_length = -length + 1\n",
    "        self.tile = P.Tile()\n",
    "        self.range_mat = P.Reshape()\n",
    "        self.sub = P.Sub()\n",
    "        self.expanddims = P.ExpandDims()\n",
    "        self.cast = P.Cast()\n",
    "\n",
    "    def construct(self):\n",
    "        \"\"\"position matrix generator\"\"\"\n",
    "        range_vec_row_out = self.cast(F.tuple_to_array(F.make_range(self._length)), mstype.int32)\n",
    "        range_vec_col_out = self.range_mat(range_vec_row_out, (self._length, -1))\n",
    "        tile_row_out = self.tile(range_vec_row_out, (self._length,))\n",
    "        tile_col_out = self.tile(range_vec_col_out, (1, self._length))\n",
    "        range_mat_out = self.range_mat(tile_row_out, (self._length, self._length))\n",
    "        transpose_out = self.range_mat(tile_col_out, (self._length, self._length))\n",
    "        distance_mat = self.sub(range_mat_out, transpose_out)\n",
    "        distance_mat_clipped = C.clip_by_value(distance_mat,\n",
    "                                               self._min_relative_position,\n",
    "                                               self._max_relative_position)\n",
    "        # Shift values to be >=0. Each integer still uniquely identifies a\n",
    "        # relative position difference.\n",
    "        final_mat = distance_mat_clipped + self._max_relative_position\n",
    "        return final_mat\n",
    "\n",
    "\n",
    "class RelaPosEmbeddingsGenerator(nn.Cell):\n",
    "    \"\"\"\n",
    "    Generates tensor of size [length, length, depth].\n",
    "\n",
    "    Args:\n",
    "        length (int): Length of one dim for the matrix to be generated.\n",
    "        depth (int): Size of each attention head.\n",
    "        max_relative_position (int): Maxmum value of relative position.\n",
    "        initializer_range (float): Initialization value of TruncatedNormal.\n",
    "        use_one_hot_embeddings (bool): Specifies whether to use one hot encoding form. Default: False.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 length,\n",
    "                 depth,\n",
    "                 max_relative_position,\n",
    "                 initializer_range,\n",
    "                 use_one_hot_embeddings=False):\n",
    "        super(RelaPosEmbeddingsGenerator, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.vocab_size = max_relative_position * 2 + 1\n",
    "        self.use_one_hot_embeddings = use_one_hot_embeddings\n",
    "        self.embeddings_table = Parameter(\n",
    "            initializer(TruncatedNormal(initializer_range),\n",
    "                        [self.vocab_size, self.depth]))\n",
    "        self.relative_positions_matrix = RelaPosMatrixGenerator(length=length,\n",
    "                                                                max_relative_position=max_relative_position)\n",
    "        self.reshape = P.Reshape()\n",
    "        self.one_hot = P.OneHot()\n",
    "        self.on_value = Tensor(1.0, mstype.float32)\n",
    "        self.off_value = Tensor(0.0, mstype.float32)\n",
    "        self.shape = P.Shape()\n",
    "        self.gather = P.Gather()  # index_select\n",
    "        self.matmul = P.BatchMatMul()\n",
    "\n",
    "    def construct(self):\n",
    "        \"\"\"position embedding generation\"\"\"\n",
    "        relative_positions_matrix_out = self.relative_positions_matrix()\n",
    "        # Generate embedding for each relative position of dimension depth.\n",
    "        if self.use_one_hot_embeddings:\n",
    "            flat_relative_positions_matrix = self.reshape(relative_positions_matrix_out, (-1,))\n",
    "            one_hot_relative_positions_matrix = self.one_hot(\n",
    "                flat_relative_positions_matrix, self.vocab_size, self.on_value, self.off_value)\n",
    "            embeddings = self.matmul(one_hot_relative_positions_matrix, self.embeddings_table)\n",
    "            my_shape = self.shape(relative_positions_matrix_out) + (self.depth,)\n",
    "            embeddings = self.reshape(embeddings, my_shape)\n",
    "        else:\n",
    "            embeddings = self.gather(self.embeddings_table,\n",
    "                                     relative_positions_matrix_out, 0)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class SaturateCast(nn.Cell):\n",
    "    \"\"\"\n",
    "    Performs a safe saturating cast. This operation applies proper clamping before casting to prevent\n",
    "    the danger that the value will overflow or underflow.\n",
    "\n",
    "    Args:\n",
    "        src_type (:class:`mindspore.dtype`): The type of the elements of the input tensor. Default: mstype.float32.\n",
    "        dst_type (:class:`mindspore.dtype`): The type of the elements of the output tensor. Default: mstype.float32.\n",
    "    \"\"\"\n",
    "    def __init__(self, src_type=mstype.float32, dst_type=mstype.float32):\n",
    "        super(SaturateCast, self).__init__()\n",
    "        np_type = mstype.dtype_to_nptype(dst_type)\n",
    "        min_type = np.finfo(np_type).min\n",
    "        max_type = np.finfo(np_type).max\n",
    "        self.tensor_min_type = Tensor([min_type], dtype=src_type)\n",
    "        self.tensor_max_type = Tensor([max_type], dtype=src_type)\n",
    "        self.min_op = P.Minimum()\n",
    "        self.max_op = P.Maximum()\n",
    "        self.cast = P.Cast()\n",
    "        self.dst_type = dst_type\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"saturate cast\"\"\"\n",
    "        out = self.max_op(x, self.tensor_min_type)\n",
    "        out = self.min_op(out, self.tensor_max_type)\n",
    "        return self.cast(out, self.dst_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba23030c-6262-44a3-b42c-cd5f65c8abfb",
   "metadata": {},
   "source": [
    "## BERT layer\n",
    "bert layer（BertEncoderCell） 主要由  BertSelfAttention 与 BertOutput 即自注意力层和线性映射层组成。\n",
    "\n",
    "而BertSelfAttention 主要包含 BertAttention 与 BertOutput 即自注意力计算和线性映射。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b08ca2ca-a846-41c3-ad48-718e51b312ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertOutput(nn.Cell):\n",
    "    \"\"\"\n",
    "    Apply a linear computation to hidden status and a residual computation to input.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Input channels.\n",
    "        out_channels (int): Output channels.\n",
    "        initializer_range (float): Initialization value of TruncatedNormal. Default: 0.02.\n",
    "        dropout_prob (float): The dropout probability. Default: 0.1.\n",
    "        compute_type (:class:`mindspore.dtype`): Compute type in BertTransformer. Default: mstype.float32.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 initializer_range=0.02,\n",
    "                 dropout_prob=0.1,\n",
    "                 compute_type=mstype.float32):\n",
    "        super(BertOutput, self).__init__()\n",
    "        self.dense = nn.Dense(in_channels, out_channels,\n",
    "                              weight_init=TruncatedNormal(initializer_range)).to_float(compute_type)\n",
    "        self.dropout = nn.Dropout(1 - dropout_prob)\n",
    "        self.add = P.Add()\n",
    "        self.is_gpu = context.get_context('device_target') == \"GPU\"\n",
    "        if self.is_gpu:\n",
    "            self.layernorm = nn.LayerNorm((out_channels,)).to_float(mstype.float32)\n",
    "            self.compute_type = compute_type\n",
    "        else:\n",
    "            self.layernorm = nn.LayerNorm((out_channels,)).to_float(compute_type)\n",
    "\n",
    "        self.cast = P.Cast()\n",
    "\n",
    "    def construct(self, hidden_status, input_tensor):\n",
    "        \"\"\"bert output\"\"\"\n",
    "        output = self.dense(hidden_status)\n",
    "        output = self.dropout(output)\n",
    "        output = self.add(input_tensor, output)\n",
    "        output = self.layernorm(output)\n",
    "        if self.is_gpu:\n",
    "            output = self.cast(output, self.compute_type)\n",
    "        return output\n",
    "\n",
    "    \n",
    "class BertAttention(nn.Cell):\n",
    "    \"\"\"\n",
    "    Apply multi-headed attention from \"from_tensor\" to \"to_tensor\".\n",
    "\n",
    "    Args:\n",
    "        from_tensor_width (int): Size of last dim of from_tensor.\n",
    "        to_tensor_width (int): Size of last dim of to_tensor.\n",
    "        from_seq_length (int): Length of from_tensor sequence.\n",
    "        to_seq_length (int): Length of to_tensor sequence.\n",
    "        num_attention_heads (int): Number of attention heads. Default: 1.\n",
    "        size_per_head (int): Size of each attention head. Default: 512.\n",
    "        query_act (str): Activation function for the query transform. Default: None.\n",
    "        key_act (str): Activation function for the key transform. Default: None.\n",
    "        value_act (str): Activation function for the value transform. Default: None.\n",
    "        has_attention_mask (bool): Specifies whether to use attention mask. Default: False.\n",
    "        attention_probs_dropout_prob (float): The dropout probability for\n",
    "                                      BertAttention. Default: 0.0.\n",
    "        use_one_hot_embeddings (bool): Specifies whether to use one hot encoding form. Default: False.\n",
    "        initializer_range (float): Initialization value of TruncatedNormal. Default: 0.02.\n",
    "        do_return_2d_tensor (bool): True for return 2d tensor. False for return 3d\n",
    "                             tensor. Default: False.\n",
    "        use_relative_positions (bool): Specifies whether to use relative positions. Default: False.\n",
    "        compute_type (:class:`mindspore.dtype`): Compute type in BertAttention. Default: mstype.float32.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 from_tensor_width,\n",
    "                 to_tensor_width,\n",
    "                 from_seq_length,\n",
    "                 to_seq_length,\n",
    "                 num_attention_heads=1,\n",
    "                 size_per_head=512,\n",
    "                 query_act=None,\n",
    "                 key_act=None,\n",
    "                 value_act=None,\n",
    "                 has_attention_mask=False,\n",
    "                 attention_probs_dropout_prob=0.0,\n",
    "                 use_one_hot_embeddings=False,\n",
    "                 initializer_range=0.02,\n",
    "                 do_return_2d_tensor=False,\n",
    "                 use_relative_positions=False,\n",
    "                 compute_type=mstype.float32):\n",
    "        super(BertAttention, self).__init__()\n",
    "        self.from_seq_length = from_seq_length\n",
    "        self.to_seq_length = to_seq_length\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.size_per_head = size_per_head\n",
    "        self.has_attention_mask = has_attention_mask\n",
    "        self.use_relative_positions = use_relative_positions\n",
    "        self.scores_mul = Tensor([1.0 / math.sqrt(float(self.size_per_head))], dtype=compute_type)\n",
    "        self.reshape = P.Reshape()\n",
    "        self.shape_from_2d = (-1, from_tensor_width)\n",
    "        self.shape_to_2d = (-1, to_tensor_width)\n",
    "        weight = TruncatedNormal(initializer_range)\n",
    "        units = num_attention_heads * size_per_head\n",
    "        self.query_layer = nn.Dense(from_tensor_width,\n",
    "                                    units,\n",
    "                                    activation=query_act,\n",
    "                                    weight_init=weight).to_float(compute_type)\n",
    "        self.key_layer = nn.Dense(to_tensor_width,\n",
    "                                  units,\n",
    "                                  activation=key_act,\n",
    "                                  weight_init=weight).to_float(compute_type)\n",
    "        self.value_layer = nn.Dense(to_tensor_width,\n",
    "                                    units,\n",
    "                                    activation=value_act,\n",
    "                                    weight_init=weight).to_float(compute_type)\n",
    "        self.shape_from = (-1, from_seq_length, num_attention_heads, size_per_head)\n",
    "        self.shape_to = (-1, to_seq_length, num_attention_heads, size_per_head)\n",
    "        self.matmul_trans_b = P.BatchMatMul(transpose_b=True)\n",
    "        self.multiply = P.Mul()\n",
    "        self.transpose = P.Transpose()\n",
    "        self.trans_shape = (0, 2, 1, 3)\n",
    "        self.trans_shape_relative = (2, 0, 1, 3)\n",
    "        self.trans_shape_position = (1, 2, 0, 3)\n",
    "        self.multiply_data = Tensor([-10000.0,], dtype=compute_type)\n",
    "        self.matmul = P.BatchMatMul()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.dropout = nn.Dropout(1 - attention_probs_dropout_prob)\n",
    "        if self.has_attention_mask:\n",
    "            self.expand_dims = P.ExpandDims()\n",
    "            self.sub = P.Sub()\n",
    "            self.add = P.Add()\n",
    "            self.cast = P.Cast()\n",
    "            self.get_dtype = P.DType()\n",
    "        if do_return_2d_tensor:\n",
    "            self.shape_return = (-1, num_attention_heads * size_per_head)\n",
    "        else:\n",
    "            self.shape_return = (-1, from_seq_length, num_attention_heads * size_per_head)\n",
    "        self.cast_compute_type = SaturateCast(dst_type=compute_type)\n",
    "        if self.use_relative_positions:\n",
    "            self._generate_relative_positions_embeddings = \\\n",
    "                RelaPosEmbeddingsGenerator(length=to_seq_length,\n",
    "                                           depth=size_per_head,\n",
    "                                           max_relative_position=16,\n",
    "                                           initializer_range=initializer_range,\n",
    "                                           use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "\n",
    "    def construct(self, from_tensor, to_tensor, attention_mask):\n",
    "        \"\"\"bert attention\"\"\"\n",
    "        # reshape 2d/3d input tensors to 2d\n",
    "        from_tensor_2d = self.reshape(from_tensor, self.shape_from_2d)\n",
    "        to_tensor_2d = self.reshape(to_tensor, self.shape_to_2d)\n",
    "        query_out = self.query_layer(from_tensor_2d)\n",
    "        key_out = self.key_layer(to_tensor_2d)\n",
    "        value_out = self.value_layer(to_tensor_2d)\n",
    "        query_layer = self.reshape(query_out, self.shape_from)\n",
    "        query_layer = self.transpose(query_layer, self.trans_shape)\n",
    "        key_layer = self.reshape(key_out, self.shape_to)\n",
    "        key_layer = self.transpose(key_layer, self.trans_shape)\n",
    "        attention_scores = self.matmul_trans_b(query_layer, key_layer)\n",
    "        # use_relative_position, supplementary logic\n",
    "        if self.use_relative_positions:\n",
    "            # relations_keys is [F|T, F|T, H]\n",
    "            relations_keys = self._generate_relative_positions_embeddings()\n",
    "            relations_keys = self.cast_compute_type(relations_keys)\n",
    "            # query_layer_t is [F, B, N, H]\n",
    "            query_layer_t = self.transpose(query_layer, self.trans_shape_relative)\n",
    "            # query_layer_r is [F, B * N, H]\n",
    "            query_layer_r = self.reshape(query_layer_t,\n",
    "                                         (self.from_seq_length,\n",
    "                                          -1,\n",
    "                                          self.size_per_head))\n",
    "            # key_position_scores is [F, B * N, F|T]\n",
    "            key_position_scores = self.matmul_trans_b(query_layer_r,\n",
    "                                                      relations_keys)\n",
    "            # key_position_scores_r is [F, B, N, F|T]\n",
    "            key_position_scores_r = self.reshape(key_position_scores,\n",
    "                                                 (self.from_seq_length,\n",
    "                                                  -1,\n",
    "                                                  self.num_attention_heads,\n",
    "                                                  self.from_seq_length))\n",
    "            # key_position_scores_r_t is [B, N, F, F|T]\n",
    "            key_position_scores_r_t = self.transpose(key_position_scores_r,\n",
    "                                                     self.trans_shape_position)\n",
    "            attention_scores = attention_scores + key_position_scores_r_t\n",
    "        attention_scores = self.multiply(self.scores_mul, attention_scores)\n",
    "        if self.has_attention_mask:\n",
    "            attention_mask = self.expand_dims(attention_mask, 1)\n",
    "            multiply_out = self.sub(self.cast(F.tuple_to_array((1.0,)), self.get_dtype(attention_scores)),\n",
    "                                    self.cast(attention_mask, self.get_dtype(attention_scores)))\n",
    "            adder = self.multiply(multiply_out, self.multiply_data)\n",
    "            attention_scores = self.add(adder, attention_scores)\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "        value_layer = self.reshape(value_out, self.shape_to)\n",
    "        value_layer = self.transpose(value_layer, self.trans_shape)\n",
    "        context_layer = self.matmul(attention_probs, value_layer)\n",
    "        # use_relative_position, supplementary logic\n",
    "        if self.use_relative_positions:\n",
    "            # relations_values is [F|T, F|T, H]\n",
    "            relations_values = self._generate_relative_positions_embeddings()\n",
    "            relations_values = self.cast_compute_type(relations_values)\n",
    "            # attention_probs_t is [F, B, N, T]\n",
    "            attention_probs_t = self.transpose(attention_probs, self.trans_shape_relative)\n",
    "            # attention_probs_r is [F, B * N, T]\n",
    "            attention_probs_r = self.reshape(\n",
    "                attention_probs_t,\n",
    "                (self.from_seq_length,\n",
    "                 -1,\n",
    "                 self.to_seq_length))\n",
    "            # value_position_scores is [F, B * N, H]\n",
    "            value_position_scores = self.matmul(attention_probs_r,\n",
    "                                                relations_values)\n",
    "            # value_position_scores_r is [F, B, N, H]\n",
    "            value_position_scores_r = self.reshape(value_position_scores,\n",
    "                                                   (self.from_seq_length,\n",
    "                                                    -1,\n",
    "                                                    self.num_attention_heads,\n",
    "                                                    self.size_per_head))\n",
    "            # value_position_scores_r_t is [B, N, F, H]\n",
    "            value_position_scores_r_t = self.transpose(value_position_scores_r,\n",
    "                                                       self.trans_shape_position)\n",
    "            context_layer = context_layer + value_position_scores_r_t\n",
    "        context_layer = self.transpose(context_layer, self.trans_shape)\n",
    "        context_layer = self.reshape(context_layer, self.shape_return)\n",
    "        return context_layer, attention_scores\n",
    "\n",
    "class BertSelfAttention(nn.Cell):\n",
    "    \"\"\"\n",
    "    Apply self-attention.\n",
    "\n",
    "    Args:\n",
    "        seq_length (int): Length of input sequence.\n",
    "        hidden_size (int): Size of the bert encoder layers.\n",
    "        num_attention_heads (int): Number of attention heads. Default: 12.\n",
    "        attention_probs_dropout_prob (float): The dropout probability for\n",
    "                                      BertAttention. Default: 0.1.\n",
    "        use_one_hot_embeddings (bool): Specifies whether to use one_hot encoding form. Default: False.\n",
    "        initializer_range (float): Initialization value of TruncatedNormal. Default: 0.02.\n",
    "        hidden_dropout_prob (float): The dropout probability for BertOutput. Default: 0.1.\n",
    "        use_relative_positions (bool): Specifies whether to use relative positions. Default: False.\n",
    "        compute_type (:class:`mindspore.dtype`): Compute type in BertSelfAttention. Default: mstype.float32.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 seq_length,\n",
    "                 hidden_size,\n",
    "                 num_attention_heads=12,\n",
    "                 attention_probs_dropout_prob=0.1,\n",
    "                 use_one_hot_embeddings=False,\n",
    "                 initializer_range=0.02,\n",
    "                 hidden_dropout_prob=0.1,\n",
    "                 use_relative_positions=False,\n",
    "                 compute_type=mstype.float32):\n",
    "        super(BertSelfAttention, self).__init__()\n",
    "        if hidden_size % num_attention_heads != 0:\n",
    "            raise ValueError(\"The hidden size (%d) is not a multiple of the number \"\n",
    "                             \"of attention heads (%d)\" % (hidden_size, num_attention_heads))\n",
    "        self.size_per_head = int(hidden_size / num_attention_heads)\n",
    "        self.attention = BertAttention(\n",
    "            from_tensor_width=hidden_size,\n",
    "            to_tensor_width=hidden_size,\n",
    "            from_seq_length=seq_length,\n",
    "            to_seq_length=seq_length,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "            size_per_head=self.size_per_head,\n",
    "            attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "            use_one_hot_embeddings=use_one_hot_embeddings,\n",
    "            initializer_range=initializer_range,\n",
    "            use_relative_positions=use_relative_positions,\n",
    "            has_attention_mask=True,\n",
    "            do_return_2d_tensor=True,\n",
    "            compute_type=compute_type)\n",
    "        self.output = BertOutput(in_channels=hidden_size,\n",
    "                                 out_channels=hidden_size,\n",
    "                                 initializer_range=initializer_range,\n",
    "                                 dropout_prob=hidden_dropout_prob,\n",
    "                                 compute_type=compute_type)\n",
    "        self.reshape = P.Reshape()\n",
    "        self.shape = (-1, hidden_size)\n",
    "\n",
    "    def construct(self, input_tensor, attention_mask):\n",
    "        \"\"\"bert self attention\"\"\"\n",
    "        input_tensor = self.reshape(input_tensor, self.shape)\n",
    "        attention_output, attention_scores = self.attention(input_tensor, input_tensor, attention_mask)\n",
    "        output = self.output(attention_output, input_tensor)\n",
    "        return output, attention_scores\n",
    "\n",
    "\n",
    "class BertEncoderCell(nn.Cell):\n",
    "    \"\"\"\n",
    "    Encoder cells used in BertTransformer.\n",
    "\n",
    "    Args:\n",
    "        hidden_size (int): Size of the bert encoder layers. Default: 768.\n",
    "        seq_length (int): Length of input sequence. Default: 512.\n",
    "        num_attention_heads (int): Number of attention heads. Default: 12.\n",
    "        intermediate_size (int): Size of intermediate layer. Default: 3072.\n",
    "        attention_probs_dropout_prob (float): The dropout probability for\n",
    "                                      BertAttention. Default: 0.02.\n",
    "        use_one_hot_embeddings (bool): Specifies whether to use one hot encoding form. Default: False.\n",
    "        initializer_range (float): Initialization value of TruncatedNormal. Default: 0.02.\n",
    "        hidden_dropout_prob (float): The dropout probability for BertOutput. Default: 0.1.\n",
    "        use_relative_positions (bool): Specifies whether to use relative positions. Default: False.\n",
    "        hidden_act (str): Activation function. Default: \"gelu\".\n",
    "        compute_type (:class:`mindspore.dtype`): Compute type in attention. Default: mstype.float32.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 hidden_size=768,\n",
    "                 seq_length=512,\n",
    "                 num_attention_heads=12,\n",
    "                 intermediate_size=3072,\n",
    "                 attention_probs_dropout_prob=0.02,\n",
    "                 use_one_hot_embeddings=False,\n",
    "                 initializer_range=0.02,\n",
    "                 hidden_dropout_prob=0.1,\n",
    "                 use_relative_positions=False,\n",
    "                 hidden_act=\"gelu\",\n",
    "                 compute_type=mstype.float32):\n",
    "        super(BertEncoderCell, self).__init__()\n",
    "        self.attention = BertSelfAttention(\n",
    "            hidden_size=hidden_size,\n",
    "            seq_length=seq_length,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "            attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "            use_one_hot_embeddings=use_one_hot_embeddings,\n",
    "            initializer_range=initializer_range,\n",
    "            hidden_dropout_prob=hidden_dropout_prob,\n",
    "            use_relative_positions=use_relative_positions,\n",
    "            compute_type=compute_type)\n",
    "        self.intermediate = nn.Dense(in_channels=hidden_size,\n",
    "                                     out_channels=intermediate_size,\n",
    "                                     activation=hidden_act,\n",
    "                                     weight_init=TruncatedNormal(initializer_range)).to_float(compute_type)\n",
    "        self.output = BertOutput(in_channels=intermediate_size,\n",
    "                                 out_channels=hidden_size,\n",
    "                                 initializer_range=initializer_range,\n",
    "                                 dropout_prob=hidden_dropout_prob,\n",
    "                                 compute_type=compute_type)\n",
    "    def construct(self, hidden_states, attention_mask):\n",
    "        \"\"\"bert encoder cell\"\"\"\n",
    "        # self-attention\n",
    "        attention_output, attention_scores = self.attention(hidden_states, attention_mask)\n",
    "        # feed construct\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        # add and normalize\n",
    "        output = self.output(intermediate_output, attention_output)\n",
    "        return output, attention_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaa7059-7a61-42fc-91e3-449e02f09878",
   "metadata": {},
   "source": [
    "## BERT layers \n",
    "\n",
    "BERT layer的堆叠。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad7f131d-690c-4386-bb3c-6866d0e10698",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTransformer(nn.Cell):\n",
    "    \"\"\"\n",
    "    Multi-layer bert transformer.\n",
    "\n",
    "    Args:\n",
    "        hidden_size (int): Size of the encoder layers.\n",
    "        seq_length (int): Length of input sequence.\n",
    "        num_hidden_layers (int): Number of hidden layers in encoder cells.\n",
    "        num_attention_heads (int): Number of attention heads in encoder cells. Default: 12.\n",
    "        intermediate_size (int): Size of intermediate layer in encoder cells. Default: 3072.\n",
    "        attention_probs_dropout_prob (float): The dropout probability for\n",
    "                                      BertAttention. Default: 0.1.\n",
    "        use_one_hot_embeddings (bool): Specifies whether to use one hot encoding form. Default: False.\n",
    "        initializer_range (float): Initialization value of TruncatedNormal. Default: 0.02.\n",
    "        hidden_dropout_prob (float): The dropout probability for BertOutput. Default: 0.1.\n",
    "        use_relative_positions (bool): Specifies whether to use relative positions. Default: False.\n",
    "        hidden_act (str): Activation function used in the encoder cells. Default: \"gelu\".\n",
    "        compute_type (:class:`mindspore.dtype`): Compute type in BertTransformer. Default: mstype.float32.\n",
    "        return_all_encoders (bool): Specifies whether to return all encoders. Default: False.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 hidden_size,\n",
    "                 seq_length,\n",
    "                 num_hidden_layers,\n",
    "                 num_attention_heads=12,\n",
    "                 intermediate_size=3072,\n",
    "                 attention_probs_dropout_prob=0.1,\n",
    "                 use_one_hot_embeddings=False,\n",
    "                 initializer_range=0.02,\n",
    "                 hidden_dropout_prob=0.1,\n",
    "                 use_relative_positions=False,\n",
    "                 hidden_act=\"gelu\",\n",
    "                 compute_type=mstype.float32,\n",
    "                 return_all_encoders=False):\n",
    "        super(BertTransformer, self).__init__()\n",
    "        self.return_all_encoders = return_all_encoders\n",
    "        layers = []\n",
    "        for _ in range(num_hidden_layers):\n",
    "            layer = BertEncoderCell(hidden_size=hidden_size,\n",
    "                                    seq_length=seq_length,\n",
    "                                    num_attention_heads=num_attention_heads,\n",
    "                                    intermediate_size=intermediate_size,\n",
    "                                    attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "                                    use_one_hot_embeddings=use_one_hot_embeddings,\n",
    "                                    initializer_range=initializer_range,\n",
    "                                    hidden_dropout_prob=hidden_dropout_prob,\n",
    "                                    use_relative_positions=use_relative_positions,\n",
    "                                    hidden_act=hidden_act,\n",
    "                                    compute_type=compute_type)\n",
    "            layers.append(layer)\n",
    "        self.layers = nn.CellList(layers)\n",
    "        self.reshape = P.Reshape()\n",
    "        self.shape = (-1, hidden_size)\n",
    "        self.out_shape = (-1, seq_length, hidden_size)\n",
    "    def construct(self, input_tensor, attention_mask):\n",
    "        \"\"\"bert transformer\"\"\"\n",
    "        prev_output = self.reshape(input_tensor, self.shape)\n",
    "        all_encoder_layers = ()\n",
    "        all_encoder_atts = ()\n",
    "        all_encoder_outputs = ()\n",
    "        all_encoder_outputs += (prev_output,)\n",
    "        for layer_module in self.layers:\n",
    "            layer_output, encoder_att = layer_module(prev_output, attention_mask)\n",
    "            prev_output = layer_output\n",
    "            if self.return_all_encoders:\n",
    "                all_encoder_outputs += (layer_output,)\n",
    "                layer_output = self.reshape(layer_output, self.out_shape)\n",
    "                all_encoder_layers += (layer_output,)\n",
    "                all_encoder_atts += (encoder_att,)\n",
    "        if not self.return_all_encoders:\n",
    "            prev_output = self.reshape(prev_output, self.out_shape)\n",
    "            all_encoder_layers += (prev_output,)\n",
    "        return all_encoder_layers, all_encoder_outputs, all_encoder_atts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46072dc2-9dc1-432c-93f6-8b39da0e2d42",
   "metadata": {},
   "source": [
    "可以选择在模型内或者外 创建attension mask。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00ffa3ce-ed5b-4e2e-a940-732a6dd83219",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateAttentionMaskFromInputMask(nn.Cell):\n",
    "    \"\"\"\n",
    "    Create attention mask according to input mask.\n",
    "\n",
    "    Args:\n",
    "        config (Class): Configuration for BertModel.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(CreateAttentionMaskFromInputMask, self).__init__()\n",
    "        self.input_mask = None\n",
    "        self.cast = P.Cast()\n",
    "        self.reshape = P.Reshape()\n",
    "        self.shape = (-1, 1, config.seq_length)\n",
    "\n",
    "    def construct(self, input_mask):\n",
    "        attention_mask = self.cast(self.reshape(input_mask, self.shape), mstype.float32)\n",
    "        return attention_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5887ea5-73af-4b8f-a629-024f8abcbd3a",
   "metadata": {},
   "source": [
    "## BERT主模型.\n",
    "\n",
    "上面规定了BERT的组件，这些组件将在BERT主模型中拼接起来形成BERT模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef099232-1329-4bb8-92b9-d7743cc7f608",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(nn.Cell):\n",
    "    \"\"\"\n",
    "    Bidirectional Encoder Representations from Transformers.\n",
    "\n",
    "    Args:\n",
    "        config (Class): Configuration for BertModel.\n",
    "        is_training (bool): True for training mode. False for eval mode.\n",
    "        use_one_hot_embeddings (bool): Specifies whether to use one hot encoding form. Default: False.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 is_training,\n",
    "                 use_one_hot_embeddings=False):\n",
    "        super(BertModel, self).__init__()\n",
    "        config = copy.deepcopy(config)\n",
    "        if not is_training:\n",
    "            config.hidden_dropout_prob = 0.0\n",
    "            config.attention_probs_dropout_prob = 0.0\n",
    "        self.seq_length = config.seq_length\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_hidden_layers = config.num_hidden_layers\n",
    "        self.embedding_size = config.hidden_size\n",
    "        self.token_type_ids = None\n",
    "        self.last_idx = self.num_hidden_layers - 1\n",
    "        output_embedding_shape = [-1, self.seq_length,\n",
    "                                  self.embedding_size]\n",
    "        self.bert_embedding_lookup = nn.Embedding(\n",
    "            vocab_size=config.vocab_size,\n",
    "            embedding_size=self.embedding_size,\n",
    "            use_one_hot=use_one_hot_embeddings)\n",
    "        self.bert_embedding_postprocessor = EmbeddingPostprocessor(\n",
    "            use_relative_positions=config.use_relative_positions,\n",
    "            embedding_size=self.embedding_size,\n",
    "            embedding_shape=output_embedding_shape,\n",
    "            use_token_type=True,\n",
    "            token_type_vocab_size=config.type_vocab_size,\n",
    "            use_one_hot_embeddings=use_one_hot_embeddings,\n",
    "            initializer_range=0.02,\n",
    "            max_position_embeddings=config.max_position_embeddings,\n",
    "            dropout_prob=config.hidden_dropout_prob)\n",
    "        self.bert_encoder = BertTransformer(\n",
    "            hidden_size=self.hidden_size,\n",
    "            seq_length=self.seq_length,\n",
    "            num_attention_heads=config.num_attention_heads,\n",
    "            num_hidden_layers=self.num_hidden_layers,\n",
    "            intermediate_size=config.intermediate_size,\n",
    "            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n",
    "            use_one_hot_embeddings=use_one_hot_embeddings,\n",
    "            initializer_range=config.initializer_range,\n",
    "            hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "            use_relative_positions=config.use_relative_positions,\n",
    "            hidden_act=config.hidden_act,\n",
    "            compute_type=config.compute_type,\n",
    "            return_all_encoders=True)\n",
    "        self.cast = P.Cast()\n",
    "        self.dtype = config.dtype\n",
    "        self.cast_compute_type = SaturateCast(dst_type=config.compute_type)\n",
    "        self.slice = P.StridedSlice()\n",
    "        self.squeeze_1 = P.Squeeze(axis=1)\n",
    "        self.dense = nn.Dense(self.hidden_size, self.hidden_size,\n",
    "                              activation=\"tanh\",\n",
    "                              weight_init=TruncatedNormal(config.initializer_range)).to_float(config.compute_type)\n",
    "        self._create_attention_mask_from_input_mask = CreateAttentionMaskFromInputMask(config)\n",
    "\n",
    "    def construct(self, input_ids, token_type_ids, input_mask):\n",
    "        \"\"\"bert model\"\"\"\n",
    "        # embedding\n",
    "        embedding_tables = self.bert_embedding_lookup.embedding_table\n",
    "        word_embeddings = self.bert_embedding_lookup(input_ids)\n",
    "        embedding_output = self.bert_embedding_postprocessor(token_type_ids, word_embeddings)\n",
    "        # attention mask [batch_size, seq_length, seq_length]\n",
    "        attention_mask = self._create_attention_mask_from_input_mask(input_mask)\n",
    "        # bert encoder\n",
    "        encoder_output, encoder_layers, layer_atts = self.bert_encoder(self.cast_compute_type(embedding_output),\n",
    "                                                                       attention_mask)\n",
    "        sequence_output = self.cast(encoder_output[self.last_idx], self.dtype)\n",
    "        # pooler\n",
    "        batch_size = P.Shape()(input_ids)[0]\n",
    "        sequence_slice = self.slice(sequence_output,\n",
    "                                    (0, 0, 0),\n",
    "                                    (batch_size, 1, self.hidden_size),\n",
    "                                    (1, 1, 1))\n",
    "        first_token = self.squeeze_1(sequence_slice)\n",
    "        pooled_output = self.dense(first_token)\n",
    "        pooled_output = self.cast(pooled_output, self.dtype)\n",
    "        encoder_outputs = ()\n",
    "        for output in encoder_layers:\n",
    "            encoder_outputs += (self.cast(output, self.dtype),)\n",
    "        attention_outputs = ()\n",
    "        for output in layer_atts:\n",
    "            attention_outputs += (self.cast(output, self.dtype),)\n",
    "        return sequence_output, pooled_output, embedding_tables, encoder_outputs, attention_outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e30dc9-6894-4e55-95e1-9a65f20800b6",
   "metadata": {},
   "source": [
    "## BERT应用\n",
    "\n",
    "BERT用于分类和NER两个任务的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e53e1ea-ce1e-4769-b8db-c52f1eee6162",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModelCLS(nn.Cell):\n",
    "    \"\"\"\n",
    "    This class is responsible for classification task evaluation,\n",
    "    i.e. XNLI(num_labels=3), LCQMC(num_labels=2), Chnsenti(num_labels=2).\n",
    "    The returned output represents the final logits as the results of log_softmax is proportional to that of softmax.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, is_training, num_labels=2, dropout_prob=0.0,\n",
    "                 use_one_hot_embeddings=False, phase_type=\"student\"):\n",
    "        super(BertModelCLS, self).__init__()\n",
    "        self.bert = BertModel(config, is_training, use_one_hot_embeddings)\n",
    "        self.cast = P.Cast()\n",
    "        self.weight_init = TruncatedNormal(config.initializer_range)\n",
    "        self.log_softmax = P.LogSoftmax(axis=-1)\n",
    "        self.dtype = config.dtype\n",
    "        self.num_labels = num_labels\n",
    "        self.phase_type = phase_type\n",
    "        self.dense_1 = nn.Dense(config.hidden_size, self.num_labels, weight_init=self.weight_init,\n",
    "                                has_bias=True).to_float(config.compute_type)\n",
    "        self.dropout = nn.ReLU()\n",
    "\n",
    "    def construct(self, input_ids, token_type_id, input_mask):\n",
    "        \"\"\"classification bert model\"\"\"\n",
    "        _, pooled_output, _, seq_output, att_output = self.bert(input_ids, token_type_id, input_mask)\n",
    "        cls = self.cast(pooled_output, self.dtype)\n",
    "        cls = self.dropout(cls)\n",
    "        logits = self.dense_1(cls)\n",
    "        logits = self.cast(logits, self.dtype)\n",
    "        log_probs = self.log_softmax(logits)\n",
    "        if self._phase == 'train' or self.phase_type == \"teacher\":\n",
    "            return seq_output, att_output, logits, log_probs\n",
    "        return log_probs\n",
    "\n",
    "class BertModelNER(nn.Cell):\n",
    "    \"\"\"\n",
    "    This class is responsible for sequence labeling task evaluation, i.e. NER(num_labels=11).\n",
    "    The returned output represents the final logits as the results of log_softmax is proportional to that of softmax.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, is_training, num_labels=11, dropout_prob=0.0,\n",
    "                 use_one_hot_embeddings=False, phase_type=\"student\"):\n",
    "        super(BertModelNER, self).__init__()\n",
    "        if not is_training:\n",
    "            config.hidden_dropout_prob = 0.0\n",
    "            config.hidden_probs_dropout_prob = 0.0\n",
    "        self.bert = BertModel(config, is_training, use_one_hot_embeddings)\n",
    "        self.cast = P.Cast()\n",
    "        self.weight_init = TruncatedNormal(config.initializer_range)\n",
    "        self.log_softmax = P.LogSoftmax(axis=-1)\n",
    "        self.dtype = config.dtype\n",
    "        self.num_labels = num_labels\n",
    "        self.dense_1 = nn.Dense(config.hidden_size, self.num_labels, weight_init=self.weight_init,\n",
    "                                has_bias=True).to_float(config.compute_type)\n",
    "        self.dropout = nn.ReLU()\n",
    "        self.reshape = P.Reshape()\n",
    "        self.shape = (-1, config.hidden_size)\n",
    "        self.origin_shape = (-1, config.seq_length, self.num_labels)\n",
    "        self.phase_type = phase_type\n",
    "\n",
    "    def construct(self, input_ids, input_mask, token_type_id):\n",
    "        \"\"\"Return the final logits as the results of log_softmax.\"\"\"\n",
    "        sequence_output, _, _, encoder_outputs, attention_outputs = \\\n",
    "            self.bert(input_ids, token_type_id, input_mask)\n",
    "        seq = self.dropout(sequence_output)\n",
    "        seq = self.reshape(seq, self.shape)\n",
    "        logits = self.dense_1(seq)\n",
    "        logits = self.cast(logits, self.dtype)\n",
    "        return_value = self.log_softmax(logits)\n",
    "        if self._phase == 'train' or self.phase_type == \"teacher\":\n",
    "            return encoder_outputs, attention_outputs, logits, return_value\n",
    "        return return_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c4e0d8-1569-4847-b059-4990bdfe22b6",
   "metadata": {},
   "source": [
    "## tinyBERT 预训练蒸馏模型和下游蒸馏模型。\n",
    "\n",
    "下面是tinyBERT蒸馏时用到的几个模型。 tinyBERT本身可以使用BERT的模型，导入tinyBERT的config即可。\n",
    "\n",
    "注意在ms中，可以将梯度计算和梯度传递写进模型中，下面有几个模型就是进行这一项工作的。\n",
    "\n",
    "先规定一些处理梯度的杂项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80415a4b-5bcb-49dc-92ac-dab9a698db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRADIENT_CLIP_TYPE = 1\n",
    "GRADIENT_CLIP_VALUE = 1.0\n",
    "\n",
    "clip_grad = C.MultitypeFuncGraph(\"clip_grad\")\n",
    "@clip_grad.register(\"Number\", \"Number\", \"Tensor\")\n",
    "def _clip_grad(clip_type, clip_value, grad):\n",
    "    \"\"\"\n",
    "    Clip gradients.\n",
    "\n",
    "    Inputs:\n",
    "        clip_type (int): The way to clip, 0 for 'value', 1 for 'norm'.\n",
    "        clip_value (float): Specifies how much to clip.\n",
    "        grad (tuple[Tensor]): Gradients.\n",
    "\n",
    "    Outputs:\n",
    "        tuple[Tensor], clipped gradients.\n",
    "    \"\"\"\n",
    "    if clip_type not in (0, 1):\n",
    "        return grad\n",
    "    dt = F.dtype(grad)\n",
    "    if clip_type == 0:\n",
    "        new_grad = C.clip_by_value(grad, F.cast(F.tuple_to_array((-clip_value,)), dt),\n",
    "                                   F.cast(F.tuple_to_array((clip_value,)), dt))\n",
    "    else:\n",
    "        new_grad = nn.ClipByNorm()(grad, F.cast(F.tuple_to_array((clip_value,)), dt))\n",
    "    return new_grad\n",
    "\n",
    "grad_scale = C.MultitypeFuncGraph(\"grad_scale\")\n",
    "reciprocal = P.Reciprocal()\n",
    "\n",
    "@grad_scale.register(\"Tensor\", \"Tensor\")\n",
    "def tensor_grad_scale(scale, grad):\n",
    "    return grad * reciprocal(scale)\n",
    "\n",
    "class ClipGradients(nn.Cell):\n",
    "    \"\"\"\n",
    "    Clip gradients.\n",
    "\n",
    "    Args:\n",
    "        grads (list): List of gradient tuples.\n",
    "        clip_type (Tensor): The way to clip, 'value' or 'norm'.\n",
    "        clip_value (Tensor): Specifies how much to clip.\n",
    "\n",
    "    Returns:\n",
    "        List, a list of clipped_grad tuples.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ClipGradients, self).__init__()\n",
    "        self.clip_by_norm = nn.ClipByNorm()\n",
    "        self.cast = P.Cast()\n",
    "        self.dtype = P.DType()\n",
    "\n",
    "    def construct(self,\n",
    "                  grads,\n",
    "                  clip_type,\n",
    "                  clip_value):\n",
    "        \"\"\"clip gradients\"\"\"\n",
    "        if clip_type not in (0, 1):\n",
    "            return grads\n",
    "        new_grads = ()\n",
    "        for grad in grads:\n",
    "            dt = self.dtype(grad)\n",
    "            if clip_type == 0:\n",
    "                t = C.clip_by_value(grad, self.cast(F.tuple_to_array((-clip_value,)), dt),\n",
    "                                    self.cast(F.tuple_to_array((clip_value,)), dt))\n",
    "            else:\n",
    "                t = self.clip_by_norm(grad, self.cast(F.tuple_to_array((clip_value,)), dt))\n",
    "            new_grads = new_grads + (t,)\n",
    "        return new_grads\n",
    "\n",
    "class SoftCrossEntropy(nn.Cell):\n",
    "    \"\"\"SoftCrossEntropy loss\"\"\"\n",
    "    def __init__(self):\n",
    "        super(SoftCrossEntropy, self).__init__()\n",
    "        self.log_softmax = P.LogSoftmax(axis=-1)\n",
    "        self.softmax = P.Softmax(axis=-1)\n",
    "        self.reduce_mean = P.ReduceMean()\n",
    "        self.cast = P.Cast()\n",
    "\n",
    "    def construct(self, predicts, targets):\n",
    "        likelihood = self.log_softmax(predicts)\n",
    "        target_prob = self.softmax(targets)\n",
    "        loss = self.reduce_mean(-target_prob * likelihood)\n",
    "\n",
    "        return self.cast(loss, mstype.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bc7edf-e55c-4617-acb6-e8ffe75353ad",
   "metadata": {},
   "source": [
    "##  预训练蒸馏模型\n",
    "下面是预训练蒸馏时所用到的计算loss的模型。 注意层级之间的对应关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a261b677-3d45-4958-aca4-3d4d0b10dcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertNetworkWithLoss_gd(nn.Cell):\n",
    "    \"\"\"\n",
    "    Provide bert pre-training loss through network.\n",
    "    Args:\n",
    "        config (BertConfig): The config of BertModel.\n",
    "        is_training (bool): Specifies whether to use the training mode.\n",
    "        use_one_hot_embeddings (bool): Specifies whether to use one-hot for embeddings. Default: False.\n",
    "    Returns:\n",
    "        Tensor, the loss of the network.\n",
    "    \"\"\"\n",
    "    def __init__(self, teacher_config, teacher_ckpt, student_config, is_training, use_one_hot_embeddings=False,\n",
    "                 is_att_fit=True, is_rep_fit=True):\n",
    "        super(BertNetworkWithLoss_gd, self).__init__()\n",
    "        # load teacher model\n",
    "        self.teacher = BertModel(teacher_config, False, use_one_hot_embeddings)\n",
    "        param_dict = load_checkpoint(teacher_ckpt)\n",
    "        new_param_dict = {}\n",
    "        for key, value in param_dict.items():\n",
    "            new_key = re.sub('^bert.bert.', 'teacher.', key)\n",
    "            new_param_dict[new_key] = value\n",
    "        load_param_into_net(self.teacher, new_param_dict)\n",
    "        # no_grad\n",
    "        self.teacher.set_train(False)\n",
    "        params = self.teacher.trainable_params()\n",
    "        for param in params:\n",
    "            param.requires_grad = False\n",
    "        # student model\n",
    "        self.bert = BertModel(student_config, is_training, use_one_hot_embeddings)\n",
    "        self.cast = P.Cast()\n",
    "        self.fit_dense = nn.Dense(student_config.hidden_size,\n",
    "                                  teacher_config.hidden_size).to_float(teacher_config.compute_type)\n",
    "        self.teacher_layers_num = teacher_config.num_hidden_layers\n",
    "        self.student_layers_num = student_config.num_hidden_layers\n",
    "        self.layers_per_block = int(self.teacher_layers_num / self.student_layers_num)\n",
    "        self.is_att_fit = is_att_fit\n",
    "        self.is_rep_fit = is_rep_fit\n",
    "        self.loss_mse = nn.MSELoss()\n",
    "        self.select = P.Select()\n",
    "        self.zeroslike = P.ZerosLike()\n",
    "        self.dtype = teacher_config.dtype\n",
    "\n",
    "    def construct(self,\n",
    "                  input_ids,\n",
    "                  input_mask,\n",
    "                  token_type_id):\n",
    "        \"\"\"general distill network with loss\"\"\"\n",
    "        # teacher model\n",
    "        _, _, _, teacher_seq_output, teacher_att_output = self.teacher(input_ids, token_type_id, input_mask)\n",
    "        # student model\n",
    "        _, _, _, student_seq_output, student_att_output = self.bert(input_ids, token_type_id, input_mask)\n",
    "        total_loss = 0\n",
    "        if self.is_att_fit:\n",
    "            selected_teacher_att_output = ()\n",
    "            selected_student_att_output = ()\n",
    "            for i in range(self.student_layers_num):\n",
    "                selected_teacher_att_output += (teacher_att_output[(i + 1) * self.layers_per_block - 1],)\n",
    "                selected_student_att_output += (student_att_output[i],)\n",
    "            att_loss = 0\n",
    "            for i in range(self.student_layers_num):\n",
    "                student_att = selected_student_att_output[i]\n",
    "                teacher_att = selected_teacher_att_output[i]\n",
    "                student_att = self.select(student_att <= self.cast(-100.0, mstype.float32), self.zeroslike(student_att),\n",
    "                                          student_att)\n",
    "                teacher_att = self.select(teacher_att <= self.cast(-100.0, mstype.float32), self.zeroslike(teacher_att),\n",
    "                                          teacher_att)\n",
    "                att_loss += self.loss_mse(student_att, teacher_att)\n",
    "            total_loss += att_loss\n",
    "        if self.is_rep_fit:\n",
    "            selected_teacher_seq_output = ()\n",
    "            selected_student_seq_output = ()\n",
    "            for i in range(self.student_layers_num + 1):\n",
    "                selected_teacher_seq_output += (teacher_seq_output[i * self.layers_per_block],)\n",
    "                fit_dense_out = self.fit_dense(student_seq_output[i])\n",
    "                fit_dense_out = self.cast(fit_dense_out, self.dtype)\n",
    "                selected_student_seq_output += (fit_dense_out,)\n",
    "            rep_loss = 0\n",
    "            for i in range(self.student_layers_num + 1):\n",
    "                teacher_rep = selected_teacher_seq_output[i]\n",
    "                student_rep = selected_student_seq_output[i]\n",
    "                rep_loss += self.loss_mse(student_rep, teacher_rep)\n",
    "            total_loss += rep_loss\n",
    "        return self.cast(total_loss, mstype.float32)\n",
    "\n",
    "class BertTrainWithLossScaleCell(nn.Cell):\n",
    "    \"\"\"\n",
    "    Encapsulation class of bert network training.\n",
    "\n",
    "    Append an optimizer to the training network after that the construct\n",
    "    function can be called to create the backward graph.\n",
    "\n",
    "    Args:\n",
    "        network (Cell): The training network. Note that loss function should have been added.\n",
    "        optimizer (Optimizer): Optimizer for updating the weights.\n",
    "        scale_update_cell (Cell): Cell to do the loss scale. Default: None.\n",
    "    \"\"\"\n",
    "    def __init__(self, network, optimizer, scale_update_cell=None):\n",
    "        super(BertTrainWithLossScaleCell, self).__init__(auto_prefix=False)\n",
    "        self.network = network\n",
    "        self.network.set_grad()\n",
    "        self.weights = optimizer.parameters\n",
    "        self.optimizer = optimizer\n",
    "        self.grad = C.GradOperation(get_by_list=True,\n",
    "                                    sens_param=True)\n",
    "        self.reducer_flag = False\n",
    "        self.allreduce = P.AllReduce()\n",
    "        self.parallel_mode = context.get_auto_parallel_context(\"parallel_mode\")\n",
    "        if self.parallel_mode in [ParallelMode.DATA_PARALLEL, ParallelMode.HYBRID_PARALLEL]:\n",
    "            self.reducer_flag = True\n",
    "        self.grad_reducer = F.identity\n",
    "        self.degree = 1\n",
    "        if self.reducer_flag:\n",
    "            self.degree = get_group_size()\n",
    "            self.grad_reducer = DistributedGradReducer(optimizer.parameters, False, self.degree)\n",
    "        self.is_distributed = (self.parallel_mode != ParallelMode.STAND_ALONE)\n",
    "        self.cast = P.Cast()\n",
    "        self.alloc_status = P.NPUAllocFloatStatus()\n",
    "        self.get_status = P.NPUGetFloatStatus()\n",
    "        self.clear_status = P.NPUClearFloatStatus()\n",
    "        self.reduce_sum = P.ReduceSum(keep_dims=False)\n",
    "        self.base = Tensor(1, mstype.float32)\n",
    "        self.less_equal = P.LessEqual()\n",
    "        self.hyper_map = C.HyperMap()\n",
    "        self.loss_scale = None\n",
    "        self.loss_scaling_manager = scale_update_cell\n",
    "        if scale_update_cell:\n",
    "            self.loss_scale = Parameter(Tensor(scale_update_cell.get_loss_scale(), dtype=mstype.float32))\n",
    "\n",
    "    def construct(self,\n",
    "                  input_ids,\n",
    "                  input_mask,\n",
    "                  token_type_id,\n",
    "                  sens=None):\n",
    "        \"\"\"Defines the computation performed.\"\"\"\n",
    "        weights = self.weights\n",
    "        loss = self.network(input_ids,\n",
    "                            input_mask,\n",
    "                            token_type_id)\n",
    "        if sens is None:\n",
    "            scaling_sens = self.loss_scale\n",
    "        else:\n",
    "            scaling_sens = sens\n",
    "        # alloc status and clear should be right before gradoperation\n",
    "        init = self.alloc_status()\n",
    "        init = F.depend(init, loss)\n",
    "        clear_status = self.clear_status(init)\n",
    "        scaling_sens = F.depend(scaling_sens, clear_status)\n",
    "        grads = self.grad(self.network, weights)(input_ids,\n",
    "                                                 input_mask,\n",
    "                                                 token_type_id,\n",
    "                                                 self.cast(scaling_sens,\n",
    "                                                           mstype.float32))\n",
    "        # apply grad reducer on grads\n",
    "        grads = self.grad_reducer(grads)\n",
    "        grads = self.hyper_map(F.partial(grad_scale, scaling_sens * self.degree), grads)\n",
    "        grads = self.hyper_map(F.partial(clip_grad, GRADIENT_CLIP_TYPE, GRADIENT_CLIP_VALUE), grads)\n",
    "        init = F.depend(init, grads)\n",
    "        get_status = self.get_status(init)\n",
    "        init = F.depend(init, get_status)\n",
    "        flag_sum = self.reduce_sum(init, (0,))\n",
    "        if self.is_distributed:\n",
    "            # sum overflow flag over devices\n",
    "            flag_reduce = self.allreduce(flag_sum)\n",
    "            cond = self.less_equal(self.base, flag_reduce)\n",
    "        else:\n",
    "            cond = self.less_equal(self.base, flag_sum)\n",
    "        overflow = cond\n",
    "        if sens is None:\n",
    "            overflow = self.loss_scaling_manager(self.loss_scale, cond)\n",
    "        if not overflow:\n",
    "            self.optimizer(grads)\n",
    "        return (loss, cond, scaling_sens)\n",
    "\n",
    "class BertTrainCell(nn.Cell):\n",
    "    \"\"\"\n",
    "    Encapsulation class of bert network training.\n",
    "\n",
    "    Append an optimizer to the training network after that the construct\n",
    "    function can be called to create the backward graph.\n",
    "\n",
    "    Args:\n",
    "        network (Cell): The training network. Note that loss function should have been added.\n",
    "        optimizer (Optimizer): Optimizer for updating the weights.\n",
    "        sens (Number): The adjust parameter. Default: 1.0.\n",
    "    \"\"\"\n",
    "    def __init__(self, network, optimizer, sens=1.0):\n",
    "        super(BertTrainCell, self).__init__(auto_prefix=False)\n",
    "        self.network = network\n",
    "        self.network.set_grad()\n",
    "        self.weights = optimizer.parameters\n",
    "        self.optimizer = optimizer\n",
    "        self.sens = sens\n",
    "        self.grad = C.GradOperation(get_by_list=True,\n",
    "                                    sens_param=True)\n",
    "        self.reducer_flag = False\n",
    "        self.parallel_mode = context.get_auto_parallel_context(\"parallel_mode\")\n",
    "        if self.parallel_mode in [ParallelMode.DATA_PARALLEL, ParallelMode.HYBRID_PARALLEL]:\n",
    "            self.reducer_flag = True\n",
    "        self.grad_reducer = F.identity\n",
    "        self.degree = 1\n",
    "        if self.reducer_flag:\n",
    "            mean = context.get_auto_parallel_context(\"gradients_mean\")\n",
    "            self.degree = get_group_size()\n",
    "            self.grad_reducer = DistributedGradReducer(optimizer.parameters, mean, self.degree)\n",
    "        self.cast = P.Cast()\n",
    "        self.hyper_map = C.HyperMap()\n",
    "\n",
    "    def construct(self,\n",
    "                  input_ids,\n",
    "                  input_mask,\n",
    "                  token_type_id):\n",
    "        \"\"\"Defines the computation performed.\"\"\"\n",
    "        weights = self.weights\n",
    "        loss = self.network(input_ids,\n",
    "                            input_mask,\n",
    "                            token_type_id)\n",
    "        grads = self.grad(self.network, weights)(input_ids,\n",
    "                                                 input_mask,\n",
    "                                                 token_type_id,\n",
    "                                                 self.cast(F.tuple_to_array((self.sens,)),\n",
    "                                                           mstype.float32))\n",
    "        # apply grad reducer on grads\n",
    "        grads = self.grad_reducer(grads)\n",
    "        grads = self.hyper_map(F.partial(clip_grad, GRADIENT_CLIP_TYPE, GRADIENT_CLIP_VALUE), grads)\n",
    "        self.optimizer(grads)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d5b50e-58c1-4653-bcd4-1db6a9bffe15",
   "metadata": {},
   "source": [
    "## 下游蒸馏模型\n",
    "\n",
    "下面是下游蒸馏时所用到的计算loss的模型。\n",
    "\n",
    "BertEvaluationCell中规定了梯度的回传。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99f042a5-f4aa-45cd-a19b-d36e51ed7152",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertNetworkWithLoss_td(nn.Cell):\n",
    "    \"\"\"\n",
    "    Provide bert pre-training loss through network.\n",
    "    Args:\n",
    "        config (BertConfig): The config of BertModel.\n",
    "        is_training (bool): Specifies whether to use the training mode.\n",
    "        use_one_hot_embeddings (bool): Specifies whether to use one-hot for embeddings. Default: False.\n",
    "    Returns:\n",
    "        Tensor, the loss of the network.\n",
    "    \"\"\"\n",
    "    def __init__(self, teacher_config, teacher_ckpt, student_config, student_ckpt,\n",
    "                 is_training, task_type, num_labels, use_one_hot_embeddings=False,\n",
    "                 is_predistill=True, is_att_fit=True, is_rep_fit=True,\n",
    "                 temperature=1.0, dropout_prob=0.1):\n",
    "        super(BertNetworkWithLoss_td, self).__init__()\n",
    "        # load teacher model\n",
    "        if task_type == \"classification\":\n",
    "            self.teacher = BertModelCLS(teacher_config, False, num_labels, dropout_prob,\n",
    "                                        use_one_hot_embeddings, \"teacher\")\n",
    "            self.bert = BertModelCLS(student_config, is_training, num_labels, dropout_prob,\n",
    "                                     use_one_hot_embeddings, \"student\")\n",
    "        elif task_type == \"ner\":\n",
    "            self.teacher = BertModelNER(teacher_config, False, num_labels, dropout_prob,\n",
    "                                        use_one_hot_embeddings, \"teacher\")\n",
    "            self.bert = BertModelNER(student_config, is_training, num_labels, dropout_prob,\n",
    "                                     use_one_hot_embeddings, \"student\")\n",
    "        else:\n",
    "            raise ValueError(f\"Not support task type: {task_type}\")\n",
    "        # param_dict = load_checkpoint(teacher_ckpt)\n",
    "        # new_param_dict = {}\n",
    "        # for key, value in param_dict.items():\n",
    "        #     new_key = re.sub('^bert.', 'teacher.', key)\n",
    "        #     new_param_dict[new_key] = value\n",
    "        # load_param_into_net(self.teacher, new_param_dict)\n",
    "\n",
    "        # no_grad\n",
    "        self.teacher.set_train(False)\n",
    "        # params = self.teacher.trainable_params()\n",
    "        # for param in params:\n",
    "        #     param.requires_grad = False\n",
    "        # # load student model\n",
    "        # param_dict = load_checkpoint(student_ckpt)\n",
    "        # if is_predistill:\n",
    "        #     new_param_dict = {}\n",
    "        #     for key, value in param_dict.items():\n",
    "        #         new_key = re.sub('tinybert_', 'bert_', 'bert.' + key)\n",
    "        #         new_param_dict[new_key] = value\n",
    "        #     load_param_into_net(self.bert, new_param_dict)\n",
    "        # else:\n",
    "        #     new_param_dict = {}\n",
    "        #     for key, value in param_dict.items():\n",
    "        #         new_key = re.sub('tinybert_', 'bert_', key)\n",
    "        #         new_param_dict[new_key] = value\n",
    "        #     load_param_into_net(self.bert, new_param_dict)\n",
    "        self.cast = P.Cast()\n",
    "        self.fit_dense = nn.Dense(student_config.hidden_size,\n",
    "                                  teacher_config.hidden_size).to_float(teacher_config.compute_type)\n",
    "        self.teacher_layers_num = teacher_config.num_hidden_layers\n",
    "        self.student_layers_num = student_config.num_hidden_layers\n",
    "        self.layers_per_block = int(self.teacher_layers_num / self.student_layers_num)\n",
    "        self.is_predistill = is_predistill\n",
    "        self.is_att_fit = is_att_fit\n",
    "        self.is_rep_fit = is_rep_fit\n",
    "        self.use_soft_cross_entropy = task_type in [\"classification\", \"ner\"]\n",
    "        self.temperature = temperature\n",
    "        self.loss_mse = nn.MSELoss()\n",
    "        self.select = P.Select()\n",
    "        self.zeroslike = P.ZerosLike()\n",
    "        self.dtype = student_config.dtype\n",
    "        self.num_labels = num_labels\n",
    "        self.dtype = teacher_config.dtype\n",
    "        self.soft_cross_entropy = SoftCrossEntropy()\n",
    "\n",
    "    def construct(self,\n",
    "                  input_ids,\n",
    "                  input_mask,\n",
    "                  token_type_id,\n",
    "                  label_ids):\n",
    "        \"\"\"task distill network with loss\"\"\"\n",
    "        # teacher model\n",
    "        teacher_seq_output, teacher_att_output, teacher_logits, _ = self.teacher(input_ids, token_type_id, input_mask)\n",
    "        # student model\n",
    "        student_seq_output, student_att_output, student_logits, _ = self.bert(input_ids, token_type_id, input_mask)\n",
    "        total_loss = 0\n",
    "        if self.is_predistill:\n",
    "            if self.is_att_fit:\n",
    "                selected_teacher_att_output = ()\n",
    "                selected_student_att_output = ()\n",
    "                for i in range(self.student_layers_num):\n",
    "                    selected_teacher_att_output += (teacher_att_output[(i + 1) * self.layers_per_block - 1],)\n",
    "                    selected_student_att_output += (student_att_output[i],)\n",
    "                att_loss = 0\n",
    "                for i in range(self.student_layers_num):\n",
    "                    student_att = selected_student_att_output[i]\n",
    "                    teacher_att = selected_teacher_att_output[i]\n",
    "                    student_att = self.select(student_att <= self.cast(-100.0, mstype.float32),\n",
    "                                              self.zeroslike(student_att),\n",
    "                                              student_att)\n",
    "                    teacher_att = self.select(teacher_att <= self.cast(-100.0, mstype.float32),\n",
    "                                              self.zeroslike(teacher_att),\n",
    "                                              teacher_att)\n",
    "                    att_loss += self.loss_mse(student_att, teacher_att)\n",
    "                total_loss += att_loss\n",
    "            if self.is_rep_fit:\n",
    "                selected_teacher_seq_output = ()\n",
    "                selected_student_seq_output = ()\n",
    "                for i in range(self.student_layers_num + 1):\n",
    "                    selected_teacher_seq_output += (teacher_seq_output[i * self.layers_per_block],)\n",
    "                    fit_dense_out = self.fit_dense(student_seq_output[i])\n",
    "                    fit_dense_out = self.cast(fit_dense_out, self.dtype)\n",
    "                    selected_student_seq_output += (fit_dense_out,)\n",
    "                rep_loss = 0\n",
    "                for i in range(self.student_layers_num + 1):\n",
    "                    teacher_rep = selected_teacher_seq_output[i]\n",
    "                    student_rep = selected_student_seq_output[i]\n",
    "                    rep_loss += self.loss_mse(student_rep, teacher_rep)\n",
    "                total_loss += rep_loss\n",
    "        else:\n",
    "            if self.use_soft_cross_entropy:\n",
    "                cls_loss = self.soft_cross_entropy(student_logits / self.temperature, teacher_logits / self.temperature)\n",
    "            else:\n",
    "                cls_loss = self.loss_mse(student_logits[len(student_logits) - 1], label_ids[len(label_ids) - 1])\n",
    "            total_loss += cls_loss\n",
    "        return self.cast(total_loss, mstype.float32)\n",
    "\n",
    "class BertEvaluationCell(nn.Cell):\n",
    "    \"\"\"\n",
    "    Especially defined for finetuning where only four inputs tensor are needed.\n",
    "    \"\"\"\n",
    "    def __init__(self, network, optimizer, sens=1.0):\n",
    "        super(BertEvaluationCell, self).__init__(auto_prefix=False)\n",
    "        self.network = network\n",
    "        self.network.set_grad()\n",
    "        self.weights = optimizer.parameters\n",
    "        self.optimizer = optimizer\n",
    "        self.sens = sens\n",
    "        self.grad = C.GradOperation(get_by_list=True,\n",
    "                                    sens_param=True)\n",
    "        self.reducer_flag = False\n",
    "        self.parallel_mode = context.get_auto_parallel_context(\"parallel_mode\")\n",
    "        if self.parallel_mode in [ParallelMode.DATA_PARALLEL, ParallelMode.HYBRID_PARALLEL]:\n",
    "            self.reducer_flag = True\n",
    "        self.grad_reducer = F.identity\n",
    "        self.degree = 1\n",
    "        if self.reducer_flag:\n",
    "            mean = context.get_auto_parallel_context(\"gradients_mean\")\n",
    "            self.degree = get_group_size()\n",
    "            self.grad_reducer = DistributedGradReducer(optimizer.parameters, mean, self.degree)\n",
    "        self.is_distributed = (self.parallel_mode != ParallelMode.STAND_ALONE)\n",
    "        self.cast = P.Cast()\n",
    "        self.hyper_map = C.HyperMap()\n",
    "\n",
    "    def construct(self,\n",
    "                  input_ids,\n",
    "                  input_mask,\n",
    "                  token_type_id,\n",
    "                  label_ids):\n",
    "        \"\"\"Defines the computation performed.\"\"\"\n",
    "        weights = self.weights\n",
    "        loss = self.network(input_ids,\n",
    "                            input_mask,\n",
    "                            token_type_id,\n",
    "                            label_ids)\n",
    "        grads = self.grad(self.network, weights)(input_ids,\n",
    "                                                 input_mask,\n",
    "                                                 token_type_id,\n",
    "                                                 label_ids,\n",
    "                                                 self.cast(F.tuple_to_array((self.sens,)),\n",
    "                                                           mstype.float32))\n",
    "        # apply grad reducer on grads\n",
    "        grads = self.grad_reducer(grads)\n",
    "        grads = self.hyper_map(F.partial(clip_grad, GRADIENT_CLIP_TYPE, GRADIENT_CLIP_VALUE), grads)\n",
    "        self.optimizer(grads)\n",
    "        return loss\n",
    "\n",
    "class BertEvaluationWithLossScaleCell(nn.Cell):\n",
    "    \"\"\"\n",
    "    Especially defined for finetuning where only four inputs tensor are needed.\n",
    "    \"\"\"\n",
    "    def __init__(self, network, optimizer, scale_update_cell=None):\n",
    "        super(BertEvaluationWithLossScaleCell, self).__init__(auto_prefix=False)\n",
    "        self.network = network\n",
    "        self.network.set_grad()\n",
    "        self.weights = optimizer.parameters\n",
    "        self.optimizer = optimizer\n",
    "        self.grad = C.GradOperation(get_by_list=True,\n",
    "                                    sens_param=True)\n",
    "        self.reducer_flag = False\n",
    "        self.allreduce = P.AllReduce()\n",
    "        self.parallel_mode = context.get_auto_parallel_context(\"parallel_mode\")\n",
    "        if self.parallel_mode in [ParallelMode.DATA_PARALLEL, ParallelMode.HYBRID_PARALLEL]:\n",
    "            self.reducer_flag = True\n",
    "        self.grad_reducer = F.identity\n",
    "        self.degree = 1\n",
    "        if self.reducer_flag:\n",
    "            self.degree = get_group_size()\n",
    "            self.grad_reducer = DistributedGradReducer(optimizer.parameters, False, self.degree)\n",
    "        self.is_distributed = (self.parallel_mode != ParallelMode.STAND_ALONE)\n",
    "        self.cast = P.Cast()\n",
    "        self.alloc_status = P.NPUAllocFloatStatus()\n",
    "        self.get_status = P.NPUGetFloatStatus()\n",
    "        self.clear_status = P.NPUClearFloatStatus()\n",
    "        self.reduce_sum = P.ReduceSum(keep_dims=False)\n",
    "        self.base = Tensor(1, mstype.float32)\n",
    "        self.less_equal = P.LessEqual()\n",
    "        self.hyper_map = C.HyperMap()\n",
    "        self.loss_scale = None\n",
    "        self.loss_scaling_manager = scale_update_cell\n",
    "        if scale_update_cell:\n",
    "            self.loss_scale = Parameter(Tensor(scale_update_cell.get_loss_scale(), dtype=mstype.float32))\n",
    "\n",
    "    def construct(self,\n",
    "                  input_ids,\n",
    "                  input_mask,\n",
    "                  token_type_id,\n",
    "                  label_ids,\n",
    "                  sens=None):\n",
    "        \"\"\"Defines the computation performed.\"\"\"\n",
    "        weights = self.weights\n",
    "        loss = self.network(input_ids,\n",
    "                            input_mask,\n",
    "                            token_type_id,\n",
    "                            label_ids)\n",
    "        if sens is None:\n",
    "            scaling_sens = self.loss_scale\n",
    "        else:\n",
    "            scaling_sens = sens\n",
    "        # alloc status and clear should be right before gradoperation\n",
    "        init = self.alloc_status()\n",
    "        init = F.depend(init, loss)\n",
    "        clear_status = self.clear_status(init)\n",
    "        scaling_sens = F.depend(scaling_sens, clear_status)\n",
    "        grads = self.grad(self.network, weights)(input_ids,\n",
    "                                                 input_mask,\n",
    "                                                 token_type_id,\n",
    "                                                 label_ids,\n",
    "                                                 self.cast(scaling_sens,\n",
    "                                                           mstype.float32))\n",
    "        # apply grad reducer on grads\n",
    "        grads = self.grad_reducer(grads)\n",
    "        grads = self.hyper_map(F.partial(grad_scale, scaling_sens * self.degree), grads)\n",
    "        grads = self.hyper_map(F.partial(clip_grad, GRADIENT_CLIP_TYPE, GRADIENT_CLIP_VALUE), grads)\n",
    "        init = F.depend(init, grads)\n",
    "        get_status = self.get_status(init)\n",
    "        init = F.depend(init, get_status)\n",
    "        flag_sum = self.reduce_sum(init, (0,))\n",
    "        if self.is_distributed:\n",
    "            # sum overflow flag over devices\n",
    "            flag_reduce = self.allreduce(flag_sum)\n",
    "            cond = self.less_equal(self.base, flag_reduce)\n",
    "        else:\n",
    "            cond = self.less_equal(self.base, flag_sum)\n",
    "        overflow = cond\n",
    "        if sens is None:\n",
    "            overflow = self.loss_scaling_manager(self.loss_scale, cond)\n",
    "        if not overflow:\n",
    "            self.optimizer(grads)\n",
    "        return (loss, cond, scaling_sens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213178a6-e3ad-442f-b821-8775ec132c8c",
   "metadata": {},
   "source": [
    "# 数据集\n",
    "\n",
    "  创建tinybert训练所需的dataset。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32c46a24-d1d7-46cd-9a77-d5766e3805bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################data_############################\n",
    "class DataType(Enum):\n",
    "    \"\"\"Enumerate supported dataset format\"\"\"\n",
    "    TFRECORD = 1\n",
    "    MINDRECORD = 2\n",
    "\n",
    "def create_tinybert_dataset(task='td', batch_size=32, device_num=1, rank=0,\n",
    "                            do_shuffle=\"true\", data_dir=None, schema_dir=None,\n",
    "                            data_type=DataType.TFRECORD):\n",
    "    \"\"\"create tinybert dataset\"\"\"\n",
    "    files = os.listdir(data_dir)\n",
    "    data_files = []\n",
    "    for file_name in files:\n",
    "        if \"record\" in file_name and \"db\" not in file_name:\n",
    "            data_files.append(os.path.join(data_dir, file_name))\n",
    "    if task == \"td\":\n",
    "        columns_list = [\"input_ids\", \"input_mask\", \"segment_ids\", \"label_ids\"]\n",
    "    else:\n",
    "        columns_list = [\"input_ids\", \"input_mask\", \"segment_ids\"]\n",
    "\n",
    "    shard_equal_rows = True\n",
    "    shuffle = (do_shuffle == \"true\")\n",
    "    if device_num == 1:\n",
    "        shard_equal_rows = False\n",
    "        shuffle = False\n",
    "\n",
    "    if data_type == DataType.MINDRECORD:\n",
    "        data_set = ds.MindDataset(data_files, columns_list=columns_list,\n",
    "                                  shuffle=(do_shuffle == \"true\"), num_shards=device_num, shard_id=rank)\n",
    "    else:\n",
    "        data_set = ds.TFRecordDataset(data_files, schema_dir if schema_dir != \"\" else None, columns_list=columns_list,\n",
    "                                      shuffle=shuffle, num_shards=device_num, shard_id=rank,\n",
    "                                      shard_equal_rows=shard_equal_rows)\n",
    "    if device_num == 1 and shuffle is True:\n",
    "        data_set = data_set.shuffle(10000)\n",
    "\n",
    "    type_cast_op = transforms.TypeCast(mstype.int32)\n",
    "    data_set = data_set.map(operations=type_cast_op, input_columns=\"segment_ids\")\n",
    "    data_set = data_set.map(operations=type_cast_op, input_columns=\"input_mask\")\n",
    "    data_set = data_set.map(operations=type_cast_op, input_columns=\"input_ids\")\n",
    "    if task == \"td\":\n",
    "        data_set = data_set.map(operations=type_cast_op, input_columns=\"label_ids\")\n",
    "    # apply batch operations\n",
    "    data_set = data_set.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174d7b0f-8f6f-48e7-9845-6aae3b3010da",
   "metadata": {},
   "source": [
    "# 训练部分。\n",
    "\n",
    "由于大部分关于训练的部分都已经写在模型之中，所以这一部分只简单的规定了学习率优化器，评价指标， 模型保存等内容。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2621b7d-499c-4839-91ea-18266ea8d36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy():\n",
    "    \"\"\"Accuracy\"\"\"\n",
    "    def __init__(self):\n",
    "        self.acc_num = 0\n",
    "        self.total_num = 0\n",
    "\n",
    "    def update(self, logits, labels):\n",
    "        labels = labels.asnumpy()\n",
    "        labels = np.reshape(labels, -1)\n",
    "        logits = logits.asnumpy()\n",
    "        logit_id = np.argmax(logits, axis=-1)\n",
    "        self.acc_num += np.sum(labels == logit_id)\n",
    "        self.total_num += len(labels)\n",
    "\n",
    "class BertLearningRate(LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    Warmup-decay learning rate for Bert network.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate, end_learning_rate, warmup_steps, decay_steps, power):\n",
    "        super(BertLearningRate, self).__init__()\n",
    "        self.warmup_flag = False\n",
    "        if warmup_steps > 0:\n",
    "            self.warmup_flag = True\n",
    "            self.warmup_lr = WarmUpLR(learning_rate, warmup_steps)\n",
    "        self.decay_lr = PolynomialDecayLR(learning_rate, end_learning_rate, decay_steps, power)\n",
    "        self.warmup_steps = Tensor(np.array([warmup_steps]).astype(np.float32))\n",
    "\n",
    "        self.greater = P.Greater()\n",
    "        self.one = Tensor(np.array([1.0]).astype(np.float32))\n",
    "        self.cast = P.Cast()\n",
    "\n",
    "    def construct(self, global_step):\n",
    "        decay_lr = self.decay_lr(global_step)\n",
    "        if self.warmup_flag:\n",
    "            is_warmup = self.cast(self.greater(self.warmup_steps, global_step), mstype.float32)\n",
    "            warmup_lr = self.warmup_lr(global_step)\n",
    "            lr = (self.one - is_warmup) * decay_lr + is_warmup * warmup_lr\n",
    "        else:\n",
    "            lr = decay_lr\n",
    "        return lr\n",
    "\n",
    "\n",
    "class LossCallBack(Callback):\n",
    "    \"\"\"\n",
    "    Monitor the loss in training.\n",
    "    If the loss in NAN or INF terminating training.\n",
    "    Note:\n",
    "        if per_print_times is 0 do not print loss.\n",
    "    Args:\n",
    "        per_print_times (int): Print loss every times. Default: 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, per_print_times=1):\n",
    "        super(LossCallBack, self).__init__()\n",
    "        if not isinstance(per_print_times, int) or per_print_times < 0:\n",
    "            raise ValueError(\"print_step must be int and >= 0\")\n",
    "        self._per_print_times = per_print_times\n",
    "\n",
    "    def step_end(self, run_context):\n",
    "        \"\"\"step end and print loss\"\"\"\n",
    "        cb_params = run_context.original_args()\n",
    "        print(\"epoch: {}, step: {}, outputs are {}\".format(cb_params.cur_epoch_num,\n",
    "                                                           cb_params.cur_step_num,\n",
    "                                                           str(cb_params.net_outputs)))\n",
    "\n",
    "class ModelSaveCkpt(Callback):\n",
    "    \"\"\"\n",
    "    Saves checkpoint.\n",
    "    If the loss in NAN or INF terminating training.\n",
    "    Args:\n",
    "        network (Network): The train network for training.\n",
    "        save_ckpt_num (int): The number to save checkpoint, default is 1000.\n",
    "        max_ckpt_num (int): The max checkpoint number, default is 3.\n",
    "    \"\"\"\n",
    "    def __init__(self, network, save_ckpt_step, max_ckpt_num, output_dir):\n",
    "        super(ModelSaveCkpt, self).__init__()\n",
    "        self.count = 0\n",
    "        self.network = network\n",
    "        self.save_ckpt_step = save_ckpt_step\n",
    "        self.max_ckpt_num = max_ckpt_num\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def step_end(self, run_context):\n",
    "        \"\"\"step end and save ckpt\"\"\"\n",
    "        cb_params = run_context.original_args()\n",
    "        if cb_params.cur_step_num % self.save_ckpt_step == 0:\n",
    "            saved_ckpt_num = cb_params.cur_step_num / self.save_ckpt_step\n",
    "            if saved_ckpt_num > self.max_ckpt_num:\n",
    "                oldest_ckpt_index = saved_ckpt_num - self.max_ckpt_num\n",
    "                # path = os.path.join(self.output_dir, \"tiny_bert_{}_{}.ckpt\".format(int(oldest_ckpt_index),\n",
    "                #                                                                    self.save_ckpt_step))\n",
    "                path = os.path.join(self.output_dir, \"tiny_bert_wiki.ckpt\".format(int(oldest_ckpt_index),\n",
    "                                                                   self.save_ckpt_step))\n",
    "                if os.path.exists(path):\n",
    "                    os.remove(path)\n",
    "            # save_checkpoint(self.network, os.path.join(self.output_dir,\n",
    "            #                                            \"tiny_bert_{}_{}.ckpt\".format(int(saved_ckpt_num),\n",
    "            #                                                                          self.save_ckpt_step)))\n",
    "            save_checkpoint(self.network, os.path.join(self.output_dir,\n",
    "                                                       \"tiny_bert_wiki.ckpt\".format(int(saved_ckpt_num),\n",
    "                                                                                     self.save_ckpt_step)))\n",
    "\n",
    "class EvalCallBack(Callback):\n",
    "    \"\"\"Evaluation callback\"\"\"\n",
    "    def __init__(self, network, dataset):\n",
    "        super(EvalCallBack, self).__init__()\n",
    "        self.network = network\n",
    "        self.global_acc = 0.0\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def epoch_end(self, run_context):\n",
    "        \"\"\"step end and do evaluation\"\"\"\n",
    "        callback = Accuracy()\n",
    "        columns_list = [\"input_ids\", \"input_mask\", \"segment_ids\", \"label_ids\"]\n",
    "        for data in self.dataset.create_dict_iterator(num_epochs=1):\n",
    "            input_data = []\n",
    "            for i in columns_list:\n",
    "                input_data.append(data[i])\n",
    "            input_ids, input_mask, token_type_id, label_ids = input_data\n",
    "            self.network.set_train(False)\n",
    "            logits = self.network(input_ids, token_type_id, input_mask)\n",
    "            self.network.set_train(True)\n",
    "            callback.update(logits, label_ids)\n",
    "        acc = callback.acc_num / callback.total_num\n",
    "        with open(\"./eval.log\", \"a+\") as f:\n",
    "            f.write(\"acc_num {}, total_num{}, accuracy{:.6f}\".format(callback.acc_num, callback.total_num,\n",
    "                                                                     callback.acc_num / callback.total_num))\n",
    "            f.write('\\n')\n",
    "\n",
    "        if acc > self.global_acc:\n",
    "            self.global_acc = acc\n",
    "            print(\"The best acc is {}\".format(acc))\n",
    "            eval_model_ckpt_file = \"eval_model.ckpt\"\n",
    "            if os.path.exists(eval_model_ckpt_file):\n",
    "                os.remove(eval_model_ckpt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6543878c-ccf9-4fbf-92b2-d15db63bf6dd",
   "metadata": {},
   "source": [
    "# Config 设置\n",
    "\n",
    "config 是代码运行的基础。这里通过手动设置和yaml文件读取两个方式共同创建config。\n",
    "\n",
    "首先定义从文件读取config的函数。\n",
    "- Config ： 定义config\n",
    "- parse_yaml ： 读取yaml文件\n",
    "- parse_cli_to_yaml： 把yaml文件加入argparse中\n",
    "- extra_operations : 将总config分离成几个负责单独部分的config。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9243c11b-7315-4ad8-a525-bfc21c175a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"\n",
    "    Configuration namespace. Convert dictionary to members.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg_dict):\n",
    "        for k, v in cfg_dict.items():\n",
    "            if isinstance(v, (list, tuple)):\n",
    "                setattr(self, k, [Config(x) if isinstance(x, dict) else x for x in v])\n",
    "            else:\n",
    "                setattr(self, k, Config(v) if isinstance(v, dict) else v)\n",
    "\n",
    "    def __str__(self):\n",
    "        return pformat(self.__dict__)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "def parse_cli_to_yaml(parser, cfg, helper=None, choices=None, cfg_path=\"pretrain_base_config.yaml\"):\n",
    "    \"\"\"\n",
    "    Parse command line arguments to the configuration according to the default yaml.\n",
    "\n",
    "    Args:\n",
    "        parser: Parent parser.\n",
    "        cfg: Base configuration.\n",
    "        helper: Helper description.\n",
    "        cfg_path: Path to the default yaml config.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"[REPLACE THIS at config.py]\",\n",
    "                                     parents=[parser])\n",
    "    helper = {} if helper is None else helper\n",
    "    choices = {} if choices is None else choices\n",
    "    for item in cfg:\n",
    "        try:\n",
    "            if not isinstance(cfg[item], list) and not isinstance(cfg[item], dict):\n",
    "                help_description = helper[item] if item in helper else \"Please reference to {}\".format(cfg_path)\n",
    "                choice = choices[item] if item in choices else None\n",
    "                if isinstance(cfg[item], bool):\n",
    "                    parser.add_argument(\"--\" + item, type=ast.literal_eval, default=cfg[item], choices=choice,\n",
    "                                        help=help_description)\n",
    "                else:\n",
    "                    parser.add_argument(\"--\" + item, type=type(cfg[item]), default=cfg[item], choices=choice,\n",
    "                                        help=help_description)\n",
    "        except:\n",
    "            pass\n",
    "    args = parser.parse_args(args=[])\n",
    "    return args\n",
    "\n",
    "\n",
    "def parse_yaml(yaml_path):\n",
    "    \"\"\"\n",
    "    Parse the yaml config file.\n",
    "\n",
    "    Args:\n",
    "        yaml_path: Path to the yaml config.\n",
    "    \"\"\"\n",
    "    with open(yaml_path, 'r') as fin:\n",
    "        try:\n",
    "            cfgs = yaml.load_all(fin.read(), Loader=yaml.FullLoader)\n",
    "            cfgs = [x for x in cfgs]\n",
    "            if len(cfgs) == 1:\n",
    "                cfg_helper = {}\n",
    "                cfg = cfgs[0]\n",
    "                cfg_choices = {}\n",
    "            elif len(cfgs) == 2:\n",
    "                cfg, cfg_helper = cfgs\n",
    "                cfg_choices = {}\n",
    "            elif len(cfgs) == 3:\n",
    "                cfg, cfg_helper, cfg_choices = cfgs\n",
    "            else:\n",
    "                raise ValueError(\"At most 3 docs (config, description for help, choices) are supported in config yaml\")\n",
    "            # print(cfg_helper)\n",
    "        except:\n",
    "            raise ValueError(\"Failed to parse yaml\")\n",
    "    return cfg, cfg_helper, cfg_choices\n",
    "\n",
    "\n",
    "def merge(args, cfg):\n",
    "    \"\"\"\n",
    "    Merge the base config from yaml file and command line arguments.\n",
    "\n",
    "    Args:\n",
    "        args: Command line arguments.\n",
    "        cfg: Base configuration.\n",
    "    \"\"\"\n",
    "    args_var = vars(args)\n",
    "    for item in args_var:\n",
    "        cfg[item] = args_var[item]\n",
    "    return cfg\n",
    "\n",
    "def extra_operations(cfg):\n",
    "    \"\"\"\n",
    "    Do extra work on config\n",
    "\n",
    "    Args:\n",
    "        config: Object after instantiation of class 'Config'.\n",
    "    \"\"\"\n",
    "    def create_filter_fun(keywords):\n",
    "        return lambda x: not (True in [key in x.name.lower() for key in keywords])\n",
    "\n",
    "    if cfg.description == 'general_distill':\n",
    "        cfg.common_cfg.loss_scale_value = 2 ** 16\n",
    "        cfg.common_cfg.AdamWeightDecay.decay_filter = create_filter_fun(cfg.common_cfg.AdamWeightDecay.decay_filter)\n",
    "        cfg.bert_teacher_net_cfg.dtype = mstype.float32\n",
    "        cfg.bert_teacher_net_cfg.compute_type = mstype.float16\n",
    "        cfg.bert_student_net_cfg.dtype = mstype.float32\n",
    "        cfg.bert_student_net_cfg.compute_type = mstype.float16\n",
    "        cfg.bert_teacher_net_cfg = BertConfig(**cfg.bert_teacher_net_cfg.__dict__)\n",
    "        cfg.bert_student_net_cfg = BertConfig(**cfg.bert_student_net_cfg.__dict__)\n",
    "    elif cfg.description == 'task_distill':\n",
    "        cfg.phase1_cfg.loss_scale_value = 2 ** 8\n",
    "        cfg.phase1_cfg.optimizer_cfg.AdamWeightDecay.decay_filter = create_filter_fun(\n",
    "            cfg.phase1_cfg.optimizer_cfg.AdamWeightDecay.decay_filter)\n",
    "        cfg.phase2_cfg.loss_scale_value = 2 ** 16\n",
    "        cfg.phase2_cfg.optimizer_cfg.AdamWeightDecay.decay_filter = create_filter_fun(\n",
    "            cfg.phase2_cfg.optimizer_cfg.AdamWeightDecay.decay_filter)\n",
    "        cfg.td_teacher_net_cfg.dtype = mstype.float32\n",
    "        cfg.td_teacher_net_cfg.compute_type = mstype.float16\n",
    "        cfg.td_student_net_cfg.dtype = mstype.float32\n",
    "        cfg.td_student_net_cfg.compute_type = mstype.float16\n",
    "        cfg.td_teacher_net_cfg = BertConfig(**cfg.td_teacher_net_cfg.__dict__)\n",
    "        cfg.td_student_net_cfg = BertConfig(**cfg.td_student_net_cfg.__dict__)\n",
    "    else:\n",
    "        pass\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21df2693-243c-4ed4-ad35-9f51f8bb7a1e",
   "metadata": {},
   "source": [
    "\n",
    "# 预训练蒸馏。\n",
    "定义好了模型，数据集，config和一些杂项，我们便可以开始在wiki数据集上的预训练蒸馏任务。\n",
    "\n",
    "\n",
    "## 预训练蒸馏config\n",
    "config 是代码运行的基础。这里通过手动设置和yaml文件读取两个方式共同创建config。 \n",
    "其中 与模型和训练相关的大部分config都在 yaml文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f81d090f-c752-4730-818d-8c59364ad16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bert_student_net_cfg': <__main__.BertConfig object at 0xffff1c02f590>,\n",
      " 'bert_teacher_net_cfg': <__main__.BertConfig object at 0xffff1bfeb490>,\n",
      " 'checkpoint_url': '',\n",
      " 'common_cfg': {'AdamWeightDecay': {'decay_filter': <function extra_operations.<locals>.create_filter_fun.<locals>.<lambda> at 0xffff1bff0170>,\n",
      " 'end_learning_rate': 1e-14,\n",
      " 'eps': 1e-06,\n",
      " 'learning_rate': 5e-05,\n",
      " 'power': 1.0,\n",
      " 'weight_decay': 0.0001},\n",
      " 'batch_size': 32,\n",
      " 'loss_scale_value': 65536,\n",
      " 'scale_factor': 2,\n",
      " 'scale_window': 1000},\n",
      " 'data_dir': 'data/wiki',\n",
      " 'data_path': '/cache/data',\n",
      " 'data_sink_steps': 1,\n",
      " 'data_url': '',\n",
      " 'dataset_type': 'tfrecord',\n",
      " 'description': 'general_distill',\n",
      " 'device_id': 0,\n",
      " 'device_num': 1,\n",
      " 'device_target': 'Ascend',\n",
      " 'distribute': 'False',\n",
      " 'do_shuffle': 'true',\n",
      " 'enable_data_sink': 'true',\n",
      " 'enable_modelarts': False,\n",
      " 'enable_profiling': False,\n",
      " 'epoch_size': 3,\n",
      " 'folder_name_under_zip_file': './',\n",
      " 'load_path': '/cache/checkpoint_path',\n",
      " 'load_teacher_ckpt_path': 'bert/ms_model_ckpt.ckpt',\n",
      " 'max_ckpt_num': 1,\n",
      " 'modelarts_dataset_unzip_name': '',\n",
      " 'output_path': '/cache/train',\n",
      " 'save_ckpt_path': 'save/tinybert_wiki',\n",
      " 'save_ckpt_step': 1,\n",
      " 'schema_dir': '',\n",
      " 'train_url': ''}\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "parser_gen = argparse.ArgumentParser(description=\"default name\", add_help=False)\n",
    "\n",
    "parser_gen.add_argument(\"--distribute\", default=\"False\",\n",
    "                    help=\"if distribute\")\n",
    "\n",
    "parser_gen.add_argument(\"--device_target\", default=\"Ascend\",\n",
    "                    help=\"device_target\")\n",
    "\n",
    "parser_gen.add_argument(\"--epoch_size\", default=3,\n",
    "                    help=\"epoch_size\")\n",
    "\n",
    "\n",
    "parser_gen.add_argument(\"--save_ckpt_step\", default=1,\n",
    "                    help=\"save_ckpt_step\")\n",
    "\n",
    "parser_gen.add_argument(\"--max_ckpt_num\", default=1,\n",
    "                    help=\"max_ckpt_num\")\n",
    "\n",
    "parser_gen.add_argument(\"--save_ckpt_path\", default=\"save/tinybert_wiki\",\n",
    "                    help=\"save_ckpt_path\")\n",
    "\n",
    "parser_gen.add_argument(\"--data_dir\", default=\"data/wiki\",\n",
    "                    help=\"data_dir\")\n",
    "\n",
    "parser_gen.add_argument(\"--load_teacher_ckpt_path\", default=\"bert/ms_model_ckpt.ckpt\",\n",
    "                    help=\"load_teacher_ckpt_path\")\n",
    "\n",
    "parser_gen.add_argument(\"--dataset_type\", default=\"tfrecord\",\n",
    "                    help=\"dataset_type\")\n",
    "\n",
    "config_gen_path = 'config/gd_config.yaml'\n",
    "default_gen, helper_gen, choices_gen = parse_yaml(config_gen_path)\n",
    "args_gen = parse_cli_to_yaml(parser=parser_gen, cfg=default_gen, helper=helper_gen, choices=choices_gen, cfg_path=config_gen_path)\n",
    "\n",
    "final_config_gen = merge(args_gen, default_gen)\n",
    "config_obj_gen = Config(final_config_gen)\n",
    "\n",
    "config = extra_operations(config_obj_gen)\n",
    "\n",
    "\n",
    "common_cfg = config.common_cfg\n",
    "bert_teacher_net_cfg = config.bert_teacher_net_cfg\n",
    "bert_student_net_cfg = config.bert_student_net_cfg\n",
    "\n",
    "args_opt = config\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92522250-0e8a-40ec-a7bc-363b2cf51cd4",
   "metadata": {},
   "source": [
    "## 环境设置。\n",
    "\n",
    "context 是用来存储训练时的环境变量的。 这里定义了存储文件夹和训练时的一些基础环境。具体含义见[ms文档](https://www.mindspore.cn/docs/zh-CN/r1.8/index.html)， \n",
    "\n",
    "在这里不启用分布式训练。在Ascend 环境中，也不使用混合精度训练。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dae6d093-48bb-4e7d-990d-9f60a2c7b493",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "set_seed(0)\n",
    "\n",
    "context.set_context(mode=context.PYNATIVE_MODE, device_target=args_opt.device_target,\n",
    "                    reserve_class_name_in_scope=False)\n",
    "\n",
    "\n",
    "# save_ckpt_dir = os.path.join(args_opt.save_ckpt_path,\n",
    "#                              datetime.datetime.now().strftime('%Y-%m-%d_time_%H_%M_%S'))\n",
    "save_ckpt_dir = args_opt.save_ckpt_path\n",
    "\n",
    "if not os.path.exists(save_ckpt_dir):\n",
    "    os.makedirs(save_ckpt_dir)\n",
    "    \n",
    "if args_opt.distribute == \"true\":\n",
    "    if args_opt.device_target == 'Ascend':\n",
    "        D.init()\n",
    "        device_num = args_opt.device_num\n",
    "        rank = args_opt.device_id % device_num\n",
    "    else:\n",
    "        D.init()\n",
    "        device_num = D.get_group_size()\n",
    "        rank = D.get_rank()\n",
    "    save_ckpt_dir = save_ckpt_dir + '_ckpt_' + str(rank)\n",
    "    context.reset_auto_parallel_context()\n",
    "    context.set_auto_parallel_context(parallel_mode=ParallelMode.DATA_PARALLEL, gradients_mean=True,\n",
    "                                      device_num=device_num)\n",
    "else:\n",
    "    rank = 0\n",
    "    device_num = 1\n",
    "    \n",
    "enable_loss_scale = True \n",
    "\n",
    "if args_opt.device_target == \"Ascend\":\n",
    "    context.set_context(device_id=args_opt.device_id)\n",
    "    \n",
    "if args_opt.device_target == \"GPU\":\n",
    "    context.set_context(enable_graph_kernel=True)\n",
    "    if bert_student_net_cfg.compute_type != mstype.float32:\n",
    "        logger.warning('Compute about the student only support float32 temporarily, run with float32.')\n",
    "        bert_student_net_cfg.compute_type = mstype.float32\n",
    "    # Backward of the network are calculated using fp32,\n",
    "    # and the loss scale is not necessary\n",
    "    enable_loss_scale = False\n",
    "\n",
    "if args_opt.device_target == \"CPU\":\n",
    "    logger.warning('CPU only support float32 temporarily, run with float32.')\n",
    "    bert_teacher_net_cfg.dtype = mstype.float32\n",
    "    bert_teacher_net_cfg.compute_type = mstype.float32\n",
    "    bert_student_net_cfg.dtype = mstype.float32\n",
    "    bert_student_net_cfg.compute_type = mstype.float32\n",
    "    enable_loss_scale = False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1166bf95-81f9-4000-b71b-4214c5b604d6",
   "metadata": {},
   "source": [
    "## 读取数据\n",
    "\n",
    "这里数据格式为TFRECORD， 利用上面定义的函数创建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30f1925e-2546-45ec-bf61-1d2160fe4d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size:  251\n",
      "dataset repeatcount:  1\n"
     ]
    }
   ],
   "source": [
    " \n",
    "if args_opt.dataset_type == \"tfrecord\":\n",
    "    dataset_type = DataType.TFRECORD\n",
    "elif args_opt.dataset_type == \"mindrecord\":\n",
    "    dataset_type = DataType.MINDRECORD\n",
    "else:\n",
    "    raise Exception(\"dataset format is not supported yet\")\n",
    "    \n",
    "dataset = create_tinybert_dataset('gd', common_cfg.batch_size, device_num, rank,\n",
    "                                  args_opt.do_shuffle, args_opt.data_dir, args_opt.schema_dir,\n",
    "                                  data_type=dataset_type)\n",
    "dataset_size = dataset.get_dataset_size()\n",
    "print('dataset size: ', dataset_size)\n",
    "print(\"dataset repeatcount: \", dataset.get_repeat_count())\n",
    "\n",
    "\n",
    "repeat_count = args_opt.epoch_size\n",
    "time_monitor_steps = dataset_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561c8f7a-a220-4d16-ab79-142495c714e1",
   "metadata": {},
   "source": [
    "## 定义模型\n",
    "  \n",
    "我们使用刚才定义的用于general distill的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ace3842-f79a-4db7-85d8-50724797ad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "netwithloss = BertNetworkWithLoss_gd(teacher_config=bert_teacher_net_cfg,\n",
    "                                     teacher_ckpt=args_opt.load_teacher_ckpt_path,\n",
    "                                     student_config=bert_student_net_cfg,\n",
    "                                     is_training=True, use_one_hot_embeddings=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ab640d-53ac-4f9b-8e83-c7b17f54dc4f",
   "metadata": {},
   "source": [
    "## 定义学习率和优化器\n",
    "\n",
    "学习率使用上面定义的BERT的学习率函数，优化器用adam。\n",
    "\n",
    "netwithgrads 模型中已经定义好了梯度计算与回传，将学习率与优化器传入即可。\n",
    "\n",
    "\n",
    "ModelSaveCkpt 定义了模型的保存方式。 注意因为这里只是预训练示例，因此训练数据非常小。 示例中只能把保存的step：<font color=black size=2 face=雅黑>**save_ckpt_step**</font> 调整的非常小。这里定义为1. 实际模型中可以考虑设置为100 或者 200等。这里为了简化， 将保存的模型统一命名为 <font color=black size=2 face=雅黑>**tiny_bert_wiki.ckpt**</font>. 方便后续使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4ffb446-4c85-46c1-9027-df8fb8e10db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = BertLearningRate(learning_rate=common_cfg.AdamWeightDecay.learning_rate,\n",
    "                               end_learning_rate=common_cfg.AdamWeightDecay.end_learning_rate,\n",
    "                               warmup_steps=int(dataset_size * args_opt.epoch_size / 10),\n",
    "                               decay_steps=int(dataset_size * args_opt.epoch_size),\n",
    "                               power=common_cfg.AdamWeightDecay.power)\n",
    "params = netwithloss.trainable_params()\n",
    "decay_params = list(filter(common_cfg.AdamWeightDecay.decay_filter, params))\n",
    "other_params = list(filter(lambda x: not common_cfg.AdamWeightDecay.decay_filter(x), params))\n",
    "group_params = [{'params': decay_params, 'weight_decay': common_cfg.AdamWeightDecay.weight_decay},\n",
    "                {'params': other_params, 'weight_decay': 0.0},\n",
    "                {'order_params': params}]\n",
    "\n",
    "optimizer = AdamWeightDecay(group_params, learning_rate=lr_schedule, eps=common_cfg.AdamWeightDecay.eps)\n",
    "\n",
    "callback = [TimeMonitor(time_monitor_steps), LossCallBack(), ModelSaveCkpt(netwithloss.bert,\n",
    "                                                                           args_opt.save_ckpt_step,\n",
    "                                                                           args_opt.max_ckpt_num,\n",
    "                                                                           save_ckpt_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9713a14a-1763-4b83-963a-8a770b5bc380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enable_loss_scale=True\n"
     ]
    }
   ],
   "source": [
    "if enable_loss_scale:\n",
    "    update_cell = DynamicLossScaleUpdateCell(loss_scale_value=common_cfg.loss_scale_value,\n",
    "                                             scale_factor=common_cfg.scale_factor,\n",
    "                                             scale_window=common_cfg.scale_window)\n",
    "    netwithgrads = BertTrainWithLossScaleCell(netwithloss, optimizer=optimizer, scale_update_cell=update_cell)\n",
    "    print('enable_loss_scale=True')\n",
    "else:\n",
    "    netwithgrads = BertTrainCell(netwithloss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e993927f-0190-4dea-80a7-4f3ed608e6d2",
   "metadata": {},
   "source": [
    "## 模型训练\n",
    "\n",
    "mindspore中，[Model](https://mindspore.cn/docs/zh-CN/r1.7/api_python/mindspore/mindspore.Model.html#mindspore.Model)是MindSpore提供的高阶API，可以进行模型训练、评估和推理。 \n",
    "\n",
    "具体见[Model详情页](https://www.mindspore.cn/tutorials/zh-CN/r1.7/advanced/train/model.html)\n",
    "\n",
    "我们可以把上面定义的模型，训练设置等传入Model，Model即可自动训练。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e82ff4e7-853c-4297-854c-b532f94aea02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(3410:281472849991552,MainProcess):2022-11-11-15:21:43.600.586 [mindspore/train/model.py:1077] For LossCallBack callback, {'step_end'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.\n",
      "[WARNING] ME(3410:281472849991552,MainProcess):2022-11-11-15:21:43.601.995 [mindspore/train/model.py:1077] For ModelSaveCkpt callback, {'step_end'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:22:02.877.166 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op191] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:22:03.395.963 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op237] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:22:03.581.466 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op256] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:22:03.842.757 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op277] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:22:03.984.361 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op287] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:22:04.107.539 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op305] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:22:04.294.265 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op323] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:22:04.599.430 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op355] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:22:04.725.194 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op373] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:22:04.921.532 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op391] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:22:05.243.184 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op423] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:22:05.364.876 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op441] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:22:05.557.881 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op459] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:22:09.423.190 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/gradStridedSlice/StridedSliceGrad-op916] don't support int64, reduce precision from int64 to int32.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, step: 1, outputs are (Tensor(shape=[], dtype=Float32, value= 52.5542), Tensor(shape=[], dtype=Bool, value= False), Parameter (name=loss_scale, shape=(), dtype=Float32, requires_grad=True))\n",
      "Train epoch time: 51524.849 ms, per step time: 51524.849 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:22:36.504.319 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op4729] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:22:37.449.514 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op4795] don't support int64, reduce precision from int64 to int32.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, step: 2, outputs are (Tensor(shape=[], dtype=Float32, value= 48.2302), Tensor(shape=[], dtype=Bool, value= False), Parameter (name=loss_scale, shape=(), dtype=Float32, requires_grad=True))\n",
      "Train epoch time: 3467.957 ms, per step time: 3467.957 ms\n",
      "epoch: 3, step: 3, outputs are (Tensor(shape=[], dtype=Float32, value= 49.1845), Tensor(shape=[], dtype=Bool, value= False), Parameter (name=loss_scale, shape=(), dtype=Float32, requires_grad=True))\n",
      "Train epoch time: 927.168 ms, per step time: 927.168 ms\n"
     ]
    }
   ],
   "source": [
    "model = Model(netwithgrads)\n",
    "model.train(repeat_count, dataset, callbacks=callback,\n",
    "            dataset_sink_mode=(args_opt.enable_data_sink == \"true\"),\n",
    "            sink_size=args_opt.data_sink_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a22e05a-eef3-4191-9856-4cbfd9ccb353",
   "metadata": {},
   "source": [
    "# 下游任务蒸馏。\n",
    "\n",
    "下面我们开始以QNLI数据集为例，进行任务蒸馏。\n",
    "\n",
    "QNLI是从另一个数据集The Stanford Question Answering Dataset(斯坦福问答数据集, SQuAD 1.0）转换而来的。SQuAD 1.0是有一个问题-段落对组成的问答数据集，其中段落来自维基百科，段落中的一个句子包含问题的答案。\n",
    "\n",
    "QNLI目标是判断问题（question）和句子（sentence，维基百科段落中的一句）是否蕴含，蕴含和不蕴含，二分类。我们可以简单的当作分类任务即可。\n",
    "\n",
    "## 任务蒸馏config\n",
    "config 是代码运行的基础。这里通过手动设置和yaml文件读取两个方式共同创建config。 \n",
    "其中 与模型和训练相关的大部分config都在 yaml文件中。\n",
    "\n",
    "注意在代码中，任务蒸馏分成了两个阶段。 而两个阶段仅仅是训练超参不同，然后仅在第二阶段进行对验证集的预测。 所以config也要分开为两个。 这一点， 可以在自己训练时根据自己喜好，只进行一个阶段也是可以的。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73352f1d-872c-4619-bff9-d233ccf2175e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'assessment_method': 'accuracy',\n",
      " 'checkpoint_url': '',\n",
      " 'ckpt_file': '',\n",
      " 'data_path': '/cache/data',\n",
      " 'data_sink_steps': 1,\n",
      " 'data_url': '',\n",
      " 'dataset_type': 'tfrecord',\n",
      " 'description': 'task_distill',\n",
      " 'device_id': 0,\n",
      " 'device_target': 'Ascend',\n",
      " 'do_eval': 'True',\n",
      " 'do_shuffle': 'true',\n",
      " 'do_train': 'True',\n",
      " 'enable_data_sink': 'true',\n",
      " 'enable_modelarts': False,\n",
      " 'enable_profiling': False,\n",
      " 'eval_cfg': {'batch_size': 32},\n",
      " 'eval_data_dir': 'data/glue/qnli',\n",
      " 'file_format': 'MINDIR',\n",
      " 'file_name': 'tinybert',\n",
      " 'folder_name_under_zip_file': '',\n",
      " 'load_gd_ckpt_path': 'save/tinybert_wiki/tiny_bert_wiki.ckpt',\n",
      " 'load_path': '/cache/checkpoint_path',\n",
      " 'load_td1_ckpt_path': '',\n",
      " 'load_teacher_ckpt_path': 'bert/ms_model_ckpt.ckpt',\n",
      " 'max_ckpt_num': 1,\n",
      " 'modelarts_dataset_unzip_name': '',\n",
      " 'num_labels': 2,\n",
      " 'onnx_path': '',\n",
      " 'output_path': '/cache/train',\n",
      " 'phase1_cfg': {'batch_size': 256,\n",
      " 'loss_scale_value': 256,\n",
      " 'optimizer_cfg': {'AdamWeightDecay': {'decay_filter': <function extra_operations.<locals>.create_filter_fun.<locals>.<lambda> at 0xffff00266d40>,\n",
      " 'end_learning_rate': 0.0,\n",
      " 'eps': 1e-06,\n",
      " 'learning_rate': 5e-05,\n",
      " 'power': 1.0,\n",
      " 'weight_decay': 0.0001}},\n",
      " 'scale_factor': 2,\n",
      " 'scale_window': 50},\n",
      " 'phase2_cfg': {'batch_size': 64,\n",
      " 'loss_scale_value': 65536,\n",
      " 'optimizer_cfg': {'AdamWeightDecay': {'decay_filter': <function extra_operations.<locals>.create_filter_fun.<locals>.<lambda> at 0xffff00266dd0>,\n",
      " 'end_learning_rate': 0.0,\n",
      " 'eps': 1e-06,\n",
      " 'learning_rate': 2e-05,\n",
      " 'power': 1.0,\n",
      " 'weight_decay': 0.0001}},\n",
      " 'scale_factor': 2,\n",
      " 'scale_window': 50},\n",
      " 'schema_dir': '',\n",
      " 'task_name': 'QNLI',\n",
      " 'task_type': 'classification',\n",
      " 'td_phase1_epoch_size': 1,\n",
      " 'td_phase2_epoch_size': 1,\n",
      " 'td_student_net_cfg': <__main__.BertConfig object at 0xffff001c6150>,\n",
      " 'td_teacher_net_cfg': <__main__.BertConfig object at 0xffff001d4290>,\n",
      " 'train_data_dir': 'data/glue/qnli',\n",
      " 'train_url': ''}\n"
     ]
    }
   ],
   "source": [
    "###################task_config####################\n",
    "\n",
    "\n",
    "parser_task = argparse.ArgumentParser(description=\"default name\", add_help=False)\n",
    "\n",
    "parser_task.add_argument(\"--do_train\", default=\"True\",\n",
    "                    help=\"do_train\")\n",
    "\n",
    "parser_task.add_argument(\"--do_eval\", default=\"True\",\n",
    "                    help=\"do_eval\")\n",
    "\n",
    "parser_task.add_argument(\"--device_target\", default=\"Ascend\",\n",
    "                    help=\"device_target\")\n",
    "\n",
    "parser_task.add_argument(\"--device_id\", default=0,\n",
    "                    help=\"device_id\")\n",
    "\n",
    "parser_task.add_argument(\"--td_phase1_epoch_size\", default=1,\n",
    "                    help=\"td_phase1_epoch_size\")\n",
    "\n",
    "parser_task.add_argument(\"--td_phase2_epoch_size\", default=1,\n",
    "                    help=\"td_phase2_epoch_size\")\n",
    "\n",
    "parser_task.add_argument(\"--do_shuffle\", default=\"true\",\n",
    "                    help=\"do_shuffle\")\n",
    "\n",
    "parser_task.add_argument(\"--max_ckpt_num\", default=1,\n",
    "                    help=\"max_ckpt_num\")\n",
    "\n",
    "parser_task.add_argument(\"--load_teacher_ckpt_path\", default=\"bert/ms_model_ckpt.ckpt\",\n",
    "                    help=\"load_teacher_ckpt_path\")\n",
    "\n",
    "parser_task.add_argument(\"--load_gd_ckpt_path\", default=\"save/tinybert_wiki/tiny_bert_wiki.ckpt\",\n",
    "                    help=\"load_gd_ckpt_path\")\n",
    "\n",
    "parser_task.add_argument(\"--load_td1_ckpt_path\", default=\"\",\n",
    "                    help=\"load_td1_ckpt_path\")\n",
    "\n",
    "parser_task.add_argument(\"--train_data_dir\", default=\"data/glue/qnli\",\n",
    "                    help=\"train_data_dir\")\n",
    "\n",
    "parser_task.add_argument(\"--eval_data_dir\", default=\"data/glue/qnli\",\n",
    "                    help=\"eval_data_dir\")\n",
    "\n",
    "parser_task.add_argument(\"--dataset_type\", default=\"tfrecord\",\n",
    "                    help=\"dataset_type\")\n",
    "\n",
    "parser_task.add_argument(\"--task_type\", default=\"classification\",\n",
    "                    help=\"task_type\")\n",
    "\n",
    "parser_task.add_argument(\"--task_name\", default=\"QNLI\",\n",
    "                    help=\"task_name\")\n",
    "\n",
    "parser_task.add_argument(\"--assessment_method\", default=\"accuracy\",\n",
    "                    help=\"assessment_method\")\n",
    "\n",
    "\n",
    "config_task_path = 'config/td_config_qnli.yaml'\n",
    "default_task, helper_task, choices_task = parse_yaml(config_task_path)\n",
    "args_task = parse_cli_to_yaml(parser=parser_task, cfg=default_task, helper=helper_task, choices=choices_task, cfg_path=config_task_path)\n",
    "final_config_task = merge(args_task, default_task)\n",
    "config_obj_task = Config(final_config_task)\n",
    "config_task = extra_operations(config_obj_task)\n",
    "config = config_task\n",
    "\n",
    "phase1_cfg = config.phase1_cfg\n",
    "phase2_cfg = config.phase2_cfg\n",
    "eval_cfg = config.eval_cfg\n",
    "td_teacher_net_cfg = config.td_teacher_net_cfg\n",
    "td_student_net_cfg = config.td_student_net_cfg\n",
    "\n",
    "\n",
    "print(config)\n",
    "args_opt = config\n",
    "\n",
    "_cur_dir = os.getcwd()\n",
    "td_phase1_save_ckpt_dir = os.path.join(_cur_dir, 'save/tinybert_td_phase1_save_ckpt')\n",
    "td_phase2_save_ckpt_dir = os.path.join(_cur_dir, 'save/tinybert_td_phase2_save_ckpt')\n",
    "if not os.path.exists(td_phase1_save_ckpt_dir):\n",
    "    os.makedirs(td_phase1_save_ckpt_dir)\n",
    "if not os.path.exists(td_phase2_save_ckpt_dir):\n",
    "    os.makedirs(td_phase2_save_ckpt_dir)\n",
    "    \n",
    "enable_loss_scale = False\n",
    "set_seed(123)\n",
    "ds.config.set_seed(12345)\n",
    "dataset_type = DataType.TFRECORD\n",
    "cfg = phase1_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ce7c089-e357-40f1-a6f5-05647c9be704",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args_opt.do_train.lower() != \"true\" and args_opt.do_eval.lower() != \"true\":\n",
    "    raise ValueError(\"do train or do eval must have one be true, please confirm your config\")\n",
    "if args_opt.task_name in [\"SST-2\", \"QNLI\", \"MNLI\", \"TNEWS\"] and args_opt.task_type != \"classification\":\n",
    "    raise ValueError(f\"{args_opt.task_name} is a classification dataset, please set --task_type=classification\")\n",
    "if args_opt.task_name in [\"CLUENER\"] and args_opt.task_type != \"ner\":\n",
    "    raise ValueError(f\"{args_opt.task_name} is a ner dataset, please set --task_type=ner\")\n",
    "if args_opt.task_name in [\"SST-2\", \"QNLI\", \"MNLI\"] and \\\n",
    "        (td_teacher_net_cfg.vocab_size != 30522 or td_student_net_cfg.vocab_size != 30522):\n",
    "    logger.warning(f\"{args_opt.task_name} is an English dataset. Usually, we use 21128 for CN vocabs and 30522 for \"\n",
    "                   f\"EN vocabs according to the origin paper.\")\n",
    "if args_opt.task_name in [\"TNEWS\", \"CLUENER\"] and \\\n",
    "        (td_teacher_net_cfg.vocab_size != 21128 or td_student_net_cfg.vocab_size != 21128):\n",
    "    logger.warning(f\"{args_opt.task_name} is a Chinese dataset. Usually, we use 21128 for CN vocabs and 30522 for \"\n",
    "                   f\"EN vocabs according to the origin paper.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5937d26c-5e3a-4651-83a2-3e964e40fd11",
   "metadata": {},
   "source": [
    "## 第一阶段\n",
    "\n",
    "### 环境 数据集 与模型设置\n",
    "\n",
    "  与预训练蒸馏相似。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4d7d48b-7b3f-4f91-9b60-0e0831b34795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "td1 dataset size:  430\n",
      "td1 dataset repeatcount:  1\n"
     ]
    }
   ],
   "source": [
    "rank = 0\n",
    "device_num = 1\n",
    "\n",
    "dataset = create_tinybert_dataset('td', cfg.batch_size,\n",
    "                                  device_num, rank, args_opt.do_shuffle,\n",
    "                                  args_opt.train_data_dir, args_opt.schema_dir,\n",
    "                                  data_type=dataset_type)\n",
    "\n",
    "dataset_size = dataset.get_dataset_size()\n",
    "print('td1 dataset size: ', dataset_size)\n",
    "print('td1 dataset repeatcount: ', dataset.get_repeat_count())\n",
    "args_opt.data_sink_steps = dataset_size\n",
    "repeat_count = args_opt.td_phase1_epoch_size\n",
    "time_monitor_steps = dataset_size\n",
    "\n",
    "load_teacher_checkpoint_path = args_opt.load_teacher_ckpt_path\n",
    "load_student_checkpoint_path = args_opt.load_gd_ckpt_path\n",
    "netwithloss = BertNetworkWithLoss_td(teacher_config=td_teacher_net_cfg, teacher_ckpt=load_teacher_checkpoint_path,\n",
    "                                     student_config=td_student_net_cfg, student_ckpt=load_student_checkpoint_path,\n",
    "                                     is_training=True, task_type=args_opt.task_type,\n",
    "                                     num_labels=args_opt.num_labels, is_predistill=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4755e0de-bc70-4e3c-b85d-45953a1b9c4e",
   "metadata": {},
   "source": [
    "### 设置学习率与优化器\n",
    "\n",
    " ModelSaveCkpt 规定了保存的位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b30ae11-65a3-4698-8c42-132ded1dc7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_cfg = cfg.optimizer_cfg\n",
    "\n",
    "lr_schedule = BertLearningRate(learning_rate=optimizer_cfg.AdamWeightDecay.learning_rate,\n",
    "                               end_learning_rate=optimizer_cfg.AdamWeightDecay.end_learning_rate,\n",
    "                               warmup_steps=int(dataset_size / 10),\n",
    "                               decay_steps=int(dataset_size * args_opt.td_phase1_epoch_size),\n",
    "                               power=optimizer_cfg.AdamWeightDecay.power)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "params = netwithloss.trainable_params()\n",
    "decay_params = list(filter(optimizer_cfg.AdamWeightDecay.decay_filter, params))\n",
    "other_params = list(filter(lambda x: not optimizer_cfg.AdamWeightDecay.decay_filter(x), params))\n",
    "group_params = [{'params': decay_params, 'weight_decay': optimizer_cfg.AdamWeightDecay.weight_decay},\n",
    "                {'params': other_params, 'weight_decay': 0.0},\n",
    "                {'order_params': params}]\n",
    "\n",
    "optimizer = AdamWeightDecay(group_params, learning_rate=lr_schedule, eps=optimizer_cfg.AdamWeightDecay.eps)\n",
    "\n",
    "callback = [TimeMonitor(time_monitor_steps), LossCallBack(), ModelSaveCkpt(netwithloss.bert,\n",
    "                                                                           dataset_size,\n",
    "                                                                           args_opt.max_ckpt_num,\n",
    "                                                                           td_phase1_save_ckpt_dir)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57268e6a-fb78-432b-8048-56e973005647",
   "metadata": {},
   "source": [
    "### 载入梯度模型并训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92d74c8-528e-4b34-927b-87d0e1293cf4",
   "metadata": {},
   "source": [
    "## 二阶段\n",
    "\n",
    " 二阶段我们载入一阶段保存的模型， 调整超参，开始新的训练，其他与一阶段相似。\n",
    " \n",
    " \n",
    " ### 载入最新一阶段模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0437b1a-92cf-43bd-b767-4cefd697e628",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(3410:281472849991552,MainProcess):2022-11-11-15:22:57.702.937 [mindspore/train/model.py:1077] For LossCallBack callback, {'step_end'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.\n",
      "[WARNING] ME(3410:281472849991552,MainProcess):2022-11-11-15:22:57.704.769 [mindspore/train/model.py:1077] For ModelSaveCkpt callback, {'step_end'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:23:01.723.721 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op5054] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:23:02.352.494 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op5105] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:23:02.572.711 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op5123] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:23:02.914.976 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op5147] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:23:03.213.830 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op5173] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:23:03.345.730 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op5191] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:23:03.564.711 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op5209] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:23:03.907.467 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op5241] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:23:04.042.002 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op5259] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:23:04.261.779 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op5277] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:23:04.607.121 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op5309] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:23:04.732.086 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op5327] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:23:04.941.666 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op5345] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:23:13.230.410 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/gradStridedSlice/StridedSliceGrad-op5960] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:23:13.558.842 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/gradStridedSlice/StridedSliceGrad-op6004] don't support int64, reduce precision from int64 to int32.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, step: 1, outputs are 5.3099165\n",
      "epoch: 1, step: 2, outputs are 5.2864685\n",
      "epoch: 1, step: 3, outputs are 5.2277293\n",
      "epoch: 1, step: 4, outputs are 5.124625\n",
      "epoch: 1, step: 5, outputs are 4.986482\n",
      "epoch: 1, step: 6, outputs are 4.8264894\n",
      "epoch: 1, step: 7, outputs are 4.673621\n",
      "epoch: 1, step: 8, outputs are 4.5482984\n",
      "epoch: 1, step: 9, outputs are 4.419226\n",
      "epoch: 1, step: 10, outputs are 4.2848015\n",
      "epoch: 1, step: 11, outputs are 4.155889\n",
      "epoch: 1, step: 12, outputs are 3.980112\n",
      "epoch: 1, step: 13, outputs are 3.8117995\n",
      "epoch: 1, step: 14, outputs are 3.6463187\n",
      "epoch: 1, step: 15, outputs are 3.4350278\n",
      "epoch: 1, step: 16, outputs are 3.2348647\n",
      "epoch: 1, step: 17, outputs are 3.0107646\n",
      "epoch: 1, step: 18, outputs are 2.8045046\n",
      "epoch: 1, step: 19, outputs are 2.5675924\n",
      "epoch: 1, step: 20, outputs are 2.3727136\n",
      "epoch: 1, step: 21, outputs are 2.1526787\n",
      "epoch: 1, step: 22, outputs are 1.9550903\n",
      "epoch: 1, step: 23, outputs are 1.7814268\n",
      "epoch: 1, step: 24, outputs are 1.6107996\n",
      "epoch: 1, step: 25, outputs are 1.4658337\n",
      "epoch: 1, step: 26, outputs are 1.3377618\n",
      "epoch: 1, step: 27, outputs are 1.2318105\n",
      "epoch: 1, step: 28, outputs are 1.131986\n",
      "epoch: 1, step: 29, outputs are 1.034252\n",
      "epoch: 1, step: 30, outputs are 0.9750411\n",
      "epoch: 1, step: 31, outputs are 0.9189178\n",
      "epoch: 1, step: 32, outputs are 0.88780427\n",
      "epoch: 1, step: 33, outputs are 0.8630854\n",
      "epoch: 1, step: 34, outputs are 0.845642\n",
      "epoch: 1, step: 35, outputs are 0.84255517\n",
      "epoch: 1, step: 36, outputs are 0.82481825\n",
      "epoch: 1, step: 37, outputs are 0.8260727\n",
      "epoch: 1, step: 38, outputs are 0.818282\n",
      "epoch: 1, step: 39, outputs are 0.79686314\n",
      "epoch: 1, step: 40, outputs are 0.77004635\n",
      "epoch: 1, step: 41, outputs are 0.7457487\n",
      "epoch: 1, step: 42, outputs are 0.7265163\n",
      "epoch: 1, step: 43, outputs are 0.7010766\n",
      "epoch: 1, step: 44, outputs are 0.6686662\n",
      "epoch: 1, step: 45, outputs are 0.64610124\n",
      "epoch: 1, step: 46, outputs are 0.6187373\n",
      "epoch: 1, step: 47, outputs are 0.60510075\n",
      "epoch: 1, step: 48, outputs are 0.586743\n",
      "epoch: 1, step: 49, outputs are 0.57984364\n",
      "epoch: 1, step: 50, outputs are 0.5746713\n",
      "epoch: 1, step: 51, outputs are 0.5527025\n",
      "epoch: 1, step: 52, outputs are 0.55968416\n",
      "epoch: 1, step: 53, outputs are 0.5441624\n",
      "epoch: 1, step: 54, outputs are 0.5365182\n",
      "epoch: 1, step: 55, outputs are 0.53112715\n",
      "epoch: 1, step: 56, outputs are 0.519372\n",
      "epoch: 1, step: 57, outputs are 0.51622367\n",
      "epoch: 1, step: 58, outputs are 0.5057848\n",
      "epoch: 1, step: 59, outputs are 0.5089898\n",
      "epoch: 1, step: 60, outputs are 0.4891591\n",
      "epoch: 1, step: 61, outputs are 0.48592484\n",
      "epoch: 1, step: 62, outputs are 0.4804929\n",
      "epoch: 1, step: 63, outputs are 0.48503363\n",
      "epoch: 1, step: 64, outputs are 0.47732946\n",
      "epoch: 1, step: 65, outputs are 0.47300836\n",
      "epoch: 1, step: 66, outputs are 0.46825653\n",
      "epoch: 1, step: 67, outputs are 0.46525776\n",
      "epoch: 1, step: 68, outputs are 0.45427898\n",
      "epoch: 1, step: 69, outputs are 0.4485423\n",
      "epoch: 1, step: 70, outputs are 0.4562286\n",
      "epoch: 1, step: 71, outputs are 0.44589755\n",
      "epoch: 1, step: 72, outputs are 0.43944412\n",
      "epoch: 1, step: 73, outputs are 0.44246593\n",
      "epoch: 1, step: 74, outputs are 0.4410622\n",
      "epoch: 1, step: 75, outputs are 0.43566886\n",
      "epoch: 1, step: 76, outputs are 0.43407965\n",
      "epoch: 1, step: 77, outputs are 0.43072525\n",
      "epoch: 1, step: 78, outputs are 0.42001918\n",
      "epoch: 1, step: 79, outputs are 0.41642767\n",
      "epoch: 1, step: 80, outputs are 0.42125836\n",
      "epoch: 1, step: 81, outputs are 0.40981328\n",
      "epoch: 1, step: 82, outputs are 0.41740522\n",
      "epoch: 1, step: 83, outputs are 0.41161495\n",
      "epoch: 1, step: 84, outputs are 0.41028246\n",
      "epoch: 1, step: 85, outputs are 0.40905124\n",
      "epoch: 1, step: 86, outputs are 0.41155556\n",
      "epoch: 1, step: 87, outputs are 0.4033391\n",
      "epoch: 1, step: 88, outputs are 0.3942395\n",
      "epoch: 1, step: 89, outputs are 0.40160146\n",
      "epoch: 1, step: 90, outputs are 0.39987618\n",
      "epoch: 1, step: 91, outputs are 0.3995743\n",
      "epoch: 1, step: 92, outputs are 0.39566666\n",
      "epoch: 1, step: 93, outputs are 0.395804\n",
      "epoch: 1, step: 94, outputs are 0.39669734\n",
      "epoch: 1, step: 95, outputs are 0.39420456\n",
      "epoch: 1, step: 96, outputs are 0.39423943\n",
      "epoch: 1, step: 97, outputs are 0.38895187\n",
      "epoch: 1, step: 98, outputs are 0.39244083\n",
      "epoch: 1, step: 99, outputs are 0.38634023\n",
      "epoch: 1, step: 100, outputs are 0.3876167\n",
      "epoch: 1, step: 101, outputs are 0.38482696\n",
      "epoch: 1, step: 102, outputs are 0.37657022\n",
      "epoch: 1, step: 103, outputs are 0.38247907\n",
      "epoch: 1, step: 104, outputs are 0.38021973\n",
      "epoch: 1, step: 105, outputs are 0.38412213\n",
      "epoch: 1, step: 106, outputs are 0.38580373\n",
      "epoch: 1, step: 107, outputs are 0.37348247\n",
      "epoch: 1, step: 108, outputs are 0.3786263\n",
      "epoch: 1, step: 109, outputs are 0.37628138\n",
      "epoch: 1, step: 110, outputs are 0.3706149\n",
      "epoch: 1, step: 111, outputs are 0.37230283\n",
      "epoch: 1, step: 112, outputs are 0.3707284\n",
      "epoch: 1, step: 113, outputs are 0.36537027\n",
      "epoch: 1, step: 114, outputs are 0.37683973\n",
      "epoch: 1, step: 115, outputs are 0.37742624\n",
      "epoch: 1, step: 116, outputs are 0.37581444\n",
      "epoch: 1, step: 117, outputs are 0.37250966\n",
      "epoch: 1, step: 118, outputs are 0.3738724\n",
      "epoch: 1, step: 119, outputs are 0.37386972\n",
      "epoch: 1, step: 120, outputs are 0.37131023\n",
      "epoch: 1, step: 121, outputs are 0.37141293\n",
      "epoch: 1, step: 122, outputs are 0.37176508\n",
      "epoch: 1, step: 123, outputs are 0.3568074\n",
      "epoch: 1, step: 124, outputs are 0.36998546\n",
      "epoch: 1, step: 125, outputs are 0.36415014\n",
      "epoch: 1, step: 126, outputs are 0.37466407\n",
      "epoch: 1, step: 127, outputs are 0.36759955\n",
      "epoch: 1, step: 128, outputs are 0.3638973\n",
      "epoch: 1, step: 129, outputs are 0.36263016\n",
      "epoch: 1, step: 130, outputs are 0.3659425\n",
      "epoch: 1, step: 131, outputs are 0.36105904\n",
      "epoch: 1, step: 132, outputs are 0.36880064\n",
      "epoch: 1, step: 133, outputs are 0.369115\n",
      "epoch: 1, step: 134, outputs are 0.35573965\n",
      "epoch: 1, step: 135, outputs are 0.35829264\n",
      "epoch: 1, step: 136, outputs are 0.3669439\n",
      "epoch: 1, step: 137, outputs are 0.36147273\n",
      "epoch: 1, step: 138, outputs are 0.3578693\n",
      "epoch: 1, step: 139, outputs are 0.35856593\n",
      "epoch: 1, step: 140, outputs are 0.36054772\n",
      "epoch: 1, step: 141, outputs are 0.3609534\n",
      "epoch: 1, step: 142, outputs are 0.36317074\n",
      "epoch: 1, step: 143, outputs are 0.34904146\n",
      "epoch: 1, step: 144, outputs are 0.35863417\n",
      "epoch: 1, step: 145, outputs are 0.36128056\n",
      "epoch: 1, step: 146, outputs are 0.35997906\n",
      "epoch: 1, step: 147, outputs are 0.35636455\n",
      "epoch: 1, step: 148, outputs are 0.3543659\n",
      "epoch: 1, step: 149, outputs are 0.36780453\n",
      "epoch: 1, step: 150, outputs are 0.35929984\n",
      "epoch: 1, step: 151, outputs are 0.35844767\n",
      "epoch: 1, step: 152, outputs are 0.34960058\n",
      "epoch: 1, step: 153, outputs are 0.35129905\n",
      "epoch: 1, step: 154, outputs are 0.3545975\n",
      "epoch: 1, step: 155, outputs are 0.35312468\n",
      "epoch: 1, step: 156, outputs are 0.35356325\n",
      "epoch: 1, step: 157, outputs are 0.35150272\n",
      "epoch: 1, step: 158, outputs are 0.3542626\n",
      "epoch: 1, step: 159, outputs are 0.33862972\n",
      "epoch: 1, step: 160, outputs are 0.3537689\n",
      "epoch: 1, step: 161, outputs are 0.34979638\n",
      "epoch: 1, step: 162, outputs are 0.34748214\n",
      "epoch: 1, step: 163, outputs are 0.3509595\n",
      "epoch: 1, step: 164, outputs are 0.35289973\n",
      "epoch: 1, step: 165, outputs are 0.34823877\n",
      "epoch: 1, step: 166, outputs are 0.3490659\n",
      "epoch: 1, step: 167, outputs are 0.34977627\n",
      "epoch: 1, step: 168, outputs are 0.35194284\n",
      "epoch: 1, step: 169, outputs are 0.3463987\n",
      "epoch: 1, step: 170, outputs are 0.3468364\n",
      "epoch: 1, step: 171, outputs are 0.35444778\n",
      "epoch: 1, step: 172, outputs are 0.34761292\n",
      "epoch: 1, step: 173, outputs are 0.34860235\n",
      "epoch: 1, step: 174, outputs are 0.34615225\n",
      "epoch: 1, step: 175, outputs are 0.35271\n",
      "epoch: 1, step: 176, outputs are 0.35598245\n",
      "epoch: 1, step: 177, outputs are 0.3383569\n",
      "epoch: 1, step: 178, outputs are 0.3478924\n",
      "epoch: 1, step: 179, outputs are 0.341859\n",
      "epoch: 1, step: 180, outputs are 0.34085655\n",
      "epoch: 1, step: 181, outputs are 0.3411411\n",
      "epoch: 1, step: 182, outputs are 0.35020733\n",
      "epoch: 1, step: 183, outputs are 0.33840528\n",
      "epoch: 1, step: 184, outputs are 0.34069222\n",
      "epoch: 1, step: 185, outputs are 0.33410475\n",
      "epoch: 1, step: 186, outputs are 0.3402382\n",
      "epoch: 1, step: 187, outputs are 0.34860417\n",
      "epoch: 1, step: 188, outputs are 0.3425049\n",
      "epoch: 1, step: 189, outputs are 0.34574002\n",
      "epoch: 1, step: 190, outputs are 0.33933002\n",
      "epoch: 1, step: 191, outputs are 0.3363319\n",
      "epoch: 1, step: 192, outputs are 0.343312\n",
      "epoch: 1, step: 193, outputs are 0.33610785\n",
      "epoch: 1, step: 194, outputs are 0.34274882\n",
      "epoch: 1, step: 195, outputs are 0.345533\n",
      "epoch: 1, step: 196, outputs are 0.34865668\n",
      "epoch: 1, step: 197, outputs are 0.3373497\n",
      "epoch: 1, step: 198, outputs are 0.33917087\n",
      "epoch: 1, step: 199, outputs are 0.34202948\n",
      "epoch: 1, step: 200, outputs are 0.33194828\n",
      "epoch: 1, step: 201, outputs are 0.34746248\n",
      "epoch: 1, step: 202, outputs are 0.33656168\n",
      "epoch: 1, step: 203, outputs are 0.3424496\n",
      "epoch: 1, step: 204, outputs are 0.34230983\n",
      "epoch: 1, step: 205, outputs are 0.3402179\n",
      "epoch: 1, step: 206, outputs are 0.3394838\n",
      "epoch: 1, step: 207, outputs are 0.33493745\n",
      "epoch: 1, step: 208, outputs are 0.33624715\n",
      "epoch: 1, step: 209, outputs are 0.3484363\n",
      "epoch: 1, step: 210, outputs are 0.33952785\n",
      "epoch: 1, step: 211, outputs are 0.34297758\n",
      "epoch: 1, step: 212, outputs are 0.3415417\n",
      "epoch: 1, step: 213, outputs are 0.3354882\n",
      "epoch: 1, step: 214, outputs are 0.34837797\n",
      "epoch: 1, step: 215, outputs are 0.33892775\n",
      "epoch: 1, step: 216, outputs are 0.33209997\n",
      "epoch: 1, step: 217, outputs are 0.33533418\n",
      "epoch: 1, step: 218, outputs are 0.33482257\n",
      "epoch: 1, step: 219, outputs are 0.33749914\n",
      "epoch: 1, step: 220, outputs are 0.33193967\n",
      "epoch: 1, step: 221, outputs are 0.3311553\n",
      "epoch: 1, step: 222, outputs are 0.34020114\n",
      "epoch: 1, step: 223, outputs are 0.34286577\n",
      "epoch: 1, step: 224, outputs are 0.33793405\n",
      "epoch: 1, step: 225, outputs are 0.33619675\n",
      "epoch: 1, step: 226, outputs are 0.33410573\n",
      "epoch: 1, step: 227, outputs are 0.33194265\n",
      "epoch: 1, step: 228, outputs are 0.33766484\n",
      "epoch: 1, step: 229, outputs are 0.3435635\n",
      "epoch: 1, step: 230, outputs are 0.3320282\n",
      "epoch: 1, step: 231, outputs are 0.33351701\n",
      "epoch: 1, step: 232, outputs are 0.34352517\n",
      "epoch: 1, step: 233, outputs are 0.33445275\n",
      "epoch: 1, step: 234, outputs are 0.33740857\n",
      "epoch: 1, step: 235, outputs are 0.3373844\n",
      "epoch: 1, step: 236, outputs are 0.33734515\n",
      "epoch: 1, step: 237, outputs are 0.33181876\n",
      "epoch: 1, step: 238, outputs are 0.33226815\n",
      "epoch: 1, step: 239, outputs are 0.33287904\n",
      "epoch: 1, step: 240, outputs are 0.33760613\n",
      "epoch: 1, step: 241, outputs are 0.33595693\n",
      "epoch: 1, step: 242, outputs are 0.33285552\n",
      "epoch: 1, step: 243, outputs are 0.33125973\n",
      "epoch: 1, step: 244, outputs are 0.3365236\n",
      "epoch: 1, step: 245, outputs are 0.33813027\n",
      "epoch: 1, step: 246, outputs are 0.32479113\n",
      "epoch: 1, step: 247, outputs are 0.33493942\n",
      "epoch: 1, step: 248, outputs are 0.3286016\n",
      "epoch: 1, step: 249, outputs are 0.34415746\n",
      "epoch: 1, step: 250, outputs are 0.32862514\n",
      "epoch: 1, step: 251, outputs are 0.34392995\n",
      "epoch: 1, step: 252, outputs are 0.33171034\n",
      "epoch: 1, step: 253, outputs are 0.33323324\n",
      "epoch: 1, step: 254, outputs are 0.32693458\n",
      "epoch: 1, step: 255, outputs are 0.3376584\n",
      "epoch: 1, step: 256, outputs are 0.3313427\n",
      "epoch: 1, step: 257, outputs are 0.33257556\n",
      "epoch: 1, step: 258, outputs are 0.32864463\n",
      "epoch: 1, step: 259, outputs are 0.3341603\n",
      "epoch: 1, step: 260, outputs are 0.3365425\n",
      "epoch: 1, step: 261, outputs are 0.33075327\n",
      "epoch: 1, step: 262, outputs are 0.3359521\n",
      "epoch: 1, step: 263, outputs are 0.33049548\n",
      "epoch: 1, step: 264, outputs are 0.3311049\n",
      "epoch: 1, step: 265, outputs are 0.32971928\n",
      "epoch: 1, step: 266, outputs are 0.33218458\n",
      "epoch: 1, step: 267, outputs are 0.32901692\n",
      "epoch: 1, step: 268, outputs are 0.3365386\n",
      "epoch: 1, step: 269, outputs are 0.33294654\n",
      "epoch: 1, step: 270, outputs are 0.32255036\n",
      "epoch: 1, step: 271, outputs are 0.3297691\n",
      "epoch: 1, step: 272, outputs are 0.33235702\n",
      "epoch: 1, step: 273, outputs are 0.32714868\n",
      "epoch: 1, step: 274, outputs are 0.32961938\n",
      "epoch: 1, step: 275, outputs are 0.32872057\n",
      "epoch: 1, step: 276, outputs are 0.338396\n",
      "epoch: 1, step: 277, outputs are 0.33702946\n",
      "epoch: 1, step: 278, outputs are 0.32940018\n",
      "epoch: 1, step: 279, outputs are 0.33179417\n",
      "epoch: 1, step: 280, outputs are 0.3214193\n",
      "epoch: 1, step: 281, outputs are 0.3320679\n",
      "epoch: 1, step: 282, outputs are 0.32876563\n",
      "epoch: 1, step: 283, outputs are 0.32590085\n",
      "epoch: 1, step: 284, outputs are 0.32949868\n",
      "epoch: 1, step: 285, outputs are 0.33503336\n",
      "epoch: 1, step: 286, outputs are 0.32759535\n",
      "epoch: 1, step: 287, outputs are 0.32947698\n",
      "epoch: 1, step: 288, outputs are 0.33172417\n",
      "epoch: 1, step: 289, outputs are 0.33267617\n",
      "epoch: 1, step: 290, outputs are 0.32882923\n",
      "epoch: 1, step: 291, outputs are 0.329791\n",
      "epoch: 1, step: 292, outputs are 0.33363503\n",
      "epoch: 1, step: 293, outputs are 0.32923335\n",
      "epoch: 1, step: 294, outputs are 0.3279125\n",
      "epoch: 1, step: 295, outputs are 0.33638322\n",
      "epoch: 1, step: 296, outputs are 0.33940434\n",
      "epoch: 1, step: 297, outputs are 0.32634723\n",
      "epoch: 1, step: 298, outputs are 0.3250993\n",
      "epoch: 1, step: 299, outputs are 0.32729083\n",
      "epoch: 1, step: 300, outputs are 0.32610574\n",
      "epoch: 1, step: 301, outputs are 0.33783513\n",
      "epoch: 1, step: 302, outputs are 0.33812568\n",
      "epoch: 1, step: 303, outputs are 0.32438266\n",
      "epoch: 1, step: 304, outputs are 0.32678592\n",
      "epoch: 1, step: 305, outputs are 0.33081287\n",
      "epoch: 1, step: 306, outputs are 0.32887554\n",
      "epoch: 1, step: 307, outputs are 0.33051637\n",
      "epoch: 1, step: 308, outputs are 0.3278262\n",
      "epoch: 1, step: 309, outputs are 0.33188075\n",
      "epoch: 1, step: 310, outputs are 0.32825774\n",
      "epoch: 1, step: 311, outputs are 0.3326324\n",
      "epoch: 1, step: 312, outputs are 0.33306006\n",
      "epoch: 1, step: 313, outputs are 0.329503\n",
      "epoch: 1, step: 314, outputs are 0.3263328\n",
      "epoch: 1, step: 315, outputs are 0.32718286\n",
      "epoch: 1, step: 316, outputs are 0.33003882\n",
      "epoch: 1, step: 317, outputs are 0.33147353\n",
      "epoch: 1, step: 318, outputs are 0.3286609\n",
      "epoch: 1, step: 319, outputs are 0.32712513\n",
      "epoch: 1, step: 320, outputs are 0.33237594\n",
      "epoch: 1, step: 321, outputs are 0.32891083\n",
      "epoch: 1, step: 322, outputs are 0.33183005\n",
      "epoch: 1, step: 323, outputs are 0.32757545\n",
      "epoch: 1, step: 324, outputs are 0.32363355\n",
      "epoch: 1, step: 325, outputs are 0.32964504\n",
      "epoch: 1, step: 326, outputs are 0.32591128\n",
      "epoch: 1, step: 327, outputs are 0.32517734\n",
      "epoch: 1, step: 328, outputs are 0.33250567\n",
      "epoch: 1, step: 329, outputs are 0.32516798\n",
      "epoch: 1, step: 330, outputs are 0.33190006\n",
      "epoch: 1, step: 331, outputs are 0.32779485\n",
      "epoch: 1, step: 332, outputs are 0.32864362\n",
      "epoch: 1, step: 333, outputs are 0.33007056\n",
      "epoch: 1, step: 334, outputs are 0.32788014\n",
      "epoch: 1, step: 335, outputs are 0.32762843\n",
      "epoch: 1, step: 336, outputs are 0.32531297\n",
      "epoch: 1, step: 337, outputs are 0.32596016\n",
      "epoch: 1, step: 338, outputs are 0.32511914\n",
      "epoch: 1, step: 339, outputs are 0.33042175\n",
      "epoch: 1, step: 340, outputs are 0.3253445\n",
      "epoch: 1, step: 341, outputs are 0.32833183\n",
      "epoch: 1, step: 342, outputs are 0.32783613\n",
      "epoch: 1, step: 343, outputs are 0.32562235\n",
      "epoch: 1, step: 344, outputs are 0.32896668\n",
      "epoch: 1, step: 345, outputs are 0.32961273\n",
      "epoch: 1, step: 346, outputs are 0.32612985\n",
      "epoch: 1, step: 347, outputs are 0.3266373\n",
      "epoch: 1, step: 348, outputs are 0.3331963\n",
      "epoch: 1, step: 349, outputs are 0.3265705\n",
      "epoch: 1, step: 350, outputs are 0.32570016\n",
      "epoch: 1, step: 351, outputs are 0.32986763\n",
      "epoch: 1, step: 352, outputs are 0.32296425\n",
      "epoch: 1, step: 353, outputs are 0.32571578\n",
      "epoch: 1, step: 354, outputs are 0.33604544\n",
      "epoch: 1, step: 355, outputs are 0.3183409\n",
      "epoch: 1, step: 356, outputs are 0.32459196\n",
      "epoch: 1, step: 357, outputs are 0.33229828\n",
      "epoch: 1, step: 358, outputs are 0.33350638\n",
      "epoch: 1, step: 359, outputs are 0.32732782\n",
      "epoch: 1, step: 360, outputs are 0.32723752\n",
      "epoch: 1, step: 361, outputs are 0.3247339\n",
      "epoch: 1, step: 362, outputs are 0.3229139\n",
      "epoch: 1, step: 363, outputs are 0.3276861\n",
      "epoch: 1, step: 364, outputs are 0.32884273\n",
      "epoch: 1, step: 365, outputs are 0.32598642\n",
      "epoch: 1, step: 366, outputs are 0.3274628\n",
      "epoch: 1, step: 367, outputs are 0.3310546\n",
      "epoch: 1, step: 368, outputs are 0.33040857\n",
      "epoch: 1, step: 369, outputs are 0.33169475\n",
      "epoch: 1, step: 370, outputs are 0.32576126\n",
      "epoch: 1, step: 371, outputs are 0.32763645\n",
      "epoch: 1, step: 372, outputs are 0.32268393\n",
      "epoch: 1, step: 373, outputs are 0.3274786\n",
      "epoch: 1, step: 374, outputs are 0.32154167\n",
      "epoch: 1, step: 375, outputs are 0.32422617\n",
      "epoch: 1, step: 376, outputs are 0.3272122\n",
      "epoch: 1, step: 377, outputs are 0.3284487\n",
      "epoch: 1, step: 378, outputs are 0.32948312\n",
      "epoch: 1, step: 379, outputs are 0.32567438\n",
      "epoch: 1, step: 380, outputs are 0.33083844\n",
      "epoch: 1, step: 381, outputs are 0.3244394\n",
      "epoch: 1, step: 382, outputs are 0.32996503\n",
      "epoch: 1, step: 383, outputs are 0.32492578\n",
      "epoch: 1, step: 384, outputs are 0.32699737\n",
      "epoch: 1, step: 385, outputs are 0.32992142\n",
      "epoch: 1, step: 386, outputs are 0.3280374\n",
      "epoch: 1, step: 387, outputs are 0.3284896\n",
      "epoch: 1, step: 388, outputs are 0.32806602\n",
      "epoch: 1, step: 389, outputs are 0.32791454\n",
      "epoch: 1, step: 390, outputs are 0.32544774\n",
      "epoch: 1, step: 391, outputs are 0.32666865\n",
      "epoch: 1, step: 392, outputs are 0.32565105\n",
      "epoch: 1, step: 393, outputs are 0.32828268\n",
      "epoch: 1, step: 394, outputs are 0.33503965\n",
      "epoch: 1, step: 395, outputs are 0.3238942\n",
      "epoch: 1, step: 396, outputs are 0.32204318\n",
      "epoch: 1, step: 397, outputs are 0.32667646\n",
      "epoch: 1, step: 398, outputs are 0.33702046\n",
      "epoch: 1, step: 399, outputs are 0.32485318\n",
      "epoch: 1, step: 400, outputs are 0.33132172\n",
      "epoch: 1, step: 401, outputs are 0.32308275\n",
      "epoch: 1, step: 402, outputs are 0.32221597\n",
      "epoch: 1, step: 403, outputs are 0.32596022\n",
      "epoch: 1, step: 404, outputs are 0.32637382\n",
      "epoch: 1, step: 405, outputs are 0.32360485\n",
      "epoch: 1, step: 406, outputs are 0.32215136\n",
      "epoch: 1, step: 407, outputs are 0.32843328\n",
      "epoch: 1, step: 408, outputs are 0.3255957\n",
      "epoch: 1, step: 409, outputs are 0.32111344\n",
      "epoch: 1, step: 410, outputs are 0.33227587\n",
      "epoch: 1, step: 411, outputs are 0.32929766\n",
      "epoch: 1, step: 412, outputs are 0.32903433\n",
      "epoch: 1, step: 413, outputs are 0.32027465\n",
      "epoch: 1, step: 414, outputs are 0.32627797\n",
      "epoch: 1, step: 415, outputs are 0.32795504\n",
      "epoch: 1, step: 416, outputs are 0.32837918\n",
      "epoch: 1, step: 417, outputs are 0.31924784\n",
      "epoch: 1, step: 418, outputs are 0.32530904\n",
      "epoch: 1, step: 419, outputs are 0.3294439\n",
      "epoch: 1, step: 420, outputs are 0.3252955\n",
      "epoch: 1, step: 421, outputs are 0.31554157\n",
      "epoch: 1, step: 422, outputs are 0.32628497\n",
      "epoch: 1, step: 423, outputs are 0.32642525\n",
      "epoch: 1, step: 424, outputs are 0.32905793\n",
      "epoch: 1, step: 425, outputs are 0.329034\n",
      "epoch: 1, step: 426, outputs are 0.31845576\n",
      "epoch: 1, step: 427, outputs are 0.32449943\n",
      "epoch: 1, step: 428, outputs are 0.32430238\n",
      "epoch: 1, step: 429, outputs are 0.3258839\n",
      "epoch: 1, step: 430, outputs are 0.3271584\n",
      "Train epoch time: 1263933.962 ms, per step time: 2939.381 ms\n"
     ]
    }
   ],
   "source": [
    "netwithgrads = BertEvaluationCell(netwithloss, optimizer=optimizer)\n",
    "\n",
    "model = Model(netwithgrads)\n",
    "model.train(repeat_count, dataset, callbacks=callback,\n",
    "            dataset_sink_mode=(args_opt.enable_data_sink == 'true'),\n",
    "            sink_size=args_opt.data_sink_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c156681f-2e1e-4bb4-b02a-bccd8d7fa87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = os.listdir(td_phase1_save_ckpt_dir)\n",
    "if lists:\n",
    "    lists.sort(key=lambda fn: os.path.getmtime(td_phase1_save_ckpt_dir + '/' + fn))\n",
    "    name_ext = os.path.splitext(lists[-1])\n",
    "    assert name_ext[-1] == \".ckpt\", \"Invalid file, checkpoint file should be .ckpt file\"\n",
    "    ckpt_file = os.path.join(td_phase1_save_ckpt_dir, lists[-1])\n",
    "    if ckpt_file == '':\n",
    "        raise ValueError(\"Student ckpt file should not be None\")\n",
    "    cfg = phase2_cfg\n",
    "    \n",
    "    \n",
    "load_teacher_checkpoint_path = args_opt.load_teacher_ckpt_path\n",
    "load_student_checkpoint_path = ckpt_file\n",
    "netwithloss = BertNetworkWithLoss_td(teacher_config=td_teacher_net_cfg, teacher_ckpt=load_teacher_checkpoint_path,\n",
    "                                     student_config=td_student_net_cfg, student_ckpt=load_student_checkpoint_path,\n",
    "                                     is_training=True, task_type=args_opt.task_type,\n",
    "                                     num_labels=args_opt.num_labels, is_predistill=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfc5d18-6580-4a6e-bf8d-84c35e6b7735",
   "metadata": {},
   "source": [
    "### 重新读入数据集\n",
    "\n",
    "在二阶段 我们需要对模型的结果进行验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a66ca05-59be-42f0-9716-92c8d922c92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "td2 train dataset size:  1721\n",
      "td2 train dataset repeatcount:  1\n",
      "td2 eval dataset size:  3443\n"
     ]
    }
   ],
   "source": [
    "rank = 0\n",
    "device_num = 1\n",
    "train_dataset = create_tinybert_dataset('td', cfg.batch_size,\n",
    "                                        device_num, rank, args_opt.do_shuffle,\n",
    "                                        args_opt.train_data_dir, args_opt.schema_dir,\n",
    "                                        data_type=dataset_type)\n",
    "\n",
    "dataset_size = train_dataset.get_dataset_size()\n",
    "print('td2 train dataset size: ', dataset_size)\n",
    "print('td2 train dataset repeatcount: ', train_dataset.get_repeat_count())\n",
    "\n",
    "repeat_count = args_opt.td_phase2_epoch_size\n",
    "\n",
    "time_monitor_steps = dataset_size\n",
    "\n",
    "eval_dataset = create_tinybert_dataset('td', eval_cfg.batch_size,\n",
    "                                       device_num, rank, args_opt.do_shuffle,\n",
    "                                       args_opt.eval_data_dir, args_opt.schema_dir,\n",
    "                                       data_type=dataset_type)\n",
    "print('td2 eval dataset size: ', eval_dataset.get_dataset_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15be077b-9e00-46de-9e77-a9ecd9c8456c",
   "metadata": {},
   "source": [
    "### 二阶段学习率与优化器  \n",
    "\n",
    "注意在callback中传入了测试集。\n",
    "在EvalCallBack 中规定了模型的保存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8791cfc-e122-467f-9b91-7ad15a035b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_cfg = cfg.optimizer_cfg\n",
    "\n",
    "lr_schedule = BertLearningRate(learning_rate=optimizer_cfg.AdamWeightDecay.learning_rate,\n",
    "                               end_learning_rate=optimizer_cfg.AdamWeightDecay.end_learning_rate,\n",
    "                               warmup_steps=int(dataset_size * args_opt.td_phase2_epoch_size / 10),\n",
    "                               decay_steps=int(dataset_size * args_opt.td_phase2_epoch_size),\n",
    "                               power=optimizer_cfg.AdamWeightDecay.power)\n",
    "\n",
    "params = netwithloss.trainable_params()\n",
    "decay_params = list(filter(optimizer_cfg.AdamWeightDecay.decay_filter, params))\n",
    "other_params = list(filter(lambda x: not optimizer_cfg.AdamWeightDecay.decay_filter(x), params))\n",
    "group_params = [{'params': decay_params, 'weight_decay': optimizer_cfg.AdamWeightDecay.weight_decay},\n",
    "                {'params': other_params, 'weight_decay': 0.0},\n",
    "                {'order_params': params}]\n",
    "\n",
    "optimizer = AdamWeightDecay(group_params, learning_rate=lr_schedule, eps=optimizer_cfg.AdamWeightDecay.eps)\n",
    "\n",
    "callback = [TimeMonitor(time_monitor_steps), LossCallBack(),\n",
    "            EvalCallBack(netwithloss.bert, eval_dataset)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f1d749-fc5e-4fe2-a037-74309611a142",
   "metadata": {},
   "source": [
    "### 训练\n",
    "\n",
    "注意 这里训练时， 如果没有屏蔽一阶段的训练，或者一阶段中途停止。 可能会出现报错:C++ Call Stack。 如果出现报错，可以先屏蔽一阶段训练的代码段。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4baca7b-9cdb-481e-aa0f-b3201b9285f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(3410:281472849991552,MainProcess):2022-11-11-15:44:20.698.010 [mindspore/train/model.py:1077] For LossCallBack callback, {'step_end'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.\n",
      "[WARNING] ME(3410:281472849991552,MainProcess):2022-11-11-15:44:20.699.335 [mindspore/train/model.py:1077] For EvalCallBack callback, {'epoch_end'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:44:24.763.042 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op19056] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:44:26.631.967 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op19107] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:44:26.821.165 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op19125] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:44:27.111.481 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op19149] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:44:27.383.410 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op19175] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:44:27.501.873 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op19193] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:44:27.696.268 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op19211] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:44:28.006.997 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op19243] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:44:28.125.784 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op19261] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:44:28.320.601 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op19279] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:44:28.648.957 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op19311] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:44:28.771.930 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op19329] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:44:28.974.768 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op19347] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] OPTIMIZER(3410,ffff813cd780,python):2022-11-11-15:44:30.263.861 [mindspore/ccsrc/frontend/optimizer/ad/kpynative.cc:1088] SetOutput] Weight is not used in network, weight: fit_dense.weight\n",
      "[WARNING] OPTIMIZER(3410,ffff813cd780,python):2022-11-11-15:44:30.266.509 [mindspore/ccsrc/frontend/optimizer/ad/kpynative.cc:1088] SetOutput] Weight is not used in network, weight: fit_dense.bias\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:44:33.118.388 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/gradStridedSlice/StridedSliceGrad-op19701] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:44:34.451.846 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Gradients/Default/gradStridedSlice/StridedSliceGrad-op19932] don't support int64, reduce precision from int64 to int32.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, step: 1, outputs are 0.34505123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] DEVICE(3410,ffff813cd780,python):2022-11-11-15:45:21.441.119 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/DropoutGenMask-op31768] don't support int64, reduce precision from int64 to int32.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, step: 2, outputs are 0.34485677\n",
      "epoch: 1, step: 3, outputs are 0.34421432\n",
      "epoch: 1, step: 4, outputs are 0.34291488\n",
      "epoch: 1, step: 5, outputs are 0.341173\n",
      "epoch: 1, step: 6, outputs are 0.33854452\n",
      "epoch: 1, step: 7, outputs are 0.33440077\n",
      "epoch: 1, step: 8, outputs are 0.32974038\n",
      "epoch: 1, step: 9, outputs are 0.32456735\n",
      "epoch: 1, step: 10, outputs are 0.3169738\n",
      "epoch: 1, step: 11, outputs are 0.30930802\n",
      "epoch: 1, step: 12, outputs are 0.30160064\n",
      "epoch: 1, step: 13, outputs are 0.29242516\n",
      "epoch: 1, step: 14, outputs are 0.2826957\n",
      "epoch: 1, step: 15, outputs are 0.27352542\n",
      "epoch: 1, step: 16, outputs are 0.2638205\n",
      "epoch: 1, step: 17, outputs are 0.25257036\n",
      "epoch: 1, step: 18, outputs are 0.2434567\n",
      "epoch: 1, step: 19, outputs are 0.2319932\n",
      "epoch: 1, step: 20, outputs are 0.22148545\n",
      "epoch: 1, step: 21, outputs are 0.21190383\n",
      "epoch: 1, step: 22, outputs are 0.20263326\n",
      "epoch: 1, step: 23, outputs are 0.19464694\n",
      "epoch: 1, step: 24, outputs are 0.1858452\n",
      "epoch: 1, step: 25, outputs are 0.17696843\n",
      "epoch: 1, step: 26, outputs are 0.16944923\n",
      "epoch: 1, step: 27, outputs are 0.161941\n",
      "epoch: 1, step: 28, outputs are 0.15551618\n",
      "epoch: 1, step: 29, outputs are 0.14809513\n",
      "epoch: 1, step: 30, outputs are 0.14144376\n",
      "epoch: 1, step: 31, outputs are 0.13486537\n",
      "epoch: 1, step: 32, outputs are 0.1297382\n",
      "epoch: 1, step: 33, outputs are 0.12412976\n",
      "epoch: 1, step: 34, outputs are 0.118760906\n",
      "epoch: 1, step: 35, outputs are 0.11369043\n",
      "epoch: 1, step: 36, outputs are 0.108958885\n",
      "epoch: 1, step: 37, outputs are 0.10486233\n",
      "epoch: 1, step: 38, outputs are 0.100809015\n",
      "epoch: 1, step: 39, outputs are 0.09713684\n",
      "epoch: 1, step: 40, outputs are 0.09334342\n",
      "epoch: 1, step: 41, outputs are 0.090251565\n",
      "epoch: 1, step: 42, outputs are 0.087154284\n",
      "epoch: 1, step: 43, outputs are 0.08409794\n",
      "epoch: 1, step: 44, outputs are 0.08107211\n",
      "epoch: 1, step: 45, outputs are 0.078666\n",
      "epoch: 1, step: 46, outputs are 0.07606133\n",
      "epoch: 1, step: 47, outputs are 0.07385656\n",
      "epoch: 1, step: 48, outputs are 0.071619675\n",
      "epoch: 1, step: 49, outputs are 0.069595195\n",
      "epoch: 1, step: 50, outputs are 0.06763614\n",
      "epoch: 1, step: 51, outputs are 0.06553042\n",
      "epoch: 1, step: 52, outputs are 0.0638899\n",
      "epoch: 1, step: 53, outputs are 0.061753646\n",
      "epoch: 1, step: 54, outputs are 0.060453743\n",
      "epoch: 1, step: 55, outputs are 0.058634408\n",
      "epoch: 1, step: 56, outputs are 0.056864727\n",
      "epoch: 1, step: 57, outputs are 0.055388555\n",
      "epoch: 1, step: 58, outputs are 0.053775866\n",
      "epoch: 1, step: 59, outputs are 0.05235213\n",
      "epoch: 1, step: 60, outputs are 0.05079601\n",
      "epoch: 1, step: 61, outputs are 0.04953292\n",
      "epoch: 1, step: 62, outputs are 0.048032954\n",
      "epoch: 1, step: 63, outputs are 0.04689189\n",
      "epoch: 1, step: 64, outputs are 0.045578327\n",
      "epoch: 1, step: 65, outputs are 0.044238977\n",
      "epoch: 1, step: 66, outputs are 0.043229803\n",
      "epoch: 1, step: 67, outputs are 0.042108856\n",
      "epoch: 1, step: 68, outputs are 0.041158646\n",
      "epoch: 1, step: 69, outputs are 0.040126476\n",
      "epoch: 1, step: 70, outputs are 0.039136536\n",
      "epoch: 1, step: 71, outputs are 0.03827828\n",
      "epoch: 1, step: 72, outputs are 0.037388604\n",
      "epoch: 1, step: 73, outputs are 0.036459304\n",
      "epoch: 1, step: 74, outputs are 0.03558371\n",
      "epoch: 1, step: 75, outputs are 0.03487771\n",
      "epoch: 1, step: 76, outputs are 0.034039155\n",
      "epoch: 1, step: 77, outputs are 0.03330965\n",
      "epoch: 1, step: 78, outputs are 0.032525543\n",
      "epoch: 1, step: 79, outputs are 0.031931303\n",
      "epoch: 1, step: 80, outputs are 0.031142117\n",
      "epoch: 1, step: 81, outputs are 0.030593207\n",
      "epoch: 1, step: 82, outputs are 0.029871035\n",
      "epoch: 1, step: 83, outputs are 0.029321078\n",
      "epoch: 1, step: 84, outputs are 0.028625403\n",
      "epoch: 1, step: 85, outputs are 0.028062623\n",
      "epoch: 1, step: 86, outputs are 0.027504317\n",
      "epoch: 1, step: 87, outputs are 0.0269086\n",
      "epoch: 1, step: 88, outputs are 0.026435688\n",
      "epoch: 1, step: 89, outputs are 0.02586741\n",
      "epoch: 1, step: 90, outputs are 0.02543781\n",
      "epoch: 1, step: 91, outputs are 0.024985824\n",
      "epoch: 1, step: 92, outputs are 0.02446672\n",
      "epoch: 1, step: 93, outputs are 0.024023285\n",
      "epoch: 1, step: 94, outputs are 0.023595281\n",
      "epoch: 1, step: 95, outputs are 0.023270842\n",
      "epoch: 1, step: 96, outputs are 0.022794573\n",
      "epoch: 1, step: 97, outputs are 0.02244797\n",
      "epoch: 1, step: 98, outputs are 0.022045078\n",
      "epoch: 1, step: 99, outputs are 0.021648038\n",
      "epoch: 1, step: 100, outputs are 0.021279823\n",
      "epoch: 1, step: 101, outputs are 0.02097396\n",
      "epoch: 1, step: 102, outputs are 0.020677473\n",
      "epoch: 1, step: 103, outputs are 0.020305626\n",
      "epoch: 1, step: 104, outputs are 0.019952698\n",
      "epoch: 1, step: 105, outputs are 0.019706\n",
      "epoch: 1, step: 106, outputs are 0.019379783\n",
      "epoch: 1, step: 107, outputs are 0.01906062\n",
      "epoch: 1, step: 108, outputs are 0.018809613\n",
      "epoch: 1, step: 109, outputs are 0.018509889\n",
      "epoch: 1, step: 110, outputs are 0.018200818\n",
      "epoch: 1, step: 111, outputs are 0.017953986\n",
      "epoch: 1, step: 112, outputs are 0.017702049\n",
      "epoch: 1, step: 113, outputs are 0.01743709\n",
      "epoch: 1, step: 114, outputs are 0.017204778\n",
      "epoch: 1, step: 115, outputs are 0.016969755\n",
      "epoch: 1, step: 116, outputs are 0.016697697\n",
      "epoch: 1, step: 117, outputs are 0.01645274\n",
      "epoch: 1, step: 118, outputs are 0.016234357\n",
      "epoch: 1, step: 119, outputs are 0.015999202\n",
      "epoch: 1, step: 120, outputs are 0.01579924\n",
      "epoch: 1, step: 121, outputs are 0.015581945\n",
      "epoch: 1, step: 122, outputs are 0.015372442\n",
      "epoch: 1, step: 123, outputs are 0.015176054\n",
      "epoch: 1, step: 124, outputs are 0.014968634\n",
      "epoch: 1, step: 125, outputs are 0.014785884\n",
      "epoch: 1, step: 126, outputs are 0.014606642\n",
      "epoch: 1, step: 127, outputs are 0.0144031765\n",
      "epoch: 1, step: 128, outputs are 0.014210671\n",
      "epoch: 1, step: 129, outputs are 0.014045362\n",
      "epoch: 1, step: 130, outputs are 0.01387874\n",
      "epoch: 1, step: 131, outputs are 0.013687456\n",
      "epoch: 1, step: 132, outputs are 0.013525212\n",
      "epoch: 1, step: 133, outputs are 0.013351604\n",
      "epoch: 1, step: 134, outputs are 0.013190459\n",
      "epoch: 1, step: 135, outputs are 0.012995361\n",
      "epoch: 1, step: 136, outputs are 0.012845901\n",
      "epoch: 1, step: 137, outputs are 0.012711547\n",
      "epoch: 1, step: 138, outputs are 0.012544411\n",
      "epoch: 1, step: 139, outputs are 0.012404822\n",
      "epoch: 1, step: 140, outputs are 0.012265818\n",
      "epoch: 1, step: 141, outputs are 0.012119707\n",
      "epoch: 1, step: 142, outputs are 0.011982813\n",
      "epoch: 1, step: 143, outputs are 0.01182318\n",
      "epoch: 1, step: 144, outputs are 0.011695053\n",
      "epoch: 1, step: 145, outputs are 0.011547048\n",
      "epoch: 1, step: 146, outputs are 0.011438115\n",
      "epoch: 1, step: 147, outputs are 0.011305073\n",
      "epoch: 1, step: 148, outputs are 0.011166525\n",
      "epoch: 1, step: 149, outputs are 0.011043083\n",
      "epoch: 1, step: 150, outputs are 0.010906445\n",
      "epoch: 1, step: 151, outputs are 0.010785747\n",
      "epoch: 1, step: 152, outputs are 0.010663856\n",
      "epoch: 1, step: 153, outputs are 0.010539662\n",
      "epoch: 1, step: 154, outputs are 0.010443159\n",
      "epoch: 1, step: 155, outputs are 0.010323342\n",
      "epoch: 1, step: 156, outputs are 0.0102135725\n",
      "epoch: 1, step: 157, outputs are 0.010095498\n",
      "epoch: 1, step: 158, outputs are 0.009995221\n",
      "epoch: 1, step: 159, outputs are 0.009871319\n",
      "epoch: 1, step: 160, outputs are 0.009771683\n",
      "epoch: 1, step: 161, outputs are 0.009670002\n",
      "epoch: 1, step: 162, outputs are 0.009565338\n",
      "epoch: 1, step: 163, outputs are 0.009461518\n",
      "epoch: 1, step: 164, outputs are 0.009378569\n",
      "epoch: 1, step: 165, outputs are 0.0092690885\n",
      "epoch: 1, step: 166, outputs are 0.009167649\n",
      "epoch: 1, step: 167, outputs are 0.009063911\n",
      "epoch: 1, step: 168, outputs are 0.008969073\n",
      "epoch: 1, step: 169, outputs are 0.008894412\n",
      "epoch: 1, step: 170, outputs are 0.008797603\n",
      "epoch: 1, step: 171, outputs are 0.0086944755\n",
      "epoch: 1, step: 172, outputs are 0.00862159\n",
      "epoch: 1, step: 173, outputs are 0.0085184\n",
      "epoch: 1, step: 174, outputs are 0.0084405765\n",
      "epoch: 1, step: 175, outputs are 0.008373007\n",
      "epoch: 1, step: 176, outputs are 0.008291163\n",
      "epoch: 1, step: 177, outputs are 0.008222826\n",
      "epoch: 1, step: 178, outputs are 0.0081529245\n",
      "epoch: 1, step: 179, outputs are 0.008071785\n",
      "epoch: 1, step: 180, outputs are 0.008019602\n",
      "epoch: 1, step: 181, outputs are 0.007943206\n",
      "epoch: 1, step: 182, outputs are 0.007875319\n",
      "epoch: 1, step: 183, outputs are 0.007813919\n",
      "epoch: 1, step: 184, outputs are 0.007745047\n",
      "epoch: 1, step: 185, outputs are 0.0076995175\n",
      "epoch: 1, step: 186, outputs are 0.0076351333\n",
      "epoch: 1, step: 187, outputs are 0.007566747\n",
      "epoch: 1, step: 188, outputs are 0.007498429\n",
      "epoch: 1, step: 189, outputs are 0.007443988\n",
      "epoch: 1, step: 190, outputs are 0.00737991\n",
      "epoch: 1, step: 191, outputs are 0.007316119\n",
      "epoch: 1, step: 192, outputs are 0.0072469506\n",
      "epoch: 1, step: 193, outputs are 0.00718008\n",
      "epoch: 1, step: 194, outputs are 0.0071013304\n",
      "epoch: 1, step: 195, outputs are 0.0070420033\n",
      "epoch: 1, step: 196, outputs are 0.0069772885\n",
      "epoch: 1, step: 197, outputs are 0.0068985233\n",
      "epoch: 1, step: 198, outputs are 0.0068404763\n",
      "epoch: 1, step: 199, outputs are 0.0067682564\n",
      "epoch: 1, step: 200, outputs are 0.006720024\n",
      "epoch: 1, step: 201, outputs are 0.0066587953\n",
      "epoch: 1, step: 202, outputs are 0.006606521\n",
      "epoch: 1, step: 203, outputs are 0.0065381923\n",
      "epoch: 1, step: 204, outputs are 0.006481178\n",
      "epoch: 1, step: 205, outputs are 0.006425398\n",
      "epoch: 1, step: 206, outputs are 0.0063802768\n",
      "epoch: 1, step: 207, outputs are 0.0063243713\n",
      "epoch: 1, step: 208, outputs are 0.00628209\n",
      "epoch: 1, step: 209, outputs are 0.0062333476\n",
      "epoch: 1, step: 210, outputs are 0.006188829\n",
      "epoch: 1, step: 211, outputs are 0.0061426926\n",
      "epoch: 1, step: 212, outputs are 0.00609624\n",
      "epoch: 1, step: 213, outputs are 0.006056631\n",
      "epoch: 1, step: 214, outputs are 0.006013955\n",
      "epoch: 1, step: 215, outputs are 0.005967118\n",
      "epoch: 1, step: 216, outputs are 0.0059388885\n",
      "epoch: 1, step: 217, outputs are 0.005886586\n",
      "epoch: 1, step: 218, outputs are 0.0058539817\n",
      "epoch: 1, step: 219, outputs are 0.0058243666\n",
      "epoch: 1, step: 220, outputs are 0.0057785455\n",
      "epoch: 1, step: 221, outputs are 0.005749532\n",
      "epoch: 1, step: 222, outputs are 0.0057136468\n",
      "epoch: 1, step: 223, outputs are 0.005676195\n",
      "epoch: 1, step: 224, outputs are 0.0056551103\n",
      "epoch: 1, step: 225, outputs are 0.00561355\n",
      "epoch: 1, step: 226, outputs are 0.0055830963\n",
      "epoch: 1, step: 227, outputs are 0.0055458136\n",
      "epoch: 1, step: 228, outputs are 0.005515054\n",
      "epoch: 1, step: 229, outputs are 0.0054813516\n",
      "epoch: 1, step: 230, outputs are 0.0054573566\n",
      "epoch: 1, step: 231, outputs are 0.005421221\n",
      "epoch: 1, step: 232, outputs are 0.0053934995\n",
      "epoch: 1, step: 233, outputs are 0.0053651924\n",
      "epoch: 1, step: 234, outputs are 0.0053316588\n",
      "epoch: 1, step: 235, outputs are 0.0053071673\n",
      "epoch: 1, step: 236, outputs are 0.0052821212\n",
      "epoch: 1, step: 237, outputs are 0.005244282\n",
      "epoch: 1, step: 238, outputs are 0.0052236672\n",
      "epoch: 1, step: 239, outputs are 0.005197388\n",
      "epoch: 1, step: 240, outputs are 0.0051707462\n",
      "epoch: 1, step: 241, outputs are 0.0051439223\n",
      "epoch: 1, step: 242, outputs are 0.0051181726\n",
      "epoch: 1, step: 243, outputs are 0.0050920066\n",
      "epoch: 1, step: 244, outputs are 0.005069521\n",
      "epoch: 1, step: 245, outputs are 0.005039748\n",
      "epoch: 1, step: 246, outputs are 0.005009708\n",
      "epoch: 1, step: 247, outputs are 0.004985638\n",
      "epoch: 1, step: 248, outputs are 0.00496193\n",
      "epoch: 1, step: 249, outputs are 0.004935028\n",
      "epoch: 1, step: 250, outputs are 0.004913588\n",
      "epoch: 1, step: 251, outputs are 0.004890134\n",
      "epoch: 1, step: 252, outputs are 0.0048579797\n",
      "epoch: 1, step: 253, outputs are 0.004842183\n",
      "epoch: 1, step: 254, outputs are 0.0048218016\n",
      "epoch: 1, step: 255, outputs are 0.0047995853\n",
      "epoch: 1, step: 256, outputs are 0.0047720214\n",
      "epoch: 1, step: 257, outputs are 0.004755243\n",
      "epoch: 1, step: 258, outputs are 0.004736517\n",
      "epoch: 1, step: 259, outputs are 0.0047098007\n",
      "epoch: 1, step: 260, outputs are 0.0046832534\n",
      "epoch: 1, step: 261, outputs are 0.0046574404\n",
      "epoch: 1, step: 262, outputs are 0.004644956\n",
      "epoch: 1, step: 263, outputs are 0.004621445\n",
      "epoch: 1, step: 264, outputs are 0.004602517\n",
      "epoch: 1, step: 265, outputs are 0.00458416\n",
      "epoch: 1, step: 266, outputs are 0.0045624655\n",
      "epoch: 1, step: 267, outputs are 0.004542823\n",
      "epoch: 1, step: 268, outputs are 0.0045193005\n",
      "epoch: 1, step: 269, outputs are 0.004502777\n",
      "epoch: 1, step: 270, outputs are 0.004483545\n",
      "epoch: 1, step: 271, outputs are 0.0044680377\n",
      "epoch: 1, step: 272, outputs are 0.004445389\n",
      "epoch: 1, step: 273, outputs are 0.0044306307\n",
      "epoch: 1, step: 274, outputs are 0.0044056247\n",
      "epoch: 1, step: 275, outputs are 0.0043950444\n",
      "epoch: 1, step: 276, outputs are 0.0043691183\n",
      "epoch: 1, step: 277, outputs are 0.0043519996\n",
      "epoch: 1, step: 278, outputs are 0.0043358346\n",
      "epoch: 1, step: 279, outputs are 0.00431188\n",
      "epoch: 1, step: 280, outputs are 0.004298191\n",
      "epoch: 1, step: 281, outputs are 0.004287631\n",
      "epoch: 1, step: 282, outputs are 0.004263064\n",
      "epoch: 1, step: 283, outputs are 0.004250734\n",
      "epoch: 1, step: 284, outputs are 0.0042298655\n",
      "epoch: 1, step: 285, outputs are 0.004215892\n",
      "epoch: 1, step: 286, outputs are 0.004194187\n",
      "epoch: 1, step: 287, outputs are 0.0041789883\n",
      "epoch: 1, step: 288, outputs are 0.0041656024\n",
      "epoch: 1, step: 289, outputs are 0.0041524875\n",
      "epoch: 1, step: 290, outputs are 0.0041336757\n",
      "epoch: 1, step: 291, outputs are 0.004118976\n",
      "epoch: 1, step: 292, outputs are 0.0041068164\n",
      "epoch: 1, step: 293, outputs are 0.004089482\n",
      "epoch: 1, step: 294, outputs are 0.004070484\n",
      "epoch: 1, step: 295, outputs are 0.004058225\n",
      "epoch: 1, step: 296, outputs are 0.004038833\n",
      "epoch: 1, step: 297, outputs are 0.0040270635\n",
      "epoch: 1, step: 298, outputs are 0.004006566\n",
      "epoch: 1, step: 299, outputs are 0.0039963108\n",
      "epoch: 1, step: 300, outputs are 0.003979277\n",
      "epoch: 1, step: 301, outputs are 0.0039668987\n",
      "epoch: 1, step: 302, outputs are 0.0039554965\n",
      "epoch: 1, step: 303, outputs are 0.0039382004\n",
      "epoch: 1, step: 304, outputs are 0.0039239572\n",
      "epoch: 1, step: 305, outputs are 0.0039127655\n",
      "epoch: 1, step: 306, outputs are 0.003901342\n",
      "epoch: 1, step: 307, outputs are 0.0038776528\n",
      "epoch: 1, step: 308, outputs are 0.0038717424\n",
      "epoch: 1, step: 309, outputs are 0.0038505485\n",
      "epoch: 1, step: 310, outputs are 0.003843277\n",
      "epoch: 1, step: 311, outputs are 0.0038308206\n",
      "epoch: 1, step: 312, outputs are 0.0038116258\n",
      "epoch: 1, step: 313, outputs are 0.0037976634\n",
      "epoch: 1, step: 314, outputs are 0.003788211\n",
      "epoch: 1, step: 315, outputs are 0.0037695426\n",
      "epoch: 1, step: 316, outputs are 0.003761331\n",
      "epoch: 1, step: 317, outputs are 0.003746055\n",
      "epoch: 1, step: 318, outputs are 0.0037378543\n",
      "epoch: 1, step: 319, outputs are 0.0037246519\n",
      "epoch: 1, step: 320, outputs are 0.0037094196\n",
      "epoch: 1, step: 321, outputs are 0.0037002568\n",
      "epoch: 1, step: 322, outputs are 0.0036842574\n",
      "epoch: 1, step: 323, outputs are 0.0036735488\n",
      "epoch: 1, step: 324, outputs are 0.0036610116\n",
      "epoch: 1, step: 325, outputs are 0.0036473344\n",
      "epoch: 1, step: 326, outputs are 0.0036331387\n",
      "epoch: 1, step: 327, outputs are 0.0036275578\n",
      "epoch: 1, step: 328, outputs are 0.0036127279\n",
      "epoch: 1, step: 329, outputs are 0.0036003967\n",
      "epoch: 1, step: 330, outputs are 0.0035884224\n",
      "epoch: 1, step: 331, outputs are 0.0035770023\n",
      "epoch: 1, step: 332, outputs are 0.0035659815\n",
      "epoch: 1, step: 333, outputs are 0.0035561472\n",
      "epoch: 1, step: 334, outputs are 0.003540233\n",
      "epoch: 1, step: 335, outputs are 0.0035335845\n",
      "epoch: 1, step: 336, outputs are 0.0035190687\n",
      "epoch: 1, step: 337, outputs are 0.003502177\n",
      "epoch: 1, step: 338, outputs are 0.0034956886\n",
      "epoch: 1, step: 339, outputs are 0.0034776153\n",
      "epoch: 1, step: 340, outputs are 0.0034682215\n",
      "epoch: 1, step: 341, outputs are 0.0034601297\n",
      "epoch: 1, step: 342, outputs are 0.003447054\n",
      "epoch: 1, step: 343, outputs are 0.0034317463\n",
      "epoch: 1, step: 344, outputs are 0.0034177997\n",
      "epoch: 1, step: 345, outputs are 0.0034064308\n",
      "epoch: 1, step: 346, outputs are 0.0033922803\n",
      "epoch: 1, step: 347, outputs are 0.0033778397\n",
      "epoch: 1, step: 348, outputs are 0.0033635825\n",
      "epoch: 1, step: 349, outputs are 0.0033512297\n",
      "epoch: 1, step: 350, outputs are 0.0033335541\n",
      "epoch: 1, step: 351, outputs are 0.0033279036\n",
      "epoch: 1, step: 352, outputs are 0.0033106224\n",
      "epoch: 1, step: 353, outputs are 0.0032976428\n",
      "epoch: 1, step: 354, outputs are 0.0032851803\n",
      "epoch: 1, step: 355, outputs are 0.0032713923\n",
      "epoch: 1, step: 356, outputs are 0.003258591\n",
      "epoch: 1, step: 357, outputs are 0.0032423716\n",
      "epoch: 1, step: 358, outputs are 0.0032304307\n",
      "epoch: 1, step: 359, outputs are 0.0032211917\n",
      "epoch: 1, step: 360, outputs are 0.0032079383\n",
      "epoch: 1, step: 361, outputs are 0.0031945042\n",
      "epoch: 1, step: 362, outputs are 0.00318036\n",
      "epoch: 1, step: 363, outputs are 0.0031683003\n",
      "epoch: 1, step: 364, outputs are 0.0031586816\n",
      "epoch: 1, step: 365, outputs are 0.0031468752\n",
      "epoch: 1, step: 366, outputs are 0.0031372602\n",
      "epoch: 1, step: 367, outputs are 0.0031210664\n",
      "epoch: 1, step: 368, outputs are 0.0031156216\n",
      "epoch: 1, step: 369, outputs are 0.0031051275\n",
      "epoch: 1, step: 370, outputs are 0.0030927784\n",
      "epoch: 1, step: 371, outputs are 0.0030834752\n",
      "epoch: 1, step: 372, outputs are 0.003069503\n",
      "epoch: 1, step: 373, outputs are 0.003057304\n",
      "epoch: 1, step: 374, outputs are 0.003046799\n",
      "epoch: 1, step: 375, outputs are 0.0030441112\n",
      "epoch: 1, step: 376, outputs are 0.0030304147\n",
      "epoch: 1, step: 377, outputs are 0.0030223005\n",
      "epoch: 1, step: 378, outputs are 0.003013357\n",
      "epoch: 1, step: 379, outputs are 0.003003364\n",
      "epoch: 1, step: 380, outputs are 0.002989624\n",
      "epoch: 1, step: 381, outputs are 0.0029815407\n",
      "epoch: 1, step: 382, outputs are 0.0029749568\n",
      "epoch: 1, step: 383, outputs are 0.0029673835\n",
      "epoch: 1, step: 384, outputs are 0.0029575562\n",
      "epoch: 1, step: 385, outputs are 0.0029484672\n",
      "epoch: 1, step: 386, outputs are 0.0029410282\n",
      "epoch: 1, step: 387, outputs are 0.0029288826\n",
      "epoch: 1, step: 388, outputs are 0.0029230167\n",
      "epoch: 1, step: 389, outputs are 0.0029152215\n",
      "epoch: 1, step: 390, outputs are 0.0029055816\n",
      "epoch: 1, step: 391, outputs are 0.0028946612\n",
      "epoch: 1, step: 392, outputs are 0.002890584\n",
      "epoch: 1, step: 393, outputs are 0.0028796124\n",
      "epoch: 1, step: 394, outputs are 0.0028720063\n",
      "epoch: 1, step: 395, outputs are 0.0028641154\n",
      "epoch: 1, step: 396, outputs are 0.0028527472\n",
      "epoch: 1, step: 397, outputs are 0.0028501442\n",
      "epoch: 1, step: 398, outputs are 0.0028419732\n",
      "epoch: 1, step: 399, outputs are 0.0028318488\n",
      "epoch: 1, step: 400, outputs are 0.0028279177\n",
      "epoch: 1, step: 401, outputs are 0.0028191064\n",
      "epoch: 1, step: 402, outputs are 0.0028084316\n",
      "epoch: 1, step: 403, outputs are 0.0028031282\n",
      "epoch: 1, step: 404, outputs are 0.0027960339\n",
      "epoch: 1, step: 405, outputs are 0.0027886545\n",
      "epoch: 1, step: 406, outputs are 0.0027808067\n",
      "epoch: 1, step: 407, outputs are 0.0027736656\n",
      "epoch: 1, step: 408, outputs are 0.0027665705\n",
      "epoch: 1, step: 409, outputs are 0.0027596946\n",
      "epoch: 1, step: 410, outputs are 0.0027512922\n",
      "epoch: 1, step: 411, outputs are 0.0027418076\n",
      "epoch: 1, step: 412, outputs are 0.0027366248\n",
      "epoch: 1, step: 413, outputs are 0.0027292473\n",
      "epoch: 1, step: 414, outputs are 0.0027219171\n",
      "epoch: 1, step: 415, outputs are 0.0027185292\n",
      "epoch: 1, step: 416, outputs are 0.0027086018\n",
      "epoch: 1, step: 417, outputs are 0.0027031014\n",
      "epoch: 1, step: 418, outputs are 0.0026951877\n",
      "epoch: 1, step: 419, outputs are 0.0026905232\n",
      "epoch: 1, step: 420, outputs are 0.0026845904\n",
      "epoch: 1, step: 421, outputs are 0.0026754648\n",
      "epoch: 1, step: 422, outputs are 0.0026674827\n",
      "epoch: 1, step: 423, outputs are 0.0026627067\n",
      "epoch: 1, step: 424, outputs are 0.0026577804\n",
      "epoch: 1, step: 425, outputs are 0.002650543\n",
      "epoch: 1, step: 426, outputs are 0.0026450586\n",
      "epoch: 1, step: 427, outputs are 0.002635395\n",
      "epoch: 1, step: 428, outputs are 0.0026343148\n",
      "epoch: 1, step: 429, outputs are 0.0026257886\n",
      "epoch: 1, step: 430, outputs are 0.0026178756\n",
      "Train epoch time: 453696.727 ms, per step time: 1055.109 ms\n",
      "The best acc is 0.500217833284926\n"
     ]
    }
   ],
   "source": [
    "netwithgrads = BertEvaluationCell(netwithloss, optimizer=optimizer)\n",
    "\n",
    "model = Model(netwithgrads)\n",
    "model.train(repeat_count, train_dataset, callbacks=callback,\n",
    "            dataset_sink_mode=(args_opt.enable_data_sink == 'true'),\n",
    "            sink_size=args_opt.data_sink_steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
